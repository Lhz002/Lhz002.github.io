<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class DataFrameDataset(Dataset):    def __init__(self, seq_df):        self._cach">
<meta property="og:type" content="article">
<meta property="og:title" content="ECnet代码解读（三）">
<meta property="og:url" content="http://example.com/2023/08/10/ECnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%89%EF%BC%89/index.html">
<meta property="og:site_name" content="The Cabin of Hamzone">
<meta property="og:description" content="12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class DataFrameDataset(Dataset):    def __init__(self, seq_df):        self._cach">
<meta property="og:locale">
<meta property="article:published_time" content="2023-08-10T09:59:42.000Z">
<meta property="article:modified_time" content="2023-08-10T10:46:51.216Z">
<meta property="article:author" content="Li Hanzhang">
<meta name="twitter:card" content="summary">


<title >ECnet代码解读（三）</title>

<!-- Favicon -->

    <link href='/Logo.svg?v=2.0.9' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/Logo.svg?v=2.0.9' rel='icon' type='image/png' sizes='32x32' ></link>


    <link href='/Logo.svg?v=2.0.9' rel='apple-touch-icon' sizes='180x180' ></link>


    <link href='/site.webmanifest' rel='manifest' ></link>


<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://npm.elemecdn.com/locomotive-scroll@4.1.4/dist/locomotive-scroll.min.css">

    
<link rel="stylesheet" href="https://npm.elemecdn.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    




<!-- Icon -->

    
<link rel="stylesheet" href="//at.alicdn.com/t/c/font_4177738_vsqmc23e2y.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"Li Hanzhang","root":"/","typed_text":["Always Learning","Always Coding","Always Improving"],"theme_version":"2.0.9","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"Logo.svg","icon16":"Logo.svg","icon32":"Logo.svg","appleTouchIcon":"Logo.svg","webmanifest":"/site.webmanifest","visibilitychange":true,"hidden":"failure.ico","showText":"(/≧▽≦/)咦！又好了！","hideText":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://npm.elemecdn.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"icon-rijian","moon":"icon-yueduye-yejianmoshi","play":"icon-24gf-playCircle","email":"icon-youxiang","next":"icon-jinrushiyan","calendar":"icon-riqi","clock":"icon-shijian","user":"icon-totop","back_top":"fas fa-arrow-up","close":"icon-anniu_guanbi","search":"icon-sousuo","reward":"icon-dashang","user_tag":"fas fa-user-alt","toc_tag":"fas fa-th-list","read":"icon-read","arrows":"icon-shangxiazhankai","double_arrows":"icon-shangxiazhankai","copy":"icon-fuzhi"},"icontype":"font","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":200},"search":{"enable":true,"type":"local","href":"https://www.google.com/search?q=site:","domain":null,"path":"search.xml"}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2023-08-10 18:46:51"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.0.9" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->
 
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="The Cabin of Hamzone" type="application/atom+xml">
</head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont icon-rijian"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont icon-yueduye-yejianmoshi"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" data-scroll-container style="opacity: 0">
          <div data-scroll-section id="content" class="trm-scroll-section">

            <div class="locomotive-scroll__sticky-target" style="position: absolute; top: 0; left: 0; right: 0; bottom: 0; pointer-events: none;"></div>

            <!-- top bar -->
            <header class="trm-top-bar" data-scroll data-scroll-sticky data-scroll-target=".locomotive-scroll__sticky-target" data-scroll-offset="-10">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/Logo.svg">
    
    
        <div class="trm-logo-text">
            Hamzone&#39;s<span>Cabin</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    home
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/categories/" target="">
                    categories
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/archives/" target="">
                    archives
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/about/" target="">
                    About
                </a>
                
                <ul>
                    
                    <li>
                        <a  href="/project/" target="">
                            Project
                        </a>
                    </li>
                    
                </ul>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/links/" target="">
                    Links
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a data-no-swup href="/gallary/" target="">
                    Gallary
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont icon-rijian"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont icon-yueduye-yejianmoshi"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
    
    <div id="trm-search-btn" class="trm-search-btn">
        <i class="iconfont icon-sousuo"></i>
    </div>
     

		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner" data-scroll data-scroll-direction="vertical">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" data-scroll data-scroll-direction="vertical" data-scroll-speed="-3" src="/img/banner.png">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container" data-scroll data-scroll-direction="vertical" data-scroll-speed="0">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            NEWS LETTER
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            ECnet代码解读（三）
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2023
                                    </span
                                ></li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <a href="#about-triger" data-scroll-to="#about-triger" data-scroll-offset="-130" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </a>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div id="page-sidebar" class="col-lg-4 hidden-sm">
                    <!-- main card -->
                    

<div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card" data-scroll data-scroll-repeat data-scroll-sticky data-scroll-target=".locomotive-scroll__sticky-target" data-scroll-offset="60"> 
    
        <div class="trm-user-tabs" id="sidebar-tabs">
           <div class="trm-tabs-nav trm-mb-40" id="trm-tabs-nav">
                <div data-to="tabs-user" class="trm-tabs-nav-item">
                    <i class="iconfont fas fa-user-alt"></i>
                </div>
                <div data-to="tabs-toc" class="trm-tabs-nav-item active">
                    <i class="iconfont fas fa-th-list"></i>
                </div>
           </div>
            <div name="tabs-user" class="trm-tabs-item">
                <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar" src="/img/avatar.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        Hamzone
    </h5>
    
        <div class="trm-label">
            I`m
            <span class="trm-typed-text">
                <!-- Words for theme.user.typedText -->
            </span>
        </div>
    
</div>
<!-- card header end -->
                <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com/Lhz002" title="github" rel="nofollow" target="_blank">
            <i class="iconfont icon-github-fill"></i>
        </a>
    
        <a href="https://bilibili.com" title="bilibili" rel="nofollow" target="_blank">
            <i class="iconfont icon-bilibili"></i>
        </a>
    
</div>

<!-- sidebar social end -->
                <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                Name:
            </div>
            <div class="trm-label trm-label-light">
                Li Hanzhang
            </div>
        </li>
    
        <li>
            <div class="trm-label">
                Location:
            </div>
            <div class="trm-label trm-label-light">
                ZheJiang, China
            </div>
        </li>
    
        <li>
            <div class="trm-label">
                Age:
            </div>
            <div class="trm-label trm-label-light">
                20
            </div>
        </li>
    
        <li>
            <div class="trm-label">
                Email:
            </div>
            <div class="trm-label trm-label-light">
                Lhz1209@outlook.com
            </div>
        </li>
    
</ul>
<!-- info end -->

                
    <div class="trm-divider trm-mb-40 trm-mt-40"></div>
    <!-- action button -->
    <div class="text-center">
        <a href="mailto:Lhz1209@outlook.com" class="trm-btn">
            联系我
            <i class="iconfont icon-youxiang"></i>
        </a>
    </div>
    <!-- action button end -->

            </div>
            <div name="tabs-toc" class="trm-tabs-item active">
                <div class="post-toc">
    <ol class="toc"><li class="toc-item toc-level-2"><a rel="nofollow" class="toc-link" href="#训练数据预处理"  data-scroll-to="#训练数据预处理"><span class="toc-number">1.</span> <span class="toc-text">训练数据预处理</span></a></li><li class="toc-item toc-level-2"><a rel="nofollow" class="toc-link" href="#TAPE编码"  data-scroll-to="#TAPE编码"><span class="toc-number">2.</span> <span class="toc-text">TAPE编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#1-初始化方法-init"  data-scroll-to="#1-初始化方法-init"><span class="toc-number">2.1.</span> <span class="toc-text">1. 初始化方法 (__init__):</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#2-编码方法-encode"  data-scroll-to="#2-编码方法-encode"><span class="toc-number">2.2.</span> <span class="toc-text">2. 编码方法 (encode):</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#3-TAPE-嵌入方法-tape-embed"  data-scroll-to="#3-TAPE-嵌入方法-tape-embed"><span class="toc-number">2.3.</span> <span class="toc-text">3. TAPE 嵌入方法 (tape_embed):</span></a></li></ol></li><li class="toc-item toc-level-2"><a rel="nofollow" class="toc-link" href="#CCMPred编码局部变量"  data-scroll-to="#CCMPred编码局部变量"><span class="toc-number">3.</span> <span class="toc-text">CCMPred编码局部变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#1-函数-all-sequence-pairwise-profile："  data-scroll-to="#1-函数-all-sequence-pairwise-profile："><span class="toc-number">3.1.</span> <span class="toc-text">1. 函数 all_sequence_pairwise_profile：</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#2-函数-all-sequence-singleton-profile："  data-scroll-to="#2-函数-all-sequence-singleton-profile："><span class="toc-number">3.2.</span> <span class="toc-text">2. 函数 all_sequence_singleton_profile：</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#3-类-CCMPredEncoder："  data-scroll-to="#3-类-CCMPredEncoder："><span class="toc-number">3.3.</span> <span class="toc-text">3. 类 CCMPredEncoder：</span></a></li></ol></li><li class="toc-item toc-level-2"><a rel="nofollow" class="toc-link" href="#模型构建"  data-scroll-to="#模型构建"><span class="toc-number">4.</span> <span class="toc-text">模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#1-PositionalEmbedding-类"  data-scroll-to="#1-PositionalEmbedding-类"><span class="toc-number">4.1.</span> <span class="toc-text">1. PositionalEmbedding 类</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#2-InputPositionEmbedding-类"  data-scroll-to="#2-InputPositionEmbedding-类"><span class="toc-number">4.2.</span> <span class="toc-text">2. InputPositionEmbedding 类</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#3-AggregateLayer-类"  data-scroll-to="#3-AggregateLayer-类"><span class="toc-number">4.3.</span> <span class="toc-text">3. AggregateLayer 类</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#4-GlobalPredictor-类"  data-scroll-to="#4-GlobalPredictor-类"><span class="toc-number">4.4.</span> <span class="toc-text">4. GlobalPredictor 类</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#5-SequenceLSTM-类"  data-scroll-to="#5-SequenceLSTM-类"><span class="toc-number">4.5.</span> <span class="toc-text">5. SequenceLSTM 类</span></a></li><li class="toc-item toc-level-3"><a rel="nofollow" class="toc-link" href="#6-LSTMPredictor-类"  data-scroll-to="#6-LSTMPredictor-类"><span class="toc-number">4.6.</span> <span class="toc-text">6. LSTMPredictor 类</span></a></li></ol></li></ol>
</div>
            </div>
        </div>
    
    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div id="page-content" class="col-lg-8">
                <div class="trm-content" id="trm-content">
                    <div data-scroll data-scroll-repeat data-scroll-offset="500" id="about-triger"></div>

                    <div id="post-info" class="row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont icon-riqi trm-icon"></i><br>
            08/10
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont icon-shijian trm-icon"></i><br>
            17:59
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont icon-totop trm-icon"></i><br>
            Li Hanzhang
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataFrameDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, seq_df</span>):</span><br><span class="line">        self._cache = []</span><br><span class="line">        <span class="keyword">for</span> seq_id, seq <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            seq_df[<span class="string">&#x27;ID&#x27;</span>].values,</span><br><span class="line">            seq_df[<span class="string">&#x27;sequence&#x27;</span>].values</span><br><span class="line">        ):</span><br><span class="line">            self._cache.append(&#123;</span><br><span class="line">                <span class="string">&#x27;id&#x27;</span>: seq_id,</span><br><span class="line">                <span class="string">&#x27;primary&#x27;</span>: seq,</span><br><span class="line">                <span class="string">&#x27;protein_length&#x27;</span>: <span class="built_in">len</span>(seq)</span><br><span class="line">            &#125;)</span><br><span class="line">        self._num_examples = <span class="built_in">len</span>(self._cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self._num_examples</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0</span> &lt;= index &lt; self._num_examples:</span><br><span class="line">            <span class="keyword">raise</span> IndexError(index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._cache[index]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pad_sequences</span>(<span class="params">sequences: <span class="type">Sequence</span>[np.ndarray], constant_value=<span class="number">0</span></span>) -&gt; np.ndarray:</span><br><span class="line">    batch_size = <span class="built_in">len</span>(sequences)</span><br><span class="line">    shape = [batch_size] + np.<span class="built_in">max</span>([seq.shape <span class="keyword">for</span> seq <span class="keyword">in</span> sequences], <span class="number">0</span>).tolist()</span><br><span class="line">    array = np.zeros(shape, sequences[<span class="number">0</span>].dtype) + constant_value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> arr, seq <span class="keyword">in</span> <span class="built_in">zip</span>(array, sequences):</span><br><span class="line">        arrslice = <span class="built_in">tuple</span>(<span class="built_in">slice</span>(dim) <span class="keyword">for</span> dim <span class="keyword">in</span> seq.shape)</span><br><span class="line">        arr[arrslice] = seq</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> array</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EmbedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 data,</span></span><br><span class="line"><span class="params">                 tokenizer: <span class="type">Union</span>[<span class="built_in">str</span>, TAPETokenizer] = <span class="string">&#x27;iupac&#x27;</span>,</span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(tokenizer, <span class="built_in">str</span>):</span><br><span class="line">            tokenizer = TAPETokenizer(vocab=tokenizer)</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, pd.DataFrame):</span><br><span class="line">            self.data = DataFrameDataset(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span><br><span class="line">        item = self.data[index]</span><br><span class="line">        token_ids = self.tokenizer.encode(item[<span class="string">&#x27;primary&#x27;</span>])</span><br><span class="line">        input_mask = np.ones_like(token_ids)</span><br><span class="line">        <span class="keyword">return</span> item[<span class="string">&#x27;id&#x27;</span>], token_ids, input_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, batch: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="type">Any</span>, ...]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        ids, tokens, input_mask = <span class="built_in">zip</span>(*batch)</span><br><span class="line">        ids = <span class="built_in">list</span>(ids)</span><br><span class="line">        tokens = torch.from_numpy(_pad_sequences(tokens))</span><br><span class="line">        input_mask = torch.from_numpy(_pad_sequences(input_mask))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;ids&#x27;</span>: ids, <span class="string">&#x27;input_ids&#x27;</span>: tokens, <span class="string">&#x27;input_mask&#x27;</span>: input_mask&#125;  <span class="comment"># type: ignore</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="训练数据预处理"><a href="#训练数据预处理" class="headerlink" title="训练数据预处理"></a>训练数据预处理</h2><ol>
<li><p><strong><code>DataFrameDataset</code> 类</strong>：</p>
<ul>
<li><p><strong>目的</strong>：将数据框（通常包含蛋白质ID和相应的蛋白质序列）转换为易于访问和使用的数据集格式。</p>
</li>
<li><p>方法</p>
<p>：</p>
<ul>
<li><code>__init__</code>: 初始化数据集并将数据存储在缓存中。</li>
<li><code>__len__</code>: 返回数据集中的项数。</li>
<li><code>__getitem__</code>: 根据索引返回数据集中的一个项。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><code>_pad_sequences</code> 函数</strong>：</p>
<ul>
<li><p><strong>目的</strong>：给定一系列不等长的序列，使用常数值填充它们，使得所有序列具有相同的长度。</p>
</li>
<li><p>参数</p>
<p>：</p>
<ul>
<li><code>sequences</code>: 要填充的序列列表。</li>
<li><code>constant_value</code>: 用于填充的常数值。</li>
</ul>
</li>
<li><p><strong>返回</strong>：一个填充后的 NumPy 数组，其中每个序列都有相同的长度。</p>
</li>
</ul>
</li>
<li><p><strong><code>EmbedDataset</code> 类</strong>：</p>
<ul>
<li><p><strong>目的</strong>：对蛋白质序列进行嵌入编码，并为训练和评估模型准备适当的输入格式。</p>
</li>
<li><p>方法</p>
<p>：</p>
<ul>
<li><code>__init__</code>: 初始化数据集并设置分词器（用于将蛋白质序列转换为嵌入向量）。</li>
<li><code>__len__</code>: 返回数据集中的项数。</li>
<li><code>__getitem__</code>: 根据索引返回数据集中的一个项。它将蛋白质序列转换为嵌入向量。</li>
<li><code>collate_fn</code>: 一个用于数据加载器的收集函数，它将一批数据组合成一个单一的输入张量。这是为了处理不同长度的蛋白质序列。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="TAPE编码"><a href="#TAPE编码" class="headerlink" title="TAPE编码"></a>TAPE编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TAPEEncoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">        batch_size: <span class="built_in">int</span> = <span class="number">128</span>,</span></span><br><span class="line"><span class="params">        model_config_file: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        full_sequence_embed: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        no_cuda: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        seed: <span class="built_in">int</span> = <span class="number">42</span>,</span></span><br><span class="line"><span class="params">        tokenizer: <span class="built_in">str</span> = <span class="string">&#x27;iupac&#x27;</span>,</span></span><br><span class="line"><span class="params">        num_workers: <span class="built_in">int</span> = <span class="number">4</span>,</span></span><br><span class="line"><span class="params">        log_level: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="built_in">int</span>] = logging.INFO,</span></span><br><span class="line"><span class="params">        progress_bar: <span class="built_in">bool</span> = <span class="literal">True</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        seq_df: pd.DataFrame object. Two columns are requried `ID` and `sequence`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        local_rank = -<span class="number">1</span>  <span class="comment"># TAPE does not support torch.distributed.launch for embedding</span></span><br><span class="line">        device, n_gpu, is_master = utils.setup_distributed(local_rank, no_cuda)</span><br><span class="line">        utils.setup_logging(local_rank, save_path=<span class="literal">None</span>, log_level=log_level)</span><br><span class="line">        utils.set_random_seeds(seed, n_gpu)</span><br><span class="line"></span><br><span class="line">        model = ProteinBertModel.from_pretrained(<span class="string">&#x27;bert-base&#x27;</span>)</span><br><span class="line">        model = model.to(device)</span><br><span class="line">        runner = ForwardRunner(model, device, n_gpu)</span><br><span class="line">        runner.initialize_distributed_model()</span><br><span class="line">        runner.<span class="built_in">eval</span>()</span><br><span class="line">        self.local_rank = local_rank</span><br><span class="line">        self.n_gpu = n_gpu</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line">        self.runner = runner</span><br><span class="line">        self.full_sequence_embed = full_sequence_embed</span><br><span class="line">        self.progress_bar = progress_bar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, sequences: [<span class="built_in">str</span>]</span>) -&gt; np.ndarray:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        sequences: list of equal-length sequences</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        np array with shape (#sequences, length of sequences, embedding dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        seq_df = pd.DataFrame(&#123;<span class="string">&#x27;ID&#x27;</span>: sequences, <span class="string">&#x27;sequence&#x27;</span>: sequences&#125;)</span><br><span class="line">        embed_dict = self.tape_embed(seq_df)</span><br><span class="line">        encoding = np.array([embed_dict[s].numpy() <span class="keyword">for</span> s <span class="keyword">in</span> sequences])</span><br><span class="line">        <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tape_embed</span>(<span class="params">self, seq_df: pd.DataFrame</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        seq_df: pd.DataFrame with at least two columns `ID` and `sequence`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        dict with ID as keys and embeddings as values. Embeddings are </span></span><br><span class="line"><span class="string">        pytorch tensor with shape (#sequences, length of sequences, embedding dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        local_rank = self.local_rank</span><br><span class="line">        n_gpu = self.n_gpu</span><br><span class="line">        batch_size = self.batch_size</span><br><span class="line">        num_workers = self.num_workers</span><br><span class="line">        runner = self.runner</span><br><span class="line">        full_sequence_embed = self.full_sequence_embed</span><br><span class="line">        progress_bar = self.progress_bar</span><br><span class="line"></span><br><span class="line">        dataset = EmbedDataset(seq_df)</span><br><span class="line">        valid_loader = utils.setup_loader(dataset, batch_size, local_rank, n_gpu, <span class="number">1</span>, num_workers)</span><br><span class="line"></span><br><span class="line">        embed_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">with</span> utils.wrap_cuda_oom_error(local_rank, batch_size, n_gpu):</span><br><span class="line">                <span class="keyword">for</span> batch <span class="keyword">in</span> (tqdm(valid_loader, total=<span class="built_in">len</span>(valid_loader),</span><br><span class="line">                        desc=<span class="string">&#x27;encode&#x27;</span>, leave=<span class="literal">False</span>) <span class="keyword">if</span> progress_bar <span class="keyword">else</span> valid_loader):</span><br><span class="line">                    outputs = runner.forward(batch, no_loss=<span class="literal">True</span>)</span><br><span class="line">                    ids = batch[<span class="string">&#x27;ids&#x27;</span>]</span><br><span class="line">                    sequence_embed = outputs[<span class="number">0</span>]</span><br><span class="line">                    pooled_embed = outputs[<span class="number">1</span>]</span><br><span class="line">                    sequence_lengths = batch[<span class="string">&#x27;input_mask&#x27;</span>].<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">                    sequence_embed = sequence_embed.cpu()</span><br><span class="line">                    sequence_lengths = sequence_lengths.cpu()</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> seqembed, length, protein_id <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">                            sequence_embed, sequence_lengths, ids):</span><br><span class="line">                        seqembed = seqembed[:length]</span><br><span class="line">                        seqembed = seqembed[<span class="number">1</span>:-<span class="number">1</span>] <span class="comment"># remove &lt;cls&gt; &lt;sep&gt;</span></span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> full_sequence_embed:</span><br><span class="line">                            seqembed = seqembed.mean(<span class="number">0</span>)</span><br><span class="line">                        embed_dict[protein_id] = seqembed</span><br><span class="line">        <span class="keyword">return</span> embed_dict</span><br></pre></td></tr></table></figure>

<h3 id="1-初始化方法-init"><a href="#1-初始化方法-init" class="headerlink" title="1. 初始化方法 (__init__):"></a>1. <strong>初始化方法 (<code>__init__</code>):</strong></h3><ul>
<li><p>参数</p>
<p>：</p>
<ul>
<li><code>batch_size</code>: 批次大小。</li>
<li><code>model_config_file</code>: 模型配置文件（如果有）。</li>
<li><code>full_sequence_embed</code>: 是否返回整个序列的嵌入。</li>
<li><code>no_cuda</code>: 是否禁用 CUDA。</li>
<li><code>seed</code>: 随机种子。</li>
<li><code>tokenizer</code>: 分词器类型。</li>
<li><code>num_workers</code>: 用于数据加载的工作进程数。</li>
<li><code>log_level</code>: 日志级别。</li>
<li><code>progress_bar</code>: 是否显示进度条。</li>
</ul>
</li>
<li><p>功能</p>
<p>：</p>
<ul>
<li>设置分布式计算环境和随机种子。</li>
<li>从预训练的权重中加载 ProteinBertModel。</li>
<li>初始化 <code>ForwardRunner</code>，用于模型推理。</li>
<li>存储相关参数和对象。</li>
</ul>
</li>
</ul>
<h3 id="2-编码方法-encode"><a href="#2-编码方法-encode" class="headerlink" title="2. 编码方法 (encode):"></a>2. <strong>编码方法 (<code>encode</code>):</strong></h3><ul>
<li><strong>参数</strong>：<code>sequences</code>，要编码的等长序列列表。</li>
<li><strong>返回</strong>：嵌入编码的 NumPy 数组，形状为 (#sequences, length of sequences, embedding dim)。</li>
<li><strong>功能</strong>：将给定的蛋白质序列转换为嵌入向量。</li>
</ul>
<h3 id="3-TAPE-嵌入方法-tape-embed"><a href="#3-TAPE-嵌入方法-tape-embed" class="headerlink" title="3. TAPE 嵌入方法 (tape_embed):"></a>3. <strong>TAPE 嵌入方法 (<code>tape_embed</code>):</strong></h3><ul>
<li><p><strong>参数</strong>：<code>seq_df</code>，一个包含至少两列（<code>ID</code> 和 <code>sequence</code>）的 pandas 数据框。</p>
</li>
<li><p><strong>返回</strong>：一个字典，其中键是 ID，值是嵌入向量。嵌入向量是具有形状 (#sequences, length of sequences, embedding dim) 的 PyTorch 张量。</p>
</li>
<li><p>功能</p>
<p>：</p>
<ul>
<li>创建一个嵌入数据集并设置数据加载器。</li>
<li>通过 TAPE 的 ProteinBertModel 运行前向传播来生成嵌入。</li>
<li>根据需要，可以选择返回整个序列的嵌入或返回序列的平均嵌入。</li>
</ul>
</li>
</ul>
<h2 id="CCMPred编码局部变量"><a href="#CCMPred编码局部变量" class="headerlink" title="CCMPred编码局部变量"></a>CCMPred编码局部变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@numba.njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">all_sequence_pairwise_profile</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    e: 4-d (L, L, 21, 21) np.array where e(i, j, a, b) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion and `b` at the j-th position.</span></span><br><span class="line"><span class="string">           Amino acids are encoded by 0-20 using the same index in CCMPred.</span></span><br><span class="line"><span class="string">    index_encoded: (N, L) array of index-encoded sequence using CCMPred index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    encoding: (N, L, L) array of sequence encoding. Each amino acid in a sequence</span></span><br><span class="line"><span class="string">    is encoded by values in the pairwise epsilon value table of CCMPred.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    e, index_encoded = args</span><br><span class="line">    N, L = index_encoded.shape</span><br><span class="line">    encoding = np.zeros((N, L, L))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L - <span class="number">1</span>):</span><br><span class="line">            a = index_encoded[k, i]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, L):</span><br><span class="line">                b = index_encoded[k, j]</span><br><span class="line">                encoding[k, i, j] = e[i, j, a, b]</span><br><span class="line">        encoding[k] += encoding[k].T</span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"><span class="meta">@numba.njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">all_sequence_singleton_profile</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    ei: 2-d (L, 20) np.array where e(i, a) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion.</span></span><br><span class="line"><span class="string">           Amino acids are encoded by 0-20 using the same index in CCMPred.</span></span><br><span class="line"><span class="string">    index_encoded: (N, L) array of index-encoded sequence using CCMPred index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    encoding: (N, L, 1) array of sequence encoding. Each amino acid in a sequence</span></span><br><span class="line"><span class="string">    is encoded by values in the singleton epsilon value table of CCMPred.    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    e, index_encoded = args</span><br><span class="line">    N, L = index_encoded.shape</span><br><span class="line">    encoding = np.zeros((N, L, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">            a = index_encoded[k, i]</span><br><span class="line">            encoding[k, i] = e[i, a]</span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CCMPredEncoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ccmpred_output=<span class="literal">None</span>, seq_len=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        brawfile: path to msgpack file storing the (L, L, 21, 21) table of</span></span><br><span class="line"><span class="string">                  CCMPred epsilon values of a target sequence</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line">        self.vocab_index = vocab.CCMPRED_AMINO_ACID_INDEX</span><br><span class="line">        brawfile = pathlib.Path(ccmpred_output)</span><br><span class="line">        self.eij, self.ei = self.load_data(brawfile)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">self, brawfile</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        eij: 4-d (L, L, 21, 21) np.array where e(i, j, a, b) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion and `b` at the j-th position.</span></span><br><span class="line"><span class="string">        ei: 2-d (L, 20) np.array where e(i, a) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion</span></span><br><span class="line"><span class="string">           Amino acids are encoded by 0-20 using the same index in CCMPred.</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> brawfile.exists():</span><br><span class="line">            <span class="keyword">raise</span> FileNotFoundError(brawfile)</span><br><span class="line">        data = msgpack.unpack(<span class="built_in">open</span>(brawfile, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">        L = self.seq_len</span><br><span class="line">        V = <span class="built_in">len</span>(self.vocab_index)</span><br><span class="line">        eij = np.zeros((L, L, V, V))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L - <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, L):</span><br><span class="line">                arr = np.array(data[<span class="string">b&#x27;x_pair&#x27;</span>][<span class="string">b&#x27;%d/%d&#x27;</span>%(i, j)][<span class="string">b&#x27;x&#x27;</span>]).reshape(V, V)</span><br><span class="line">                eij[i, j] = arr</span><br><span class="line">                eij[j, i] = arr.T</span><br><span class="line"></span><br><span class="line">        ei = np.array(data[<span class="string">b&#x27;x_single&#x27;</span>]).reshape(L, V - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> eij, ei</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">index_encoding</span>(<span class="params">self, sequences, letter_to_index_dict</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Modified from https://github.com/openvax/mhcflurry/blob/master/mhcflurry/amino_acid.py#L110-L130</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        sequences: list of equal-length sequences</span></span><br><span class="line"><span class="string">        letter_to_index_dict: char -&gt; int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        np.array with shape (#sequences, length of sequences)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        df = pd.DataFrame(<span class="built_in">iter</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> sequences)</span><br><span class="line">        encoding = df.replace(letter_to_index_dict)</span><br><span class="line">        encoding = encoding.values.astype(np.<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ccmpred_encoding</span>(<span class="params">self, index_encoded, profile=<span class="string">&#x27;pair&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">if</span> profile == <span class="string">&#x27;pair&#x27;</span>:</span><br><span class="line">            encoding = all_sequence_pairwise_profile((self.eij, index_encoded))</span><br><span class="line">        <span class="keyword">elif</span> profile == <span class="string">&#x27;single&#x27;</span>:</span><br><span class="line">            encoding = all_sequence_singleton_profile((self.ei, index_encoded))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, sequences, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        encoding: (N, L, L + 1) array of sequence encoding. Each amino acid in a sequence</span></span><br><span class="line"><span class="string">        is encoded by values in the singble and pairwise epsilon value tables of CCMPred</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        index_encoded = self.index_encoding(sequences, self.vocab_index)        </span><br><span class="line">        single = self.ccmpred_encoding(index_encoded, profile=<span class="string">&#x27;single&#x27;</span>)</span><br><span class="line">        pair = self.ccmpred_encoding(index_encoded, profile=<span class="string">&#x27;pair&#x27;</span>)</span><br><span class="line">        self.ccmpred_encoded = np.concatenate([single, pair], axis=<span class="number">2</span>)        </span><br><span class="line">        <span class="keyword">return</span> self.ccmpred_encoded</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="1-函数-all-sequence-pairwise-profile："><a href="#1-函数-all-sequence-pairwise-profile：" class="headerlink" title="1. 函数 all_sequence_pairwise_profile："></a>1. <strong>函数 <code>all_sequence_pairwise_profile</code>：</strong></h3><ul>
<li><strong>参数</strong>：一个包含 CCMPred 的 �<em>ε</em> 值和索引编码序列的元组。</li>
<li><strong>返回</strong>：一个编码数组，形状为 (�,�,�)(<em>N</em>,<em>L</em>,<em>L</em>)。</li>
<li><strong>功能</strong>：计算所有序列的两两配对特征。</li>
</ul>
<h3 id="2-函数-all-sequence-singleton-profile："><a href="#2-函数-all-sequence-singleton-profile：" class="headerlink" title="2. 函数 all_sequence_singleton_profile："></a>2. <strong>函数 <code>all_sequence_singleton_profile</code>：</strong></h3><ul>
<li><strong>参数</strong>：一个包含 CCMPred 的 �<em>ε</em> 值和索引编码序列的元组。</li>
<li><strong>返回</strong>：一个编码数组，形状为 (�,�,1)(<em>N</em>,<em>L</em>,1)。</li>
<li><strong>功能</strong>：计算所有序列的单体特征。</li>
</ul>
<h3 id="3-类-CCMPredEncoder："><a href="#3-类-CCMPredEncoder：" class="headerlink" title="3. 类 CCMPredEncoder："></a>3. <strong>类 <code>CCMPredEncoder</code>：</strong></h3><ul>
<li>方法 <code>__init__</code>：<ul>
<li>初始化类，加载 CCMPred 输出文件并设置词汇表索引。</li>
</ul>
</li>
<li>方法 <code>load_data</code>：<ul>
<li>从给定的文件中加载 CCMPred 数据并返回两个数组，分别表示两两配对和单体特征。</li>
</ul>
</li>
<li>方法 <code>index_encoding</code>：<ul>
<li>将给定的序列列表转换为整数索引编码。</li>
</ul>
</li>
<li>方法 <code>ccmpred_encoding</code>：<ul>
<li>根据 CCMPred 的两两配对或单体特征计算序列的编码。</li>
</ul>
</li>
<li>方法 <code>encode</code>：<ul>
<li>返回序列的 CCMPred 编码，将单体和两两配对特征组合在一起。</li>
</ul>
</li>
</ul>
<p>该代码部分的核心目标是使用 CCMPred 的共进化信息为蛋白质序列生成特征编码。共进化信息可以捕获蛋白质残基之间的相互作用和依赖性，对于许多下游分析和预测任务（例如，三维结构预测）可能是有用的。通过使用 Numba（一个用于编译 Python 的高性能库）进行即时编译，相关的数值计算得到了优化。</p>
<h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Modified from Annotated Transformer</span></span><br><span class="line"><span class="string">    http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">1024</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEmbedding, self).__init__()</span><br><span class="line">        pe = torch.zeros((max_len, d_model), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InputPositionEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size=<span class="literal">None</span>, embed_dim=<span class="literal">None</span>, dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                init_weight=<span class="literal">None</span>, seq_len=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InputPositionEmbedding, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.position_embed = PositionalEmbedding(embed_dim, max_len=seq_len)</span><br><span class="line">        self.reproject = nn.Identity()</span><br><span class="line">        <span class="keyword">if</span> init_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embed = nn.Embedding.from_pretrained(init_weight)</span><br><span class="line">            self.reproject = nn.Linear(init_weight.size(<span class="number">1</span>), embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x = self.embed(inputs)</span><br><span class="line">        x = x + self.position_embed(inputs)</span><br><span class="line">        x = self.reproject(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AggregateLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="literal">None</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AggregateLayer, self).__init__()        </span><br><span class="line">        self.attn = nn.Sequential(collections.OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;layernorm&#x27;</span>, nn.LayerNorm(d_model)),</span><br><span class="line">            (<span class="string">&#x27;fc&#x27;</span>, nn.Linear(d_model, <span class="number">1</span>, bias=<span class="literal">False</span>)),</span><br><span class="line">            (<span class="string">&#x27;dropout&#x27;</span>, nn.Dropout(dropout)),</span><br><span class="line">            (<span class="string">&#x27;softmax&#x27;</span>, nn.Softmax(dim=<span class="number">1</span>))</span><br><span class="line">        ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, context</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        context: token embedding from encoder (Transformer/LSTM)</span></span><br><span class="line"><span class="string">                (batch_size, seq_len, embed_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        weight = self.attn(context)</span><br><span class="line">        <span class="comment"># (batch_size, seq_len, embed_dim).T * (batch_size, seq_len, 1) *  -&gt;</span></span><br><span class="line">        <span class="comment"># (batch_size, embed_dim, 1)</span></span><br><span class="line">        output = torch.bmm(context.transpose(<span class="number">1</span>, <span class="number">2</span>), weight)</span><br><span class="line">        output = output.squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GlobalPredictor</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="literal">None</span>, d_h=<span class="literal">None</span>, d_out=<span class="literal">None</span>, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GlobalPredictor, self).__init__()</span><br><span class="line">        self.predict_layer = nn.Sequential(collections.OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;batchnorm&#x27;</span>, nn.BatchNorm1d(d_model)),</span><br><span class="line">            (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(d_model, d_h)),</span><br><span class="line">            (<span class="string">&#x27;tanh&#x27;</span>, nn.Tanh()),</span><br><span class="line">            (<span class="string">&#x27;dropout&#x27;</span>, nn.Dropout(dropout)),</span><br><span class="line">            (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(d_h, d_out))</span><br><span class="line">        ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.predict_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SequenceLSTM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Container module with an encoder, a recurrent module, and a decoder.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_input=<span class="literal">None</span>, d_embed=<span class="number">20</span>, d_model=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                vocab_size=<span class="literal">None</span>, seq_len=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                dropout=<span class="number">0.1</span>, lstm_dropout=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                nlayers=<span class="number">1</span>, bidirectional=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                proj_loc_config=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SequenceLSTM, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embed = InputPositionEmbedding(vocab_size=vocab_size,</span><br><span class="line">                    seq_len=seq_len, embed_dim=d_embed)</span><br><span class="line"></span><br><span class="line">        self.lstm = nn.LSTM(input_size=d_input,</span><br><span class="line">                            hidden_size=d_model//<span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> d_model,</span><br><span class="line">                            num_layers=nlayers, dropout=lstm_dropout,</span><br><span class="line">                            bidirectional=bidirectional)</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.proj_loc_layer = proj_loc_config[<span class="string">&#x27;layer&#x27;</span>](</span><br><span class="line">                    proj_loc_config[<span class="string">&#x27;d_in&#x27;</span>], proj_loc_config[<span class="string">&#x27;d_out&#x27;</span>]</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, loc_feat=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.embed(x)</span><br><span class="line">        <span class="keyword">if</span> loc_feat <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loc_feat = self.proj_loc_layer(loc_feat)</span><br><span class="line">            x = torch.cat([x, loc_feat], dim=<span class="number">2</span>)</span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        x, _ = self.lstm(x)</span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMPredictor</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_embed=<span class="number">20</span>, d_model=<span class="number">128</span>, d_h=<span class="number">128</span>, d_out=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                vocab_size=<span class="literal">None</span>, seq_len=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                dropout=<span class="number">0.1</span>, lstm_dropout=<span class="number">0</span>, nlayers=<span class="number">1</span>, bidirectional=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                use_loc_feat=<span class="literal">True</span>, use_glob_feat=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                proj_loc_config=<span class="literal">None</span>, proj_glob_config=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMPredictor, self).__init__()</span><br><span class="line">        self.seq_lstm = SequenceLSTM(</span><br><span class="line">            d_input=d_embed + (proj_loc_config[<span class="string">&#x27;d_out&#x27;</span>] <span class="keyword">if</span> use_loc_feat <span class="keyword">else</span> <span class="number">0</span>),</span><br><span class="line">            d_embed=d_embed, d_model=d_model,</span><br><span class="line">            vocab_size=vocab_size, seq_len=seq_len,</span><br><span class="line">            dropout=dropout, lstm_dropout=lstm_dropout,</span><br><span class="line">            nlayers=nlayers, bidirectional=bidirectional,</span><br><span class="line">            proj_loc_config=proj_loc_config)</span><br><span class="line">        self.proj_glob_layer = proj_glob_config[<span class="string">&#x27;layer&#x27;</span>](</span><br><span class="line">            proj_glob_config[<span class="string">&#x27;d_in&#x27;</span>], proj_glob_config[<span class="string">&#x27;d_out&#x27;</span>]</span><br><span class="line">        )</span><br><span class="line">        self.aggragator = AggregateLayer(</span><br><span class="line">            d_model = d_model + (proj_glob_config[<span class="string">&#x27;d_out&#x27;</span>] <span class="keyword">if</span> use_glob_feat <span class="keyword">else</span> <span class="number">0</span>))</span><br><span class="line">        self.predictor = GlobalPredictor(</span><br><span class="line">            d_model = d_model + (proj_glob_config[<span class="string">&#x27;d_out&#x27;</span>] <span class="keyword">if</span> use_glob_feat <span class="keyword">else</span> <span class="number">0</span>),</span><br><span class="line">            d_h=d_h, d_out=d_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, glob_feat=<span class="literal">None</span>, loc_feat=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.seq_lstm(x, loc_feat=loc_feat)</span><br><span class="line">        <span class="keyword">if</span> glob_feat <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            glob_feat = self.proj_glob_layer(glob_feat)</span><br><span class="line">            x = torch.cat([x, glob_feat], dim=<span class="number">2</span>)</span><br><span class="line">        x = self.aggragator(x)</span><br><span class="line">        output = self.predictor(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="1-PositionalEmbedding-类"><a href="#1-PositionalEmbedding-类" class="headerlink" title="1. PositionalEmbedding 类"></a>1. <code>PositionalEmbedding</code> 类</h3><ul>
<li><p><strong>功能</strong>：创建了一个位置嵌入矩阵，用于向序列的每个位置添加独特的嵌入。</p>
</li>
<li><p>属性</p>
<p>：</p>
<ul>
<li><code>pe</code>：存储位置嵌入的张量。</li>
</ul>
</li>
<li><p><strong>原理</strong>：使用 sine 和 cosine 函数生成位置嵌入，使得每个位置的嵌入都是唯一的。</p>
</li>
</ul>
<h3 id="2-InputPositionEmbedding-类"><a href="#2-InputPositionEmbedding-类" class="headerlink" title="2. InputPositionEmbedding 类"></a>2. <code>InputPositionEmbedding</code> 类</h3><ul>
<li><p><strong>功能</strong>：将输入的词汇表索引转换为连续的嵌入表示，并添加位置嵌入。</p>
</li>
<li><p>属性和组件</p>
<p>：</p>
<ul>
<li><code>embed</code>：一个嵌入层，用于将输入的词汇表索引转换为连续的向量表示。</li>
<li><code>position_embed</code>：一个 <code>PositionalEmbedding</code> 层，用于添加位置信息。</li>
<li><code>reproject</code>：一个线性层，用于重新投影嵌入空间（如果需要）。</li>
<li><code>dropout</code>：Dropout 层，用于正则化。</li>
</ul>
</li>
</ul>
<h3 id="3-AggregateLayer-类"><a href="#3-AggregateLayer-类" class="headerlink" title="3. AggregateLayer 类"></a>3. <code>AggregateLayer</code> 类</h3><ul>
<li><p><strong>功能</strong>：聚合编码后的序列表示。</p>
</li>
<li><p>组件</p>
<p>：</p>
<ul>
<li><code>attn</code>：一个包括层归一化、全连接层、Dropout 层和 Softmax 层的序列，用于计算每个位置的权重，然后通过加权平均进行聚合。</li>
</ul>
</li>
</ul>
<h3 id="4-GlobalPredictor-类"><a href="#4-GlobalPredictor-类" class="headerlink" title="4. GlobalPredictor 类"></a>4. <code>GlobalPredictor</code> 类</h3><ul>
<li><p><strong>功能</strong>：从聚合后的表示生成最终的预测输出。</p>
</li>
<li><p>组件</p>
<p>：</p>
<ul>
<li><code>predict_layer</code>：一个包括批归一化、全连接层、激活函数和 Dropout 层的序列，用于最终的预测。</li>
</ul>
</li>
</ul>
<h3 id="5-SequenceLSTM-类"><a href="#5-SequenceLSTM-类" class="headerlink" title="5. SequenceLSTM 类"></a>5. <code>SequenceLSTM</code> 类</h3><ul>
<li><p><strong>功能</strong>：使用 LSTM 处理序列输入并生成连续的序列表示。</p>
</li>
<li><p>属性和组件</p>
<p>：</p>
<ul>
<li><code>embed</code>：一个 <code>InputPositionEmbedding</code> 层，用于输入和位置嵌入。</li>
<li><code>lstm</code>：LSTM 层，用于编码序列。</li>
<li><code>drop</code>：Dropout 层，用于正则化。</li>
<li><code>proj_loc_layer</code>：可选的全连接层，用于投影局部特征。</li>
</ul>
</li>
</ul>
<h3 id="6-LSTMPredictor-类"><a href="#6-LSTMPredictor-类" class="headerlink" title="6. LSTMPredictor 类"></a>6. <code>LSTMPredictor</code> 类</h3><ul>
<li><p><strong>功能</strong>：整个 LSTM 预测模型的容器。</p>
</li>
<li><p>属性和组件</p>
<p>：</p>
<ul>
<li><code>seq_lstm</code>：一个 <code>SequenceLSTM</code> 层，用于序列编码。</li>
<li><code>proj_glob_layer</code>：全连接层，用于投影全局特征。</li>
<li><code>aggregator</code>：一个 <code>AggregateLayer</code> 层，用于聚合编码后的序列表示。</li>
<li><code>predictor</code>：一个 <code>GlobalPredictor</code> 层，用于最终的预测。</li>
</ul>
</li>
</ul>

</article>
    
    <div class="trm-reward">
        
            <span class="trm-reward-btn trm-glow" onclick='var qr = document.getElementById("qr"); qr.style.display = (qr.style.display === "none") ? "block" : "none";'>
                <i class="iconfont icon-dashang"></i>
            </span>
        
        <p class="trm-reward-comment">I'm so cute. Please give me money.</p>
        <div id="qr" style="display:none;">
            
                <div style="display:inline-block">
                    <a rel="noopener noreferrer" href='' target='_blank' >
                       <img src="/null" alt="支付宝" loading="lazy">
                    </a>
                    <p>支付宝</p>
                </div>
            
        </div>
    </div>

    

</div>
<div id="post-next-prev" class="row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation" data-scroll data-scroll-offset="40">
        <a href="/2023/08/18/Python%E5%85%A5%E9%97%A81/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="/img/block.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/%E6%8A%80%E6%9C%AF%E7%B1%BB/">
                    技术类
                </a>
            </div>
            <h5>
                <a href="/2023/08/18/Python%E5%85%A5%E9%97%A81/" class="trm-anima-link">
                    Python入门1
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>23/08/18</li>
                <li>10:55</li>
                
                    <li>125</li>
                
                
                    <li>1</li>
                
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="col-lg-6">
    <div class="trm-blog-card trm-scroll-animation" data-scroll data-scroll-offset="40">
        <a href="/2023/08/10/ECnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89/" class="trm-cover-frame trm-anima-link">
            
            
                <img alt="cover" class="no-fancybox" src="/img/block.jpg">
            
        </a>
        
        <div class="trm-card-descr">
            <div class="trm-label trm-category trm-mb-20">
                <a href=" /categories/%E6%8A%80%E6%9C%AF%E7%B1%BB/">
                    技术类
                </a>
            </div>
            <h5>
                <a href="/2023/08/10/ECnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89/" class="trm-anima-link">
                    ECnet代码解读（二）
                </a>
            </h5>
            <div class="trm-divider trm-mb-20 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>23/08/10</li>
                <li>17:07</li>
                
                    <li>2.9k</li>
                
                
                    <li>13</li>
                
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-scroll-animation" data-scroll data-scroll-offset="50">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v6.3.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.0.9
            </span>
        </div>
      

     

     
</footer>
 
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            <div class="trm-fixed-container" data-scroll data-scroll-sticky data-scroll-target=".locomotive-scroll__sticky-target" data-scroll-offset="-10">
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont icon-read"></i>
        </div>
    
    
        <div class="trm-fixed-btn" data-title="单栏和双栏切换" onclick="asyncFun.switchSingleColumn()">
            <i class="iconfont icon-shangxiazhankai"></i>
        </div>
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
          </div>
        </div>
      </div>
      <!-- scroll container end -->

  </div>
  <!-- app wrapper end -->

  
    <div class="trm-search-popup">
        <div class="trm-search-wrapper">
            <div class="form trm-search-form">
                <div class="trm-search-input-icon">
                    <i class="iconfont icon-sousuo"></i>
                </div>
                <input class="trm-search-input" type="text" placeholder="搜索文章...">
                <div class="trm-search-btn-close">
                    <i class="iconfont icon-anniu_guanbi"></i>
                </div>
            </div>
            <div class="trm-search-result-container">
                <div class="trm-search-empty">
                    请输入关键词进行搜索
                </div>
            </div>
            <div class="trm-search-footer">
                <div class="trm-search-stats"></div>
                <ul class="trm-search-commands">
                    <li>
                        <kbd class="command-palette-commands-key">
                            <svg width="15" height="15" aria-label="Escape key" role="img">
                                <g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"
                                    stroke-width="1.2">
                                    <path
                                        d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993 0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016.8634 0 1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5c.032.5663-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864 0 1.6425 1.031 1.5443 2.2492h-2.956">
                                    </path>
                                </g>
                            </svg>
                        </kbd>
                        <span class="command-palette-Label">to close</span>
                    </li>
                </ul>
            </div>
        </div>
    </div>

  <!-- Plugin -->




    
    
<script src="https://npm.elemecdn.com/locomotive-scroll@4.1.4/dist/locomotive-scroll.min.js"></script>

    
<script src="https://npm.elemecdn.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    
        <script src="/js/plugins/typing.js?v=2.0.9"></script>
    

    
        
<script src="https://npm.elemecdn.com/hexo-generator-searchdb@1.4.0/dist/search.js"></script>

        <script src="/js/plugins/local_search.js?v=2.0.9"></script>
    

    <!-- 数学公式 -->
    

    <!-- 评论插件 -->
    
        

        
    



<!-- CDN -->


    

    

    




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.0.9"></script>

<script src="https://unpkg.com/live2d-widget@^3.1.3/lib/L2Dwidget.min.js"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"https://unpkg.com/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>

</html>