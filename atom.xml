<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>The Cabin of Hamzone</title>
  
  <subtitle>脏脏的小屋</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-07-26T10:42:22.738Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Li Hanzhang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>文献阅读：davidBaker组RFdiffusion</title>
    <link href="http://example.com/2023/07/24/Baker%E7%BB%84RFdiffusion/"/>
    <id>http://example.com/2023/07/24/Baker%E7%BB%84RFdiffusion/</id>
    <published>2023-07-24T05:28:29.000Z</published>
    <updated>2023-07-26T10:42:22.738Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章总述"><a href="#文章总述" class="headerlink" title="文章总述"></a>文章总述</h1><ul><li><p>论文介绍了一种基于深度学习的蛋白质设计框架，称为 RFdiffusion，它利用了 RoseTTAFold 结构预测网络的能力，通过逆向去噪过程生成多样化和精确的蛋白质结构。</p></li><li><p>论文展示了 RFdiffusion 在无条件和有条件的蛋白质单体设计、蛋白质结合物设计、对称寡聚体设计、酶活性位点支架设计和对称功能基团支架设计等多个方面的优异性能。</p></li><li><p>论文通过实验验证了数百个设计的对称组装体、金属结合蛋白和蛋白质结合物的结构和功能，并通过冷冻电镜解析了一个设计的结合物与流感血凝素的复合物结构，证明了 RFdiffusion 可以以原子级精度设计功能性蛋白质。</p></li><li><p>论文讨论了 RFdiffusion 的优势和局限，以及未来可能的扩展方向，例如将其应用于核酸和配体结合蛋白的设计，以及利用外部势能和微调技术来定制特定的设计挑战。</p><h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><ul><li><p>原理：RFdiffusion 的基本思想是利用一个去噪网络将随机状态映射到一个低维流形上，该流形包含了所有可能的合理的蛋白质结构。通过在流形上进行随机游走，RFdiffusion 可以探索出各种不同的蛋白质结构，并通过条件信息来指导生成过程，以满足特定的设计目标。去噪网络的作用是将输入的状态修正为更接近真实蛋白质结构的状态，从而消除噪声和不合理的结构。条件信息的作用是提供一些额外的约束或指示，例如拓扑类型、对称性、功能基团、结合伙伴等，从而使生成的结构符合设计要求。RFdiffusion 的核心假设是存在一个从随机状态到真实蛋白质结构的映射关系，该映射关系可以由去噪网络来近似学习和表示。通过逆向扩散过程，RFdiffusion 可以从任意初始状态出发，沿着梯度方向逐步接近真实蛋白质结构，同时保持多样性和创新性。</p></li><li><p>实现方法：RFdiffusion 的实现方法主要包括以下几个步骤：</p><ul><li>建立去噪网络：RFdiffusion 使用了 RoseTTAFold 结构预测网络作为去噪网络，因为它已经被证明能够根据蛋白质序列预测出其三维结构，并达到接近实验水平的精度。RoseTTAFold 结构预测网络由三个模块组成：一个序列编码器、一个三维距离预测器和一个三维坐标预测器。序列编码器将蛋白质序列转换为一个高维特征向量，三维距离预测器将特征向量转换为一个距离矩阵，三维坐标预测器将距离矩阵转换为一个坐标矩阵。RFdiffusion 只使用了 RoseTTAFold 的最后一个模块作为去噪网络，即三维坐标预测器，它可以将任意输入的坐标矩阵修正为更接近真实蛋白质结构的坐标矩阵。</li><li>训练去噪网络：RFdiffusion 使用了与 RoseTTAFold 相同的数据集来训练去噪网络，即 Protein Data Bank（PDB）中的所有已知蛋白质结构。训练过程中，RFdiffusion 对每个蛋白质结构添加一些随机扰动或噪声，然后将扰动后的坐标矩阵作为输入，将原始的坐标矩阵作为输出，计算两者之间的均方误差（MSE）作为损失函数，并使用反向传播算法更新去噪网络的参数。训练目标是使去噪网络能够尽可能地恢复原始的蛋白质结构，并消除输入中的噪声和不合理性。</li><li>生成蛋白质结构：RFdiffusion 使用了一种基于 Langevin 动力学（LD）的生成过程来生成蛋白质结构。LD 是一种描述粒子在随机力场中运动的物理模型，它可以用来模拟蛋白质的折叠过程。RFdiffusion 将去噪网络的输出视为一个势能函数，将输入的坐标矩阵视为一个粒子状态，然后使用 LD 方程来更新粒子状态，从而实现在流形上的随机游走。LD 方程的形式如下：</li></ul><p>!LD equation</p><p>其中，!x_t 是粒子状态，!U(x_t) 是势能函数，!T 是温度参数，!\xi_t 是高斯白噪声。RFdiffusion 使用了 Euler-Maruyama 方法来离散化 LD 方程，并使用梯度下降法来计算势能函数的梯度。生成过程中，RFdiffusion 从一个随机初始化的坐标矩阵开始，然后重复以下步骤：</p><ul><li><p>将当前的坐标矩阵输入去噪网络，得到修正后的坐标矩阵；</p></li><li><p>计算修正后的坐标矩阵和当前的坐标矩阵之间的梯度；</p></li><li><p>根据 LD 方程更新当前的坐标矩阵，并添加一些随机扰动；</p></li><li><p>如果有条件信息，则将当前的坐标矩阵与条件信息进行对齐或匹配；</p></li><li><p>如果满足停止条件，则结束生成过程，否则继续重复以上步骤。</p></li><li><p>设计蛋白质序列：RFdiffusion 使用了另一个基于深度学习的网络，称为 ProteinMPNN，来设计与生成的蛋白质结构匹配的蛋白质序列。ProteinMPNN 是一种基于消息传递神经网络（MPNN）的序列设计网络，它可以根据蛋白质结构和条件信息预测出最优的蛋白质序列。ProteinMPNN 的输入是一个由残基类型、距离、角度、二级结构等特征组成的特征矩阵，以及一个由拓扑类型、对称性、功能基团等特征组成的条件向量。ProteinMPNN 的输出是一个由残基类型概率分布组成的序列矩阵。ProteinMPNN 的训练数据是从 PDB 中提取的所有已知蛋白质结构和序列，以及一些人工合成的条件信息。ProteinMPNN 的训练目标是使预测出的序列与给定的结构和条件信息尽可能地匹配，并具有高度的可行性和稳定性。设计过程中，RFdiffusion 将生成的蛋白质结构和条件信息输入 ProteinMPNN，得到预测出的序列矩阵，然后从每个位置上选择最大概率的残基类型作为最终的序列。如果需要，RFdiffusion 还可以对预测出的序列进行一些后处理，例如添加或删除一些残基，以满足一些额外的约束或优化一些性能指标。</p><h2 id="蛋白设计示例"><a href="#蛋白设计示例" class="headerlink" title="蛋白设计示例"></a>蛋白设计示例</h2><p>论文展示了RFdiffusion在以下几个方面的优异性能：</p><ul><li>无条件的蛋白质单体设计：RFdiffusion可以从随机噪声开始，生成多种复杂的蛋白质结构，包括α螺旋、β折叠和混合α&#x2F;β拓扑，这些结构与AlphaFold2和ESMFold预测的结构非常接近，而且与已知蛋白质结构有很大差异，表明了模型的创新能力。论文还实验验证了一些设计的蛋白质具有高度稳定性和正确的二级结构。</li><li>有条件的蛋白质单体设计：RFdiffusion可以根据用户指定的二级结构和&#x2F;或拓扑信息生成符合要求的蛋白质结构，例如TIM桶或NTF2折叠。论文也实验验证了一些设计的TIM桶具有高度稳定性和正确的二级结构。</li><li>蛋白质结合物设计：RFdiffusion可以根据目标蛋白质结构信息和界面热点残基生成新颖的高亲和力结合物。论文展示了对五个不同目标（流感病毒血凝素、IL-7Rα、PD-L1、胰岛素受体和TrkA）设计并实验筛选出具有纳摩尔级亲和力的结合物，并且通过冷冻电镜解析了一个流感病毒血凝素结合物与目标复合物的高分辨率结构，与设计模型几乎完全一致。</li><li>对称寡聚体设计：RFdiffusion可以根据用户指定的对称性生成具有任意点群对称性（如循环、二面角、四面体、八面体或二十面体）的寡聚体结构。论文展示了多种复杂而新颖的对称寡聚体结构，并且通过电镜验证了它们具有正确的寡聚化状态和形状。</li><li>酶活性位点支架设计：RFdiffusion可以根据用户指定的酶活性位点（包括多个残基和背景原子）生成能够精确固定这些位点在空间中位置和取向的支架蛋白质。论文展示了对多种酶类别（如水解酶、转移酶、还原酶等）进行活性位点支架设计，并且通过AlphaFold2验证了设计模型的准确性。</li><li>对称功能基团支架设计：RFdiffusion可以根据用户指定的对称功能基团（如金属配位位点或病毒抗原表位）生成能够精确固定这些基团在空间中位置和取向的对称寡聚体结构。论文展示了对四面体Ni2+配位位点和三聚体SARS-CoV-2刺突蛋白结合位点进行对称功能基团支架设计，并且通过热力学和电镜验证了设计蛋白质的功能和结构。</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;文章总述&quot;&gt;&lt;a href=&quot;#文章总述&quot; class=&quot;headerlink&quot; title=&quot;文章总述&quot;&gt;&lt;/a&gt;文章总述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;论文介绍了一种基于深度学习的蛋白质设计框架，称为 RFdiffusion，它利用了 RoseTTAFold</summary>
      
    
    
    
    <category term="文献阅读类" scheme="http://example.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>文献阅读林一瀚老师组：Modulating gene regulation function by chemically controlled transcription factor clustering</title>
    <link href="http://example.com/2023/07/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    <id>http://example.com/2023/07/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/</id>
    <published>2023-07-23T05:59:39.000Z</published>
    <updated>2023-07-23T08:26:18.800Z</updated>
    
    <content type="html"><![CDATA[<h1 id="文章总述"><a href="#文章总述" class="headerlink" title="文章总述"></a>文章总述</h1><p>​ 这篇文章的主题是通过化学控制转录因子（TF）聚集来调节基因调控功能。研究人员构建了合成的转录因子，其聚集行为可以通过化学方式控制。通过调整系统的单一参数（即，转录因子聚集倾向），他们提供了证据支持转录因子聚集直接激活和放大目标基因。单基因成像表明，这种放大结果来自转录动态的调节。重要的是，转录因子聚集倾向通过显著调节有效的转录因子结合亲和力，以及在较小程度上调节超敏感性，从而调节基因调控功能，这有助于双峰性和持续反应行为，这些都让人联想到典型的细胞命运控制系统。总的来说，这些结果表明，转录因子聚集可以调节基因调控功能，以实现突现行为，并强调了化学控制蛋白质聚集的潜在应用。</p><h2 id="化学调控系统的构建"><a href="#化学调控系统的构建" class="headerlink" title="化学调控系统的构建"></a>化学调控系统的构建</h2><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig1_HTML.png?as=webp" alt="image-20230723141553747" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p><ul><li><p>作者构建的系统在原有的雷帕霉素诱导系统上进行改造，通过Tag6和HOTag6的作用，使其在无雷帕霉素的存在时，将会形成四聚体，而存在雷帕霉素时，将会相互作用成簇。为了定量表达过程，作者通过和TF结合域（TetO）相连的iRFP和能形成茎环的24xPP7来作为报告系统。</p></li><li><p>之后通过探究雷帕霉素浓度对转录因子聚集的影响，通过使用U2OS细胞系，滴定从0nM到1000nM的雷帕霉素，之后在共聚焦显微镜下观察切片。通过数学建模和观察发现，随着雷帕霉素浓度的升高，细胞中聚类因子的打分（直径和数量&#x2F;细胞）都有上升。且在达到最大浓度的雷帕霉素时，聚类的直径平均曲线并未到达表观最大值，但另外两条曲线则达到了，说明随着浓度的升高，聚类倾向于形状变大而不是数量增多（当浓度超过100nM时）。</p><h2 id="TF聚类倾向直接影响目标蛋白表达水平"><a href="#TF聚类倾向直接影响目标蛋白表达水平" class="headerlink" title="TF聚类倾向直接影响目标蛋白表达水平"></a>TF聚类倾向直接影响目标蛋白表达水平</h2><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig2_HTML.png?as=webp" alt="image-2" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p></li><li><p>作者通过在三种细胞系中进行对照试验，变量包括DNA结合位点，TF结合位点数目，细胞系。通过对照试验发现目标蛋白表达水平受到雷帕霉素浓度的显著影响，这种影响不是因为其他别的因素引起的。</p></li><li><p>这些数据表明，TF表达扩增的聚类依赖于DNA结合位点的数量，这与最近的研究结果相一致。而且在低雷帕霉素的浓度下也可以产生表达扩增的现象，这种现象在目测情况下和无雷帕霉素的条件下产生的表达扩增无明显增加。而且表达扩增在比可视检测TF聚类饱和更低的雷帕霉素的浓度下饱和。<strong>说明基因表达的扩增可以由TF聚类来促进，但是存在上限，这种上限不是由聚类的大小和数量来决定的。</strong></p><h2 id="TF聚类可以直接激活和调节转录动态"><a href="#TF聚类可以直接激活和调节转录动态" class="headerlink" title="TF聚类可以直接激活和调节转录动态"></a>TF聚类可以直接激活和调节转录动态</h2><p><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig3_HTML.png?as=webp" alt="image" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p></li><li><p>作者想用PP7形成的茎环来作为报告，探究TF簇对靶基因的时空激活，因此作者想探究雷帕霉素依赖的TF簇能否可以以多西环素依赖的方式整合到靶基因位点上。而且因为可以观察同一细胞的转录信号，所以可以在时空上探索TF簇和基因结合位点的相互作用，最后，想通过不同浓度的雷帕霉素来探究靶基因在不同聚类倾向下的TF的作用下的转录水平</p></li><li><p>作者进行了一系列的实验，包括使用荧光原位杂交（FISH）技术来同时观察报告基因位点和转录因子聚集体，以及使用双色时间延迟成像来观察稳定整合的目标基因位点的新生转录信号和转录因子聚集体的信号。这些实验结果显示，转录因子聚集体可以特异性地与DNA结合，并且这种结合是可控的，这为合成转录因子聚集体的因果基因调控作用提供了证据。</p></li><li><p>作者发现，转录因子聚集体和新生转录信号之间存在空间上的相互作用，这些相互作用可能是由转录因子聚集体与报告基因位点的结合引起的。此外，他们还发现，转录因子聚集体的大小与新生转录信号的强度之间存在负相关性。</p></li><li><p>通过调整转录因子聚集体的倾向，作者发现，转录因子聚集体可以显著影响转录动态。具体来说，增加聚集倾向可以显著增加新生转录位点被检测到的时间比例，减少添加多西环素后到第一次转录爆发的时间，并导致更多的基因在同一调控元中被激活。</p></li><li><p>总的来说，这些数据提供了基于成像的证据，表明转录因子聚集体可以直接并增强地激活目标基因的转录。这些数据也暗示，通过增强转录因子的聚集倾向，细胞可以在不改变其表达水平的情况下激活特定调控元中的更多基因。</p></li><li><p>作者发现，当转录因子（TF）聚集倾向高时，基因调控功能可以被调节以实现双模态。他们使用流式细胞术来表征U2OS-7TetO细胞，发现输入信号在不同的雷帕霉素浓度下呈单峰分布，而输出信号在较高的雷帕霉素浓度下呈双峰分布。这些数据表明，当转录因子聚集倾向高时，基因调控功能可以被调节以实现双模态。</p></li><li><p>作者推测，基因调控功能的有效转录因子结合亲和力和超敏感性的调节可能导致双模态的出现。他们使用Hill函数拟合流式细胞术的输入-输出数据，并发现随着雷帕霉素浓度的增加，转录因子对DNA的有效结合亲和力（即，解离常数减小）和Hill系数都有所增加。</p></li><li><p>作者发现，转录因子聚集倾向可以调节基因调控功能的有效结合亲和力和超敏感性，其中有效结合亲和力的调节程度比超敏感性大得多，这表明在他们的系统中，转录因子聚集对基因调控功能的敏感性的影响大于对超敏感性的影响。</p></li><li><p>作者还发现，这些调节有效地导致输出信号的绝对细胞间变异性（即，标准差）增加，而输入信号的变异性在所有条件下都保持相对恒定。相比之下，相对细胞间变异性（即，变异系数或噪声）显示出非单调行为，这表明转录因子聚集可以调节基因表达噪声。</p></li></ul><p><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig4_HTML.png?as=webp" alt="image" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p><ul><li>作者通过一系列的计算模拟验证了他们的观察结果，这些模拟成功地复现了持续响应行为。他们发现，有效的转录因子结合亲和力的调节，而不是超敏感性的调节，对观察到的行为起主要作用。这些结果表明，转录因子聚集大大调节了基因调控功能的有效结合亲和力，从而导致观察到的持续响应（即，类似记忆的）行为。</li><li>在讨论部分，作者指出，越来越多的研究表明转录因子聚集在决定细胞命运中起关键作用，然而，转录因子聚集在基因调控中的定量作用仍然不太清楚。在这项工作中，作者通过合理设计一个具有化学可调转录因子聚集的自下而上的合成基因调控系统，阐明了转录因子聚集在激活和放大基因转录以及调节基因调控功能中的作用。</li><li>作者的合成系统为建立转录因子聚集倾向和聚集转录因子在基因调控作用之间的因果关系提供了独特的机会。他们通过调整转录因子的聚集倾向，并同时测量相关的目标基因的反应变化，发现转录因子聚集可以显著影响转录动态。</li><li>作者通过系统的解剖，证明了基因调控功能的参数可以通过转录因子聚集进行定量调节。他们发现，转录因子聚集对基因调控功能的敏感性的影响大于对超敏感性的影响。</li><li>作者证明了基因调控功能的调节可以在高转录因子聚集倾向下导致系统出现新的行为，包括双模态和持续响应行为。他们发现，转录因子聚集可以显著调节有效的转录因子结合亲和力，这可能为许多在转录组中双峰分布的基因提供了见解。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;文章总述&quot;&gt;&lt;a href=&quot;#文章总述&quot; class=&quot;headerlink&quot; title=&quot;文章总述&quot;&gt;&lt;/a&gt;文章总述&lt;/h1&gt;&lt;p&gt;​	 这篇文章的主题是通过化学控制转录因子（TF）聚集来调节基因调控功能。研究人员构建了合成的转录因子，其聚集行为可以通过化</summary>
      
    
    
    
    <category term="文献阅读类" scheme="http://example.com/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>基于VAE和transformer模型根据抗原结构生成抗体序列</title>
    <link href="http://example.com/2023/07/22/%E5%9F%BA%E4%BA%8EVAE%E5%92%8Ctransformer%E6%A8%A1%E5%9E%8B%E6%A0%B9%E6%8D%AE%E6%8A%97%E5%8E%9F%E7%BB%93%E6%9E%84%E7%94%9F%E6%88%90%E6%8A%97%E4%BD%93%E5%BA%8F%E5%88%97/"/>
    <id>http://example.com/2023/07/22/%E5%9F%BA%E4%BA%8EVAE%E5%92%8Ctransformer%E6%A8%A1%E5%9E%8B%E6%A0%B9%E6%8D%AE%E6%8A%97%E5%8E%9F%E7%BB%93%E6%9E%84%E7%94%9F%E6%88%90%E6%8A%97%E4%BD%93%E5%BA%8F%E5%88%97/</id>
    <published>2023-07-22T06:46:48.000Z</published>
    <updated>2023-07-23T08:26:28.396Z</updated>
    
    <content type="html"><![CDATA[<h1 id="构建思路"><a href="#构建思路" class="headerlink" title="构建思路"></a>构建思路</h1><ol><li><p><strong>数据准备</strong>：首先，需要从PDB数据库获取抗体-抗原复合物的数据。需要先对数据进行预处理，以便用于训练模型。之后可以将抗体的氨基酸序列用于训练VAE，并将抗原的结构数据用于训练Transformer模型。</p></li><li><p><strong>训练VAE</strong>：VAE是一种生成模型，可以用于学习抗体氨基酸序列的潜在表示。你可以将抗体的氨基酸序列编码为一种潜在表示，然后从这种潜在表示中解码出原始序列。通过优化VAE的参数，使模型学习到如何生成新的抗体序列。</p></li><li><p><strong>训练Transformer模型</strong>：Transformer模型是一种序列到序列的模型，可以用于根据抗原的结构数据生成抗体的潜在表示。你可以将抗原的结构数据编码为一个序列，然后使用Transformer模型将这个序列转换为抗体的潜在表示。</p></li><li><p><strong>联合训练</strong>：一旦VAE和Transformer模型都被单独训练过，就可以开始联合训练这两个模型。可以将Transformer模型的输出（即抗体的潜在表示）用作VAE的输入，然后让VAE生成抗体的氨基酸序列。通过优化这两个模型的参数，使模型学习到如何根据抗原的结构数据生成抗体序列。</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>首先，我们需要一个函数来解析PDB文件。PDB文件是一种用于存储蛋白质数据的标准格式，其中包含了蛋白质的氨基酸序列和三维结构信息。然后我们需要创建一个数据集类来加载这些数据。</p><p>以下是一个解析PDB文件和创建数据集的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析PDB文件并返回序列和结构信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_pdb_file</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser()</span><br><span class="line">    structure = parser.get_structure(<span class="string">&#x27;X&#x27;</span>, file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取序列信息</span></span><br><span class="line">    ppb = PPBuilder()</span><br><span class="line">    <span class="keyword">for</span> pp <span class="keyword">in</span> ppb.build_peptides(structure):</span><br><span class="line">        sequence = pp.get_sequence()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取结构信息</span></span><br><span class="line">    atom_list = Selection.unfold_entities(structure, <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">    <span class="comment"># 此处简化为取每个氨基酸的CA原子的坐标，实际应用中可能需要更详细的结构信息</span></span><br><span class="line">    structure_info = [atom.get_coord() <span class="keyword">for</span> atom <span class="keyword">in</span> atom_list <span class="keyword">if</span> atom.get_name() == <span class="string">&#x27;CA&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sequence, structure_info</span><br><span class="line"><span class="comment"># 创建PyTorch数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PDBDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dir_path, transform=<span class="literal">None</span></span>):</span><br><span class="line">        self.dir_path = dir_path</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.file_list = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(dir_path) <span class="keyword">if</span> f.endswith(<span class="string">&#x27;.pdb&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        file_path = os.path.join(self.dir_path, self.file_list[idx])</span><br><span class="line">        sequence, structure_info = parse_pdb_file(file_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将序列和结构信息转换为Tensor</span></span><br><span class="line">        sequence_tensor = torch.tensor([amino_acid_to_index[aa] <span class="keyword">for</span> aa <span class="keyword">in</span> sequence], dtype=torch.long)</span><br><span class="line">        sequence_tensor = one_hot(sequence_tensor, num_classes=<span class="built_in">len</span>(amino_acid_to_index))  <span class="comment"># 对氨基酸进行one-hot编码</span></span><br><span class="line">        structure_tensor = torch.tensor(structure_info, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sequence_tensor, structure_tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        sequences, structures = <span class="built_in">zip</span>(*batch)</span><br><span class="line">        <span class="comment"># 使用填充来处理长度不同的序列</span></span><br><span class="line">        sequences_padded = pad_sequence([torch.flatten(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences], batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">        structures_padded = pad_sequence(structures, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> sequences_padded, structures_padded</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataset = PDBDataset(<span class="string">&#x27;/path/to/your/pdb/files&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, collate_fn=PDBDataset.collate_fn)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在这段代码中，我们首先定义了一个函数<code>parse_pdb_file</code>来解析PDB文件。然后我们定义了一个PyTorch数据集类<code>PDBDataset</code>，它会遍历给定目录中的所有PDB文件，对每个文件调用<code>parse_pdb_file</code>函数，并将结果转换为Tensor。</p><p>由于蛋白质序列的长度可能会不同，我们需要对<code>PDBDataset</code>类进行一些修改，以处理这些长度不同的序列。</p><p>一种常用的方法是使用填充（padding），即在较短的序列后面添加一些特殊的元素（例如零），以使所有的序列都有相同的长度。在PyTorch中，我们可以使用<code>torch.nn.utils.rnn.pad_sequence</code>函数来实现这个功能。</p><p>我在<code>__getitem__</code>方法中增加了一个one-hot编码的步骤。之后在<code>collate_fn</code>方法中添加了一个填充的步骤，这个步骤会在每个批次中被调用，以处理长度不同的序列。最后，我在创建数据加载器时传入了这个新的<code>collate_fn</code>方法，以覆盖默认的数据组合方法。</p><h2 id="模型定义和训练"><a href="#模型定义和训练" class="headerlink" title="模型定义和训练"></a>模型定义和训练</h2><p>由于我们想要的模型是基于抗原结构生成抗体序列的生成模型，我们需要对模型做出一些修改以适应这个任务。我们需要使用抗原的结构信息作为输入，然后使用VAE的解码器部分生成抗体的氨基酸序列。此外，我们还需要在模型中添加一个Transformer层，用于处理输入的抗原结构信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义VAE</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">1024</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc21 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc22 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">20</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">400</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h1 = F.relu(self.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc21(h1), self.fc22(h1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span>*logvar)</span><br><span class="line">        eps = torch.randn_like(std)</span><br><span class="line">        <span class="keyword">return</span> mu + eps*std</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, z</span>):</span><br><span class="line">        h3 = F.relu(self.fc3(z))</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.fc4(h3))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mu, logvar = self.encode(x.view(-<span class="number">1</span>, <span class="number">1024</span>))</span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> self.decode(z), mu, logvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Transformer模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self.bert(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型并设置优化器</span></span><br><span class="line">vae = VAE().to(device)</span><br><span class="line">transformer = TransformerModel().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(<span class="built_in">list</span>(vae.parameters()) + <span class="built_in">list</span>(transformer.parameters()), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch, vae, transformer, optimizer, train_loader, device</span>):</span><br><span class="line">    vae.train()</span><br><span class="line">    transformer.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (sequence, structure) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        sequence = sequence.to(device)</span><br><span class="line">        structure = structure.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用Transformer模型处理结构信息</span></span><br><span class="line">        structure_representation = transformer(structure)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将sequence调整为合适的形状</span></span><br><span class="line">        sequence_reshaped = sequence.view(sequence.shape[<span class="number">0</span>], -<span class="number">1</span>, <span class="built_in">len</span>(amino_acid_to_index))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用VAE生成序列</span></span><br><span class="line">        recon_batch, mu, logvar = vae(structure_representation)</span><br><span class="line">        </span><br><span class="line">        loss = loss_function(recon_batch, sequence_reshaped, mu, logvar)</span><br><span class="line">        loss.backward()</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;====&gt; Epoch: &#123;&#125; Average loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss / <span class="built_in">len</span>(train_loader.dataset)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    train(epoch, vae, transformer, optimizer, train_loader, device)</span><br></pre></td></tr></table></figure><p>这个修改后的代码首先定义了一个VAE模型和一个Transformer模型。然后，在训练过程中，我们首先使用Transformer模型处理输入的结构信息，然后将这个处理后的结构信息作为输入传递给VAE，让VAE生成序列。我们使用重构损失和KL散度损失来训练这个模型。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;构建思路&quot;&gt;&lt;a href=&quot;#构建思路&quot; class=&quot;headerlink&quot; title=&quot;构建思路&quot;&gt;&lt;/a&gt;构建思路&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据准备&lt;/strong&gt;：首先，需要从PDB数据库获取抗体-抗原复合物的数据。需要先对数</summary>
      
    
    
    
    <category term="技术类" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>针对pdb文件的数据预处理</title>
    <link href="http://example.com/2023/07/22/%E9%92%88%E5%AF%B9pdb%E6%96%87%E4%BB%B6%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://example.com/2023/07/22/%E9%92%88%E5%AF%B9pdb%E6%96%87%E4%BB%B6%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</id>
    <published>2023-07-22T06:17:54.000Z</published>
    <updated>2023-07-23T08:26:05.363Z</updated>
    
    <content type="html"><![CDATA[<h1 id="针对PDB文件进行数据预处理"><a href="#针对PDB文件进行数据预处理" class="headerlink" title="针对PDB文件进行数据预处理"></a>针对PDB文件进行数据预处理</h1><h2 id="PDB文件的基本构成"><a href="#PDB文件的基本构成" class="headerlink" title="PDB文件的基本构成"></a>PDB文件的基本构成</h2><p>首先，我们需要理解 PDB (Protein Data Bank) 文件的基本结构。PDB 文件通常包含以下几种记录类型：</p><ul><li>HEADER：包含有关整个结构的基本信息。</li><li>COMPND：描述了分子组成。</li><li>AUTHOR：指明了该结构的作者。</li><li>ATOM：描述了分子中每个原子的坐标。</li><li>TER：标识链的结束。</li><li>CONECT：提供了原子间的键连接信息。</li></ul><p>对于模型训练，我们通常关注 ATOM 记录，它包含了原子的坐标信息。在利用 PyTorch 进行模型训练时，我们可能需要将这些坐标转化为张量。</p><p>以下是一个基础的 PDB 文件预处理方法。这个方法将读取 PDB 文件，提取原子坐标，并将它们转化为 PyTorch 张量。</p><p>首先，我们需要安装并导入一些必要的库。然而，当前的环境下没有互联网访问，所以我们假设您已经安装了以下所需的库：</p><ul><li><code>numpy</code>：用于数据处理</li><li><code>torch</code>：PyTorch 库，用于数据预处理和模型训练</li><li><code>Bio.PDB</code>：一个用于处理 PDB 文件的库，是 <code>Biopython</code> 包的一部分</li></ul><p><strong>注意：在您的本地环境中运行下面的代码之前，确保已经安装了这些库。</strong></p><p>接下来，我们可以编写一个函数来读取 PDB 文件并将原子坐标转化为张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">pythonCopy codeimport torch</span><br><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_to_tensor</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line">    </span><br><span class="line">    coordinates = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> model <span class="keyword">in</span> structure:</span><br><span class="line">        <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">            <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">                <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                    coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">    coordinates = torch.tensor(coordinates)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> coordinates</span><br></pre></td></tr></table></figure><p>在上述函数中，我们首先使用 <code>PDBParser</code> 来读取 PDB 文件。然后，我们遍历该结构的所有模型、链、残基和原子，将每个原子的坐标添加到 <code>coordinates</code> 列表中。最后，我们将 <code>coordinates</code> 列表转化为 PyTorch 张量。</p><p>使用这个函数，可以将 PDB 文件转化为 PyTorch 可处理的张量，这样就可以用于模型训练了。</p><h2 id="将氨基酸纳入模型参数范围"><a href="#将氨基酸纳入模型参数范围" class="headerlink" title="将氨基酸纳入模型参数范围"></a>将氨基酸纳入模型参数范围</h2><p>如果你需要将氨基酸的化学性质和组成的结构作为参数进行训练，你需要首先编码这些属性。常见的一种方法是使用独热编码（one-hot encoding）来表示氨基酸的类型，而氨基酸的化学性质则可以通过手动定义的特征来表示。也可以使用某种嵌入（embedding）策略来表示这些特性，但这通常需要训练数据来学习。</p><p>以下是一个扩展了前面函数的版本，这个函数将原子坐标、氨基酸类型（通过独热编码）和二级结构（通过 DSSP 计算）一起返回。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser, DSSP</span><br><span class="line"><span class="keyword">from</span> Bio.SeqUtils <span class="keyword">import</span> IUPACData</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_to_tensor</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line"></span><br><span class="line">    coordinates = []</span><br><span class="line">    amino_acids = []</span><br><span class="line">    sec_structure = []</span><br><span class="line"></span><br><span class="line">    model = structure[<span class="number">0</span>] <span class="comment"># We&#x27;re only using the first model here</span></span><br><span class="line">    dssp = DSSP(model, file_path) <span class="comment"># DSSP for secondary structure</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">        <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">            <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">                <span class="comment"># One-hot encoding of amino acid type</span></span><br><span class="line">                amino_acid = [<span class="number">0</span>]*<span class="number">20</span></span><br><span class="line">                <span class="keyword">if</span> residue.get_resname() <span class="keyword">in</span> IUPACData.protein_letters_1to3:</span><br><span class="line">                    idx = IUPACData.protein_letters.index(IUPACData.protein_letters_1to3[residue.get_resname()])</span><br><span class="line">                    amino_acid[idx] = <span class="number">1</span></span><br><span class="line">                amino_acids.append(amino_acid)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Secondary structure</span></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    sec_structure.append(dssp[(chain.get_id(), residue.get_id())][<span class="number">2</span>])</span><br><span class="line">                <span class="keyword">except</span> KeyError:</span><br><span class="line">                    sec_structure.append(<span class="number">0</span>) <span class="comment"># If residue not in DSSP output, assign a default value</span></span><br><span class="line"></span><br><span class="line">    coordinates = torch.tensor(coordinates)</span><br><span class="line">    amino_acids = torch.tensor(amino_acids)</span><br><span class="line">    sec_structure = torch.tensor(sec_structure)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> coordinates, amino_acids, sec_structure</span><br></pre></td></tr></table></figure><p>这个函数首先使用 <code>PDBParser</code> 来读取 PDB 文件。然后，遍历结构的所有链和残基，并将每个原子的坐标、残基的独热编码和二级结构添加到各自的列表中。最后，我们将这些列表转化为 PyTorch 张量。</p><p>函数使用了 DSSP 来计算二级结构。DSSP 是一个可以从原子坐标计算蛋白质二级结构的程序。<code>Bio.PDB.DSSP</code> 模块提供了一个接口来运行 DSSP 并解析其输出。然而，这需要你的计算环境中已经安装了 DSSP。</p><h2 id="针对多个PDB文件进行批量处理"><a href="#针对多个PDB文件进行批量处理" class="headerlink" title="针对多个PDB文件进行批量处理"></a>针对多个PDB文件进行批量处理</h2><p>为了对多个 PDB 文件进行批量处理，我们可以修改上述函数，使其可以接收一个包含多个 PDB 文件路径的列表，然后对每个文件进行处理。以下是一个修改后的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser, DSSP</span><br><span class="line"><span class="keyword">from</span> Bio.SeqUtils <span class="keyword">import</span> IUPACData</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_files_to_tensors</span>(<span class="params">file_paths</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    data = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> file_paths:</span><br><span class="line">        structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line"></span><br><span class="line">        coordinates = []</span><br><span class="line">        amino_acids = []</span><br><span class="line">        sec_structure = []</span><br><span class="line"></span><br><span class="line">        model = structure[<span class="number">0</span>] <span class="comment"># We&#x27;re only using the first model here</span></span><br><span class="line">        dssp = DSSP(model, file_path) <span class="comment"># DSSP for secondary structure</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">            <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">                <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                    coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># One-hot encoding of amino acid type</span></span><br><span class="line">                    amino_acid = [<span class="number">0</span>]*<span class="number">20</span></span><br><span class="line">                    <span class="keyword">if</span> residue.get_resname() <span class="keyword">in</span> IUPACData.protein_letters_1to3:</span><br><span class="line">                        idx = IUPACData.protein_letters.index(IUPACData.protein_letters_1to3[residue.get_resname()])</span><br><span class="line">                        amino_acid[idx] = <span class="number">1</span></span><br><span class="line">                    amino_acids.append(amino_acid)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Secondary structure</span></span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        sec_structure.append(dssp[(chain.get_id(), residue.get_id())][<span class="number">2</span>])</span><br><span class="line">                    <span class="keyword">except</span> KeyError:</span><br><span class="line">                        sec_structure.append(<span class="number">0</span>) <span class="comment"># If residue not in DSSP output, assign a default value</span></span><br><span class="line"></span><br><span class="line">        coordinates = torch.tensor(coordinates)</span><br><span class="line">        amino_acids = torch.tensor(amino_acids)</span><br><span class="line">        sec_structure = torch.tensor(sec_structure)</span><br><span class="line">        </span><br><span class="line">        data.append((coordinates, amino_acids, sec_structure))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>这个函数首先创建了一个空的 <code>data</code> 列表，然后对 <code>file_paths</code> 列表中的每个文件路径，它都执行和上述函数相同的处理过程。每次处理一个文件，它都会将结果（即坐标、氨基酸类型和二级结构的张量）添加到 <code>data</code> 列表中。最后，函数返回 <code>data</code> 列表。</p><p>你可以使用类似以下的方式来获取目录下所有 PDB 文件的路径，然后将这些路径传递给函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pdb_dir = <span class="string">&quot;/path/to/your/pdb/files&quot;</span></span><br><span class="line">pdb_files = [os.path.join(pdb_dir, file) <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(pdb_dir) <span class="keyword">if</span> file.endswith(<span class="string">&#x27;.pdb&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">data = pdb_files_to_tensors(pdb_files)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;针对PDB文件进行数据预处理&quot;&gt;&lt;a href=&quot;#针对PDB文件进行数据预处理&quot; class=&quot;headerlink&quot; title=&quot;针对PDB文件进行数据预处理&quot;&gt;&lt;/a&gt;针对PDB文件进行数据预处理&lt;/h1&gt;&lt;h2 id=&quot;PDB文件的基本构成&quot;&gt;&lt;a hr</summary>
      
    
    
    
    <category term="技术类" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>AntiPre基于抗原的辅助抗体生成模型</title>
    <link href="http://example.com/2023/07/22/AntiPre%E5%9F%BA%E4%BA%8E%E6%8A%97%E5%8E%9F%E7%9A%84%E8%BE%85%E5%8A%A9%E6%8A%97%E4%BD%93%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2023/07/22/AntiPre%E5%9F%BA%E4%BA%8E%E6%8A%97%E5%8E%9F%E7%9A%84%E8%BE%85%E5%8A%A9%E6%8A%97%E4%BD%93%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</id>
    <published>2023-07-22T05:26:12.000Z</published>
    <updated>2023-07-23T08:26:49.588Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型构建思路"><a href="#模型构建思路" class="headerlink" title="模型构建思路"></a>模型构建思路</h1><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>处理蛋白质序列数据并使用 PyTorch 进行模型训练的任务包含了以下几个步骤：</p><p>首先，从 Protein Data Bank (PDB) 获取蛋白质序列数据。下为一个通常用于获取 PDB 数据的 python 代码示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># The protein id</span></span><br><span class="line">protein_id = <span class="string">&#x27;1abc&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the url</span></span><br><span class="line">url = <span class="string">f&#x27;https://files.rcsb.org/download/<span class="subst">&#123;protein_id&#125;</span>.pdb&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the local filename</span></span><br><span class="line">filename = <span class="string">f&#x27;<span class="subst">&#123;protein_id&#125;</span>.pdb&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If the file doesn&#x27;t exist, download it</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(filename):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Downloading PDB file <span class="subst">&#123;protein_id&#125;</span>...&#x27;</span>)</span><br><span class="line">    urllib.request.urlretrieve(url, filename)</span><br></pre></td></tr></table></figure><p>然后，需要将蛋白质序列数据转换成适合用于模型训练的形式。这通常包括将蛋白质序列数据进行 one-hot 编码或者转换为词嵌入向量等。下面是一个如何进行 one-hot 编码的代码示例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the protein sequence</span></span><br><span class="line">protein_sequence = <span class="string">&#x27;ACDEFGHIKLMNPQRSTVWY&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape the protein sequence to be 2D</span></span><br><span class="line">protein_sequence = np.array(<span class="built_in">list</span>(protein_sequence)).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the one-hot encoder</span></span><br><span class="line">encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit and transform the protein sequence</span></span><br><span class="line">one_hot_encoded_sequence = encoder.fit_transform(protein_sequence)</span><br></pre></td></tr></table></figure><p>最后，可以使用 PyTorch 来定义和训练模型。如果正在进行分类任务，可能需要使用一个卷积神经网络 (CNN) 或循环神经网络 (RNN)。如果正在进行序列生成任务，可能需要使用一个生成对抗网络 (GAN) 或变分自编码器 (VAE)。</p><h2 id="变分自编码器（VAE）和扩散模型（Diffusion）"><a href="#变分自编码器（VAE）和扩散模型（Diffusion）" class="headerlink" title="变分自编码器（VAE）和扩散模型（Diffusion）"></a>变分自编码器（VAE）和扩散模型（Diffusion）</h2><h3 id="变分自编码器（VAE）"><a href="#变分自编码器（VAE）" class="headerlink" title="变分自编码器（VAE）"></a>变分自编码器（VAE）</h3><p>首先，我们来理解一下变分自编码器(VAEs)和扩散模型的基本概念。</p><ol><li>变分自编码器(VAEs): VAEs是一类生成模型，通过最大化数据的边缘对数似然性，并在潜在空间中强制执行先验分布(通常为高斯分布)，从而学习数据的隐含表示。VAEs包含编码器和解码器两部分，编码器将输入数据编码为潜在空间的均值和方差，然后从此分布中采样，生成解码器可以解码的潜在向量。</li><li>扩散模型: 扩散模型是一种连续生成模型，可以理解为一个逐步“去噪”的过程。在这个过程中，模型从一个简单的先验分布（如高斯噪声）开始，然后通过一系列的扩散步骤（或去噪步骤）来生成数据。扩散模型最近在图像生成任务上取得了显著的成功。</li></ol><p>结合VAEs和扩散模型来构建生成模型的一种可能的方式是，使用VAEs来学习数据的隐含表示，然后在这个隐含表示上应用扩散过程。这样可以结合VAEs的表示学习能力和扩散模型的生成能力。</p><p>以下是一个简单的例子，展示如何使用PyTorch构建一个基本的变分自编码器。由于扩散模型的实现较为复杂，这里我们只展示了VAEs的部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Encoder</span></span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, latent_dim*<span class="number">2</span>) <span class="comment"># 为均值和方差各自预留空间</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Decoder</span></span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(latent_dim, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, input_dim),</span><br><span class="line">            nn.Sigmoid() <span class="comment"># 输出层使用sigmoid激活函数，使得输出在(0, 1)范围内</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span>*logvar)</span><br><span class="line">        eps = torch.randn_like(std)</span><br><span class="line">        <span class="keyword">return</span> mu + eps*std</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        h = self.encoder(x)</span><br><span class="line">        mu, logvar = torch.chunk(h, <span class="number">2</span>, dim=<span class="number">1</span>) <span class="comment"># 分割为均值和方差</span></span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(z), mu, logvar</span><br></pre></td></tr></table></figure><p>在上述模型中，前向传播函数返回了解码的结果，以及隐含空间的均值和方差。这些值可以用于计算重构损失和KL散度，这两者是变分自编码器的两部分损失。</p><p>扩散模型的部分实现涉及到更多的细节，包括扩散过程的时间步设置、噪声水平的选择等。具体的实现方式需要根据你的具体需求和任务来决定。此外，由于当前对于结合VAEs和扩散模型的研究还不多，如何结合这两种模型可能需要进行一些创新性的尝试。</p><h3 id="扩散模型（Diffusion）"><a href="#扩散模型（Diffusion）" class="headerlink" title="扩散模型（Diffusion）"></a>扩散模型（Diffusion）</h3><p>在实际应用中，扩散模型通常需要大量的计算资源和训练时间，而且模型的性能还取决于许多超参数的精细调整。</p><p>下面的代码显示了一个基本的扩散模型的框架，它使用了一个简单的多层感知器（MLP）作为反向扩散过程的神经网络。由于扩散模型的复杂性，这只是一个简化的版本，仅供参考。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiffusionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, num_steps</span>):</span><br><span class="line">        <span class="built_in">super</span>(DiffusionModel, self).__init__()</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义扩散过程的神经网络，这里使用一个简单的MLP</span></span><br><span class="line">        self.network = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, hidden_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(hidden_dim, input_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 在输入上添加高斯噪声</span></span><br><span class="line">        noise = torch.randn_like(x)</span><br><span class="line">        x_noisy = x + noise</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行反向扩散过程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_steps):</span><br><span class="line">            x_hat = self.network(x_noisy)</span><br><span class="line">            x_noisy = x_hat + torch.randn_like(x_hat)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_noisy</span><br></pre></td></tr></table></figure><p>在这个模型中，前向传播函数首先在输入上添加了高斯噪声，然后进行了多步的反向扩散过程。在每一步中，它都使用神经网络来预测去噪后的数据，并在预测结果上再次添加高斯噪声。经过多步的反向扩散后，最终返回生成的数据。</p><p>这是一个非常基础的扩散模型，实际上，在高级应用中，这个模型可能需要使用更复杂的神经网络结构，例如卷积神经网络（CNN）或者变换器（Transformer），并可能需要在每一步中使用不同的神经网络。此外，模型的训练过程也需要进行特别的设计，例如使用噪声对比估计（Noise Contrastive Estimation）或者分解概率流（Denoising Score Matching）等方法。</p><p>在将VAE和扩散模型结合的时候，一个可能的方式是，首先使用VAE的编码器将输入数据编码为潜在空间的表示，然后在这个表示上进行扩散过程。在反向传播的时候，可以同时优化VAE的编码器和解码器，以及扩散模型的神经网络。</p><h2 id="模型构建思路-1"><a href="#模型构建思路-1" class="headerlink" title="模型构建思路"></a>模型构建思路</h2><ol><li>首先，我们需要收集大量的蛋白质序列数据作为训练数据。由于我们无法直接从互联网获取数据，我们假设这些数据已经被保存在一个名为<code>protein_sequences</code>的列表中。</li><li>接下来，我们需要将这些蛋白质序列转换为适合神经网络处理的数值数据。在这个例子中，我们将使用一个简单的方法，将每个氨基酸编码为一个唯一的整数。然后，我们可以使用one-hot编码将这些整数转换为二进制向量。</li><li>接下来，我们将使用VAE来学习蛋白质序列的潜在表示。我们将使用一个简单的多层感知器（MLP）作为VAE的编码器和解码器。</li><li>最后，我们将在VAE的潜在表示上应用扩散模型，生成新的蛋白质序列。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;模型构建思路&quot;&gt;&lt;a href=&quot;#模型构建思路&quot; class=&quot;headerlink&quot; title=&quot;模型构建思路&quot;&gt;&lt;/a&gt;模型构建思路&lt;/h1&gt;&lt;h2 id=&quot;数据预处理&quot;&gt;&lt;a href=&quot;#数据预处理&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    <category term="技术类" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>MIT线性代数学习（一）</title>
    <link href="http://example.com/2023/07/21/MIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2023/07/21/MIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0/</id>
    <published>2023-07-21T08:19:49.000Z</published>
    <updated>2023-07-23T08:26:35.249Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用几何理解本质，用代数运用方法"><a href="#用几何理解本质，用代数运用方法" class="headerlink" title="用几何理解本质，用代数运用方法"></a>用几何理解本质，用代数运用方法</h1><h2 id="行图像（Raw-picture）"><a href="#行图像（Raw-picture）" class="headerlink" title="行图像（Raw picture）"></a><strong>行图像（Raw picture）</strong></h2><p>将未知数的系数作为矩阵的行，构成系数矩阵。有m组方程则构成m行，有n个未知数则作为n列。第二个矩阵则是由未知数构成。矩阵方程等号右侧则为方程右侧的数或者参数</p><p>在图像上理解为按行来讲，第一个方程到第m个方程在图像上相交的点作为方程组的解（从几何上理解求解的过程和本质）。而当三个未知数时，则可以看作平面在坐标系上的相交，相交的一点则可以作为方程式的解。</p><h2 id="列图像（Column-picture）"><a href="#列图像（Column-picture）" class="headerlink" title="列图像（Column picture）"></a><strong>列图像（Column picture）</strong></h2><p>按照参数，有m组方程，则写为未知数×m个未知数参数作为行的矩阵方程。这样的写法可以理解成为以m维向量n个基底向量来组成等号右侧的向量。</p><p>理论来说，当(x,y)取任意值的时候，可以表示坐标系上任何一个向量。而当参数从两个，即二维向量不断上升，到三维或者更高维度时，则看作数学意义上的高维向量来组成等号右侧的向量。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;用几何理解本质，用代数运用方法&quot;&gt;&lt;a href=&quot;#用几何理解本质，用代数运用方法&quot; class=&quot;headerlink&quot; title=&quot;用几何理解本质，用代数运用方法&quot;&gt;&lt;/a&gt;用几何理解本质，用代数运用方法&lt;/h1&gt;&lt;h2 id=&quot;行图像（Raw-pictu</summary>
      
    
    
    
    <category term="数理基础类" scheme="http://example.com/categories/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80%E7%B1%BB/"/>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2023/07/20/hello-world/"/>
    <id>http://example.com/2023/07/20/hello-world/</id>
    <published>2023-07-20T12:15:02.661Z</published>
    <updated>2023-07-20T17:04:41.094Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    <category term="技术类" scheme="http://example.com/categories/%E6%8A%80%E6%9C%AF%E7%B1%BB/"/>
    
    
  </entry>
  
</feed>
