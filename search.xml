<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2023/07/20/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>MIT线性代数学习（一）</title>
    <url>/2023/07/21/MIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="用几何理解本质，用代数运用方法"><a href="#用几何理解本质，用代数运用方法" class="headerlink" title="用几何理解本质，用代数运用方法"></a>用几何理解本质，用代数运用方法</h1><h2 id="行图像（Raw-picture）"><a href="#行图像（Raw-picture）" class="headerlink" title="行图像（Raw picture）"></a><strong>行图像（Raw picture）</strong></h2><p>将未知数的系数作为矩阵的行，构成系数矩阵。有m组方程则构成m行，有n个未知数则作为n列。第二个矩阵则是由未知数构成。矩阵方程等号右侧则为方程右侧的数或者参数</p>
<p>在图像上理解为按行来讲，第一个方程到第m个方程在图像上相交的点作为方程组的解（从几何上理解求解的过程和本质）。而当三个未知数时，则可以看作平面在坐标系上的相交，相交的一点则可以作为方程式的解。</p>
<h2 id="列图像（Column-picture）"><a href="#列图像（Column-picture）" class="headerlink" title="列图像（Column picture）"></a><strong>列图像（Column picture）</strong></h2><p>按照参数，有m组方程，则写为未知数×m个未知数参数作为行的矩阵方程。这样的写法可以理解成为以m维向量n个基底向量来组成等号右侧的向量。</p>
<p>理论来说，当(x,y)取任意值的时候，可以表示坐标系上任何一个向量。而当参数从两个，即二维向量不断上升，到三维或者更高维度时，则看作数学意义上的高维向量来组成等号右侧的向量。</p>
]]></content>
      <categories>
        <category>数理基础类</category>
      </categories>
  </entry>
  <entry>
    <title>AntiPre基于抗原的辅助抗体生成模型</title>
    <url>/2023/07/22/AntiPre%E5%9F%BA%E4%BA%8E%E6%8A%97%E5%8E%9F%E7%9A%84%E8%BE%85%E5%8A%A9%E6%8A%97%E4%BD%93%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="模型构建思路"><a href="#模型构建思路" class="headerlink" title="模型构建思路"></a>模型构建思路</h1><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>处理蛋白质序列数据并使用 PyTorch 进行模型训练的任务包含了以下几个步骤：</p>
<p>首先，从 Protein Data Bank (PDB) 获取蛋白质序列数据。下为一个通常用于获取 PDB 数据的 python 代码示例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># The protein id</span></span><br><span class="line">protein_id = <span class="string">&#x27;1abc&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the url</span></span><br><span class="line">url = <span class="string">f&#x27;https://files.rcsb.org/download/<span class="subst">&#123;protein_id&#125;</span>.pdb&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the local filename</span></span><br><span class="line">filename = <span class="string">f&#x27;<span class="subst">&#123;protein_id&#125;</span>.pdb&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If the file doesn&#x27;t exist, download it</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(filename):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Downloading PDB file <span class="subst">&#123;protein_id&#125;</span>...&#x27;</span>)</span><br><span class="line">    urllib.request.urlretrieve(url, filename)</span><br></pre></td></tr></table></figure>

<p>然后，需要将蛋白质序列数据转换成适合用于模型训练的形式。这通常包括将蛋白质序列数据进行 one-hot 编码或者转换为词嵌入向量等。下面是一个如何进行 one-hot 编码的代码示例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the protein sequence</span></span><br><span class="line">protein_sequence = <span class="string">&#x27;ACDEFGHIKLMNPQRSTVWY&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape the protein sequence to be 2D</span></span><br><span class="line">protein_sequence = np.array(<span class="built_in">list</span>(protein_sequence)).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the one-hot encoder</span></span><br><span class="line">encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit and transform the protein sequence</span></span><br><span class="line">one_hot_encoded_sequence = encoder.fit_transform(protein_sequence)</span><br></pre></td></tr></table></figure>

<p>最后，可以使用 PyTorch 来定义和训练模型。如果正在进行分类任务，可能需要使用一个卷积神经网络 (CNN) 或循环神经网络 (RNN)。如果正在进行序列生成任务，可能需要使用一个生成对抗网络 (GAN) 或变分自编码器 (VAE)。</p>
<h2 id="变分自编码器（VAE）和扩散模型（Diffusion）"><a href="#变分自编码器（VAE）和扩散模型（Diffusion）" class="headerlink" title="变分自编码器（VAE）和扩散模型（Diffusion）"></a>变分自编码器（VAE）和扩散模型（Diffusion）</h2><h3 id="变分自编码器（VAE）"><a href="#变分自编码器（VAE）" class="headerlink" title="变分自编码器（VAE）"></a>变分自编码器（VAE）</h3><p>首先，我们来理解一下变分自编码器(VAEs)和扩散模型的基本概念。</p>
<ol>
<li>变分自编码器(VAEs): VAEs是一类生成模型，通过最大化数据的边缘对数似然性，并在潜在空间中强制执行先验分布(通常为高斯分布)，从而学习数据的隐含表示。VAEs包含编码器和解码器两部分，编码器将输入数据编码为潜在空间的均值和方差，然后从此分布中采样，生成解码器可以解码的潜在向量。</li>
<li>扩散模型: 扩散模型是一种连续生成模型，可以理解为一个逐步“去噪”的过程。在这个过程中，模型从一个简单的先验分布（如高斯噪声）开始，然后通过一系列的扩散步骤（或去噪步骤）来生成数据。扩散模型最近在图像生成任务上取得了显著的成功。</li>
</ol>
<p>结合VAEs和扩散模型来构建生成模型的一种可能的方式是，使用VAEs来学习数据的隐含表示，然后在这个隐含表示上应用扩散过程。这样可以结合VAEs的表示学习能力和扩散模型的生成能力。</p>
<p>以下是一个简单的例子，展示如何使用PyTorch构建一个基本的变分自编码器。由于扩散模型的实现较为复杂，这里我们只展示了VAEs的部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Encoder</span></span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, latent_dim*<span class="number">2</span>) <span class="comment"># 为均值和方差各自预留空间</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Decoder</span></span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(latent_dim, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, input_dim),</span><br><span class="line">            nn.Sigmoid() <span class="comment"># 输出层使用sigmoid激活函数，使得输出在(0, 1)范围内</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span>*logvar)</span><br><span class="line">        eps = torch.randn_like(std)</span><br><span class="line">        <span class="keyword">return</span> mu + eps*std</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        h = self.encoder(x)</span><br><span class="line">        mu, logvar = torch.chunk(h, <span class="number">2</span>, dim=<span class="number">1</span>) <span class="comment"># 分割为均值和方差</span></span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(z), mu, logvar</span><br></pre></td></tr></table></figure>

<p>在上述模型中，前向传播函数返回了解码的结果，以及隐含空间的均值和方差。这些值可以用于计算重构损失和KL散度，这两者是变分自编码器的两部分损失。</p>
<p>扩散模型的部分实现涉及到更多的细节，包括扩散过程的时间步设置、噪声水平的选择等。具体的实现方式需要根据你的具体需求和任务来决定。此外，由于当前对于结合VAEs和扩散模型的研究还不多，如何结合这两种模型可能需要进行一些创新性的尝试。</p>
<h3 id="扩散模型（Diffusion）"><a href="#扩散模型（Diffusion）" class="headerlink" title="扩散模型（Diffusion）"></a>扩散模型（Diffusion）</h3><p>在实际应用中，扩散模型通常需要大量的计算资源和训练时间，而且模型的性能还取决于许多超参数的精细调整。</p>
<p>下面的代码显示了一个基本的扩散模型的框架，它使用了一个简单的多层感知器（MLP）作为反向扩散过程的神经网络。由于扩散模型的复杂性，这只是一个简化的版本，仅供参考。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiffusionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, num_steps</span>):</span><br><span class="line">        <span class="built_in">super</span>(DiffusionModel, self).__init__()</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义扩散过程的神经网络，这里使用一个简单的MLP</span></span><br><span class="line">        self.network = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, hidden_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(hidden_dim, input_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 在输入上添加高斯噪声</span></span><br><span class="line">        noise = torch.randn_like(x)</span><br><span class="line">        x_noisy = x + noise</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行反向扩散过程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_steps):</span><br><span class="line">            x_hat = self.network(x_noisy)</span><br><span class="line">            x_noisy = x_hat + torch.randn_like(x_hat)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_noisy</span><br></pre></td></tr></table></figure>

<p>在这个模型中，前向传播函数首先在输入上添加了高斯噪声，然后进行了多步的反向扩散过程。在每一步中，它都使用神经网络来预测去噪后的数据，并在预测结果上再次添加高斯噪声。经过多步的反向扩散后，最终返回生成的数据。</p>
<p>这是一个非常基础的扩散模型，实际上，在高级应用中，这个模型可能需要使用更复杂的神经网络结构，例如卷积神经网络（CNN）或者变换器（Transformer），并可能需要在每一步中使用不同的神经网络。此外，模型的训练过程也需要进行特别的设计，例如使用噪声对比估计（Noise Contrastive Estimation）或者分解概率流（Denoising Score Matching）等方法。</p>
<p>在将VAE和扩散模型结合的时候，一个可能的方式是，首先使用VAE的编码器将输入数据编码为潜在空间的表示，然后在这个表示上进行扩散过程。在反向传播的时候，可以同时优化VAE的编码器和解码器，以及扩散模型的神经网络。</p>
<h2 id="模型构建思路-1"><a href="#模型构建思路-1" class="headerlink" title="模型构建思路"></a>模型构建思路</h2><ol>
<li>首先，我们需要收集大量的蛋白质序列数据作为训练数据。由于我们无法直接从互联网获取数据，我们假设这些数据已经被保存在一个名为<code>protein_sequences</code>的列表中。</li>
<li>接下来，我们需要将这些蛋白质序列转换为适合神经网络处理的数值数据。在这个例子中，我们将使用一个简单的方法，将每个氨基酸编码为一个唯一的整数。然后，我们可以使用one-hot编码将这些整数转换为二进制向量。</li>
<li>接下来，我们将使用VAE来学习蛋白质序列的潜在表示。我们将使用一个简单的多层感知器（MLP）作为VAE的编码器和解码器。</li>
<li>最后，我们将在VAE的潜在表示上应用扩散模型，生成新的蛋白质序列。</li>
</ol>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>针对pdb文件的数据预处理</title>
    <url>/2023/07/22/%E9%92%88%E5%AF%B9pdb%E6%96%87%E4%BB%B6%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="针对PDB文件进行数据预处理"><a href="#针对PDB文件进行数据预处理" class="headerlink" title="针对PDB文件进行数据预处理"></a>针对PDB文件进行数据预处理</h1><h2 id="PDB文件的基本构成"><a href="#PDB文件的基本构成" class="headerlink" title="PDB文件的基本构成"></a>PDB文件的基本构成</h2><p>首先，我们需要理解 PDB (Protein Data Bank) 文件的基本结构。PDB 文件通常包含以下几种记录类型：</p>
<ul>
<li>HEADER：包含有关整个结构的基本信息。</li>
<li>COMPND：描述了分子组成。</li>
<li>AUTHOR：指明了该结构的作者。</li>
<li>ATOM：描述了分子中每个原子的坐标。</li>
<li>TER：标识链的结束。</li>
<li>CONECT：提供了原子间的键连接信息。</li>
</ul>
<p>对于模型训练，我们通常关注 ATOM 记录，它包含了原子的坐标信息。在利用 PyTorch 进行模型训练时，我们可能需要将这些坐标转化为张量。</p>
<p>以下是一个基础的 PDB 文件预处理方法。这个方法将读取 PDB 文件，提取原子坐标，并将它们转化为 PyTorch 张量。</p>
<p>首先，我们需要安装并导入一些必要的库。然而，当前的环境下没有互联网访问，所以我们假设您已经安装了以下所需的库：</p>
<ul>
<li><code>numpy</code>：用于数据处理</li>
<li><code>torch</code>：PyTorch 库，用于数据预处理和模型训练</li>
<li><code>Bio.PDB</code>：一个用于处理 PDB 文件的库，是 <code>Biopython</code> 包的一部分</li>
</ul>
<p><strong>注意：在您的本地环境中运行下面的代码之前，确保已经安装了这些库。</strong></p>
<p>接下来，我们可以编写一个函数来读取 PDB 文件并将原子坐标转化为张量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pythonCopy codeimport torch</span><br><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_to_tensor</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line">    </span><br><span class="line">    coordinates = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> model <span class="keyword">in</span> structure:</span><br><span class="line">        <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">            <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">                <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                    coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">    coordinates = torch.tensor(coordinates)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> coordinates</span><br></pre></td></tr></table></figure>

<p>在上述函数中，我们首先使用 <code>PDBParser</code> 来读取 PDB 文件。然后，我们遍历该结构的所有模型、链、残基和原子，将每个原子的坐标添加到 <code>coordinates</code> 列表中。最后，我们将 <code>coordinates</code> 列表转化为 PyTorch 张量。</p>
<p>使用这个函数，可以将 PDB 文件转化为 PyTorch 可处理的张量，这样就可以用于模型训练了。</p>
<h2 id="将氨基酸纳入模型参数范围"><a href="#将氨基酸纳入模型参数范围" class="headerlink" title="将氨基酸纳入模型参数范围"></a>将氨基酸纳入模型参数范围</h2><p>如果你需要将氨基酸的化学性质和组成的结构作为参数进行训练，你需要首先编码这些属性。常见的一种方法是使用独热编码（one-hot encoding）来表示氨基酸的类型，而氨基酸的化学性质则可以通过手动定义的特征来表示。也可以使用某种嵌入（embedding）策略来表示这些特性，但这通常需要训练数据来学习。</p>
<p>以下是一个扩展了前面函数的版本，这个函数将原子坐标、氨基酸类型（通过独热编码）和二级结构（通过 DSSP 计算）一起返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser, DSSP</span><br><span class="line"><span class="keyword">from</span> Bio.SeqUtils <span class="keyword">import</span> IUPACData</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_to_tensor</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line"></span><br><span class="line">    coordinates = []</span><br><span class="line">    amino_acids = []</span><br><span class="line">    sec_structure = []</span><br><span class="line"></span><br><span class="line">    model = structure[<span class="number">0</span>] <span class="comment"># We&#x27;re only using the first model here</span></span><br><span class="line">    dssp = DSSP(model, file_path) <span class="comment"># DSSP for secondary structure</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">        <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">            <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">                <span class="comment"># One-hot encoding of amino acid type</span></span><br><span class="line">                amino_acid = [<span class="number">0</span>]*<span class="number">20</span></span><br><span class="line">                <span class="keyword">if</span> residue.get_resname() <span class="keyword">in</span> IUPACData.protein_letters_1to3:</span><br><span class="line">                    idx = IUPACData.protein_letters.index(IUPACData.protein_letters_1to3[residue.get_resname()])</span><br><span class="line">                    amino_acid[idx] = <span class="number">1</span></span><br><span class="line">                amino_acids.append(amino_acid)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Secondary structure</span></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    sec_structure.append(dssp[(chain.get_id(), residue.get_id())][<span class="number">2</span>])</span><br><span class="line">                <span class="keyword">except</span> KeyError:</span><br><span class="line">                    sec_structure.append(<span class="number">0</span>) <span class="comment"># If residue not in DSSP output, assign a default value</span></span><br><span class="line"></span><br><span class="line">    coordinates = torch.tensor(coordinates)</span><br><span class="line">    amino_acids = torch.tensor(amino_acids)</span><br><span class="line">    sec_structure = torch.tensor(sec_structure)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> coordinates, amino_acids, sec_structure</span><br></pre></td></tr></table></figure>

<p>这个函数首先使用 <code>PDBParser</code> 来读取 PDB 文件。然后，遍历结构的所有链和残基，并将每个原子的坐标、残基的独热编码和二级结构添加到各自的列表中。最后，我们将这些列表转化为 PyTorch 张量。</p>
<p>函数使用了 DSSP 来计算二级结构。DSSP 是一个可以从原子坐标计算蛋白质二级结构的程序。<code>Bio.PDB.DSSP</code> 模块提供了一个接口来运行 DSSP 并解析其输出。然而，这需要你的计算环境中已经安装了 DSSP。</p>
<h2 id="针对多个PDB文件进行批量处理"><a href="#针对多个PDB文件进行批量处理" class="headerlink" title="针对多个PDB文件进行批量处理"></a>针对多个PDB文件进行批量处理</h2><p>为了对多个 PDB 文件进行批量处理，我们可以修改上述函数，使其可以接收一个包含多个 PDB 文件路径的列表，然后对每个文件进行处理。以下是一个修改后的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser, DSSP</span><br><span class="line"><span class="keyword">from</span> Bio.SeqUtils <span class="keyword">import</span> IUPACData</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_files_to_tensors</span>(<span class="params">file_paths</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    data = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> file_paths:</span><br><span class="line">        structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line"></span><br><span class="line">        coordinates = []</span><br><span class="line">        amino_acids = []</span><br><span class="line">        sec_structure = []</span><br><span class="line"></span><br><span class="line">        model = structure[<span class="number">0</span>] <span class="comment"># We&#x27;re only using the first model here</span></span><br><span class="line">        dssp = DSSP(model, file_path) <span class="comment"># DSSP for secondary structure</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">            <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">                <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                    coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># One-hot encoding of amino acid type</span></span><br><span class="line">                    amino_acid = [<span class="number">0</span>]*<span class="number">20</span></span><br><span class="line">                    <span class="keyword">if</span> residue.get_resname() <span class="keyword">in</span> IUPACData.protein_letters_1to3:</span><br><span class="line">                        idx = IUPACData.protein_letters.index(IUPACData.protein_letters_1to3[residue.get_resname()])</span><br><span class="line">                        amino_acid[idx] = <span class="number">1</span></span><br><span class="line">                    amino_acids.append(amino_acid)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Secondary structure</span></span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        sec_structure.append(dssp[(chain.get_id(), residue.get_id())][<span class="number">2</span>])</span><br><span class="line">                    <span class="keyword">except</span> KeyError:</span><br><span class="line">                        sec_structure.append(<span class="number">0</span>) <span class="comment"># If residue not in DSSP output, assign a default value</span></span><br><span class="line"></span><br><span class="line">        coordinates = torch.tensor(coordinates)</span><br><span class="line">        amino_acids = torch.tensor(amino_acids)</span><br><span class="line">        sec_structure = torch.tensor(sec_structure)</span><br><span class="line">        </span><br><span class="line">        data.append((coordinates, amino_acids, sec_structure))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

<p>这个函数首先创建了一个空的 <code>data</code> 列表，然后对 <code>file_paths</code> 列表中的每个文件路径，它都执行和上述函数相同的处理过程。每次处理一个文件，它都会将结果（即坐标、氨基酸类型和二级结构的张量）添加到 <code>data</code> 列表中。最后，函数返回 <code>data</code> 列表。</p>
<p>你可以使用类似以下的方式来获取目录下所有 PDB 文件的路径，然后将这些路径传递给函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pdb_dir = <span class="string">&quot;/path/to/your/pdb/files&quot;</span></span><br><span class="line">pdb_files = [os.path.join(pdb_dir, file) <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(pdb_dir) <span class="keyword">if</span> file.endswith(<span class="string">&#x27;.pdb&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">data = pdb_files_to_tensors(pdb_files)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>基于VAE和transformer模型根据抗原结构生成抗体序列</title>
    <url>/2023/07/22/%E5%9F%BA%E4%BA%8EVAE%E5%92%8Ctransformer%E6%A8%A1%E5%9E%8B%E6%A0%B9%E6%8D%AE%E6%8A%97%E5%8E%9F%E7%BB%93%E6%9E%84%E7%94%9F%E6%88%90%E6%8A%97%E4%BD%93%E5%BA%8F%E5%88%97/</url>
    <content><![CDATA[<h1 id="构建思路"><a href="#构建思路" class="headerlink" title="构建思路"></a>构建思路</h1><ol>
<li><p><strong>数据准备</strong>：首先，需要从PDB数据库获取抗体-抗原复合物的数据。需要先对数据进行预处理，以便用于训练模型。之后可以将抗体的氨基酸序列用于训练VAE，并将抗原的结构数据用于训练Transformer模型。</p>
</li>
<li><p><strong>训练VAE</strong>：VAE是一种生成模型，可以用于学习抗体氨基酸序列的潜在表示。你可以将抗体的氨基酸序列编码为一种潜在表示，然后从这种潜在表示中解码出原始序列。通过优化VAE的参数，使模型学习到如何生成新的抗体序列。</p>
</li>
<li><p><strong>训练Transformer模型</strong>：Transformer模型是一种序列到序列的模型，可以用于根据抗原的结构数据生成抗体的潜在表示。你可以将抗原的结构数据编码为一个序列，然后使用Transformer模型将这个序列转换为抗体的潜在表示。</p>
</li>
<li><p><strong>联合训练</strong>：一旦VAE和Transformer模型都被单独训练过，就可以开始联合训练这两个模型。可以将Transformer模型的输出（即抗体的潜在表示）用作VAE的输入，然后让VAE生成抗体的氨基酸序列。通过优化这两个模型的参数，使模型学习到如何根据抗原的结构数据生成抗体序列。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>首先，我们需要一个函数来解析PDB文件。PDB文件是一种用于存储蛋白质数据的标准格式，其中包含了蛋白质的氨基酸序列和三维结构信息。然后我们需要创建一个数据集类来加载这些数据。</p>
<p>以下是一个解析PDB文件和创建数据集的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析PDB文件并返回序列和结构信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_pdb_file</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser()</span><br><span class="line">    structure = parser.get_structure(<span class="string">&#x27;X&#x27;</span>, file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取序列信息</span></span><br><span class="line">    ppb = PPBuilder()</span><br><span class="line">    <span class="keyword">for</span> pp <span class="keyword">in</span> ppb.build_peptides(structure):</span><br><span class="line">        sequence = pp.get_sequence()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取结构信息</span></span><br><span class="line">    atom_list = Selection.unfold_entities(structure, <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">    <span class="comment"># 此处简化为取每个氨基酸的CA原子的坐标，实际应用中可能需要更详细的结构信息</span></span><br><span class="line">    structure_info = [atom.get_coord() <span class="keyword">for</span> atom <span class="keyword">in</span> atom_list <span class="keyword">if</span> atom.get_name() == <span class="string">&#x27;CA&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sequence, structure_info</span><br><span class="line"><span class="comment"># 创建PyTorch数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PDBDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dir_path, transform=<span class="literal">None</span></span>):</span><br><span class="line">        self.dir_path = dir_path</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.file_list = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(dir_path) <span class="keyword">if</span> f.endswith(<span class="string">&#x27;.pdb&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        file_path = os.path.join(self.dir_path, self.file_list[idx])</span><br><span class="line">        sequence, structure_info = parse_pdb_file(file_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将序列和结构信息转换为Tensor</span></span><br><span class="line">        sequence_tensor = torch.tensor([amino_acid_to_index[aa] <span class="keyword">for</span> aa <span class="keyword">in</span> sequence], dtype=torch.long)</span><br><span class="line">        sequence_tensor = one_hot(sequence_tensor, num_classes=<span class="built_in">len</span>(amino_acid_to_index))  <span class="comment"># 对氨基酸进行one-hot编码</span></span><br><span class="line">        structure_tensor = torch.tensor(structure_info, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sequence_tensor, structure_tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        sequences, structures = <span class="built_in">zip</span>(*batch)</span><br><span class="line">        <span class="comment"># 使用填充来处理长度不同的序列</span></span><br><span class="line">        sequences_padded = pad_sequence([torch.flatten(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences], batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">        structures_padded = pad_sequence(structures, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> sequences_padded, structures_padded</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataset = PDBDataset(<span class="string">&#x27;/path/to/your/pdb/files&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, collate_fn=PDBDataset.collate_fn)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这段代码中，我们首先定义了一个函数<code>parse_pdb_file</code>来解析PDB文件。然后我们定义了一个PyTorch数据集类<code>PDBDataset</code>，它会遍历给定目录中的所有PDB文件，对每个文件调用<code>parse_pdb_file</code>函数，并将结果转换为Tensor。</p>
<p>由于蛋白质序列的长度可能会不同，我们需要对<code>PDBDataset</code>类进行一些修改，以处理这些长度不同的序列。</p>
<p>一种常用的方法是使用填充（padding），即在较短的序列后面添加一些特殊的元素（例如零），以使所有的序列都有相同的长度。在PyTorch中，我们可以使用<code>torch.nn.utils.rnn.pad_sequence</code>函数来实现这个功能。</p>
<p>我在<code>__getitem__</code>方法中增加了一个one-hot编码的步骤。之后在<code>collate_fn</code>方法中添加了一个填充的步骤，这个步骤会在每个批次中被调用，以处理长度不同的序列。最后，我在创建数据加载器时传入了这个新的<code>collate_fn</code>方法，以覆盖默认的数据组合方法。</p>
<h2 id="模型定义和训练"><a href="#模型定义和训练" class="headerlink" title="模型定义和训练"></a>模型定义和训练</h2><p>由于我们想要的模型是基于抗原结构生成抗体序列的生成模型，我们需要对模型做出一些修改以适应这个任务。我们需要使用抗原的结构信息作为输入，然后使用VAE的解码器部分生成抗体的氨基酸序列。此外，我们还需要在模型中添加一个Transformer层，用于处理输入的抗原结构信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义VAE</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">1024</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc21 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc22 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">20</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">400</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h1 = F.relu(self.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc21(h1), self.fc22(h1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span>*logvar)</span><br><span class="line">        eps = torch.randn_like(std)</span><br><span class="line">        <span class="keyword">return</span> mu + eps*std</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, z</span>):</span><br><span class="line">        h3 = F.relu(self.fc3(z))</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.fc4(h3))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mu, logvar = self.encode(x.view(-<span class="number">1</span>, <span class="number">1024</span>))</span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> self.decode(z), mu, logvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Transformer模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self.bert(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型并设置优化器</span></span><br><span class="line">vae = VAE().to(device)</span><br><span class="line">transformer = TransformerModel().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(<span class="built_in">list</span>(vae.parameters()) + <span class="built_in">list</span>(transformer.parameters()), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch, vae, transformer, optimizer, train_loader, device</span>):</span><br><span class="line">    vae.train()</span><br><span class="line">    transformer.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (sequence, structure) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        sequence = sequence.to(device)</span><br><span class="line">        structure = structure.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用Transformer模型处理结构信息</span></span><br><span class="line">        structure_representation = transformer(structure)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将sequence调整为合适的形状</span></span><br><span class="line">        sequence_reshaped = sequence.view(sequence.shape[<span class="number">0</span>], -<span class="number">1</span>, <span class="built_in">len</span>(amino_acid_to_index))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用VAE生成序列</span></span><br><span class="line">        recon_batch, mu, logvar = vae(structure_representation)</span><br><span class="line">        </span><br><span class="line">        loss = loss_function(recon_batch, sequence_reshaped, mu, logvar)</span><br><span class="line">        loss.backward()</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;====&gt; Epoch: &#123;&#125; Average loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss / <span class="built_in">len</span>(train_loader.dataset)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    train(epoch, vae, transformer, optimizer, train_loader, device)</span><br></pre></td></tr></table></figure>

<p>这个修改后的代码首先定义了一个VAE模型和一个Transformer模型。然后，在训练过程中，我们首先使用Transformer模型处理输入的结构信息，然后将这个处理后的结构信息作为输入传递给VAE，让VAE生成序列。我们使用重构损失和KL散度损失来训练这个模型。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>文献阅读林一瀚老师组：Modulating gene regulation function by chemically controlled transcription factor clustering</title>
    <url>/2023/07/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<h1 id="文章总述"><a href="#文章总述" class="headerlink" title="文章总述"></a>文章总述</h1><p>​	 这篇文章的主题是通过化学控制转录因子（TF）聚集来调节基因调控功能。研究人员构建了合成的转录因子，其聚集行为可以通过化学方式控制。通过调整系统的单一参数（即，转录因子聚集倾向），他们提供了证据支持转录因子聚集直接激活和放大目标基因。单基因成像表明，这种放大结果来自转录动态的调节。重要的是，转录因子聚集倾向通过显著调节有效的转录因子结合亲和力，以及在较小程度上调节超敏感性，从而调节基因调控功能，这有助于双峰性和持续反应行为，这些都让人联想到典型的细胞命运控制系统。总的来说，这些结果表明，转录因子聚集可以调节基因调控功能，以实现突现行为，并强调了化学控制蛋白质聚集的潜在应用。</p>
<h2 id="化学调控系统的构建"><a href="#化学调控系统的构建" class="headerlink" title="化学调控系统的构建"></a>化学调控系统的构建</h2><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig1_HTML.png?as=webp" alt="image-20230723141553747" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li><p>作者构建的系统在原有的雷帕霉素诱导系统上进行改造，通过Tag6和HOTag6的作用，使其在无雷帕霉素的存在时，将会形成四聚体，而存在雷帕霉素时，将会相互作用成簇。为了定量表达过程，作者通过和TF结合域（TetO）相连的iRFP和能形成茎环的24xPP7来作为报告系统。</p>
</li>
<li><p>之后通过探究雷帕霉素浓度对转录因子聚集的影响，通过使用U2OS细胞系，滴定从0nM到1000nM的雷帕霉素，之后在共聚焦显微镜下观察切片。通过数学建模和观察发现，随着雷帕霉素浓度的升高，细胞中聚类因子的打分（直径和数量&#x2F;细胞）都有上升。且在达到最大浓度的雷帕霉素时，聚类的直径平均曲线并未到达表观最大值，但另外两条曲线则达到了，说明随着浓度的升高，聚类倾向于形状变大而不是数量增多（当浓度超过100nM时）。</p>
<h2 id="TF聚类倾向直接影响目标蛋白表达水平"><a href="#TF聚类倾向直接影响目标蛋白表达水平" class="headerlink" title="TF聚类倾向直接影响目标蛋白表达水平"></a>TF聚类倾向直接影响目标蛋白表达水平</h2><p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig2_HTML.png?as=webp" alt="image-2" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
</li>
<li><p>作者通过在三种细胞系中进行对照试验，变量包括DNA结合位点，TF结合位点数目，细胞系。通过对照试验发现目标蛋白表达水平受到雷帕霉素浓度的显著影响，这种影响不是因为其他别的因素引起的。</p>
</li>
<li><p>这些数据表明，TF表达扩增的聚类依赖于DNA结合位点的数量，这与最近的研究结果相一致。而且在低雷帕霉素的浓度下也可以产生表达扩增的现象，这种现象在目测情况下和无雷帕霉素的条件下产生的表达扩增无明显增加。而且表达扩增在比可视检测TF聚类饱和更低的雷帕霉素的浓度下饱和。<strong>说明基因表达的扩增可以由TF聚类来促进，但是存在上限，这种上限不是由聚类的大小和数量来决定的。</strong></p>
<h2 id="TF聚类可以直接激活和调节转录动态"><a href="#TF聚类可以直接激活和调节转录动态" class="headerlink" title="TF聚类可以直接激活和调节转录动态"></a>TF聚类可以直接激活和调节转录动态</h2><p><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig3_HTML.png?as=webp" alt="image" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
</li>
<li><p>作者想用PP7形成的茎环来作为报告，探究TF簇对靶基因的时空激活，因此作者想探究雷帕霉素依赖的TF簇能否可以以多西环素依赖的方式整合到靶基因位点上。而且因为可以观察同一细胞的转录信号，所以可以在时空上探索TF簇和基因结合位点的相互作用，最后，想通过不同浓度的雷帕霉素来探究靶基因在不同聚类倾向下的TF的作用下的转录水平</p>
</li>
<li><p>作者进行了一系列的实验，包括使用荧光原位杂交（FISH）技术来同时观察报告基因位点和转录因子聚集体，以及使用双色时间延迟成像来观察稳定整合的目标基因位点的新生转录信号和转录因子聚集体的信号。这些实验结果显示，转录因子聚集体可以特异性地与DNA结合，并且这种结合是可控的，这为合成转录因子聚集体的因果基因调控作用提供了证据。</p>
</li>
<li><p>作者发现，转录因子聚集体和新生转录信号之间存在空间上的相互作用，这些相互作用可能是由转录因子聚集体与报告基因位点的结合引起的。此外，他们还发现，转录因子聚集体的大小与新生转录信号的强度之间存在负相关性。</p>
</li>
<li><p>通过调整转录因子聚集体的倾向，作者发现，转录因子聚集体可以显著影响转录动态。具体来说，增加聚集倾向可以显著增加新生转录位点被检测到的时间比例，减少添加多西环素后到第一次转录爆发的时间，并导致更多的基因在同一调控元中被激活。</p>
</li>
<li><p>总的来说，这些数据提供了基于成像的证据，表明转录因子聚集体可以直接并增强地激活目标基因的转录。这些数据也暗示，通过增强转录因子的聚集倾向，细胞可以在不改变其表达水平的情况下激活特定调控元中的更多基因。</p>
</li>
<li><p>作者发现，当转录因子（TF）聚集倾向高时，基因调控功能可以被调节以实现双模态。他们使用流式细胞术来表征U2OS-7TetO细胞，发现输入信号在不同的雷帕霉素浓度下呈单峰分布，而输出信号在较高的雷帕霉素浓度下呈双峰分布。这些数据表明，当转录因子聚集倾向高时，基因调控功能可以被调节以实现双模态。</p>
</li>
<li><p>作者推测，基因调控功能的有效转录因子结合亲和力和超敏感性的调节可能导致双模态的出现。他们使用Hill函数拟合流式细胞术的输入-输出数据，并发现随着雷帕霉素浓度的增加，转录因子对DNA的有效结合亲和力（即，解离常数减小）和Hill系数都有所增加。</p>
</li>
<li><p>作者发现，转录因子聚集倾向可以调节基因调控功能的有效结合亲和力和超敏感性，其中有效结合亲和力的调节程度比超敏感性大得多，这表明在他们的系统中，转录因子聚集对基因调控功能的敏感性的影响大于对超敏感性的影响。</p>
</li>
<li><p>作者还发现，这些调节有效地导致输出信号的绝对细胞间变异性（即，标准差）增加，而输入信号的变异性在所有条件下都保持相对恒定。相比之下，相对细胞间变异性（即，变异系数或噪声）显示出非单调行为，这表明转录因子聚集可以调节基因表达噪声。</p>
</li>
</ul>
<p><img src="https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-022-30397-2/MediaObjects/41467_2022_30397_Fig4_HTML.png?as=webp" alt="image" loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<ul>
<li>作者通过一系列的计算模拟验证了他们的观察结果，这些模拟成功地复现了持续响应行为。他们发现，有效的转录因子结合亲和力的调节，而不是超敏感性的调节，对观察到的行为起主要作用。这些结果表明，转录因子聚集大大调节了基因调控功能的有效结合亲和力，从而导致观察到的持续响应（即，类似记忆的）行为。</li>
<li>在讨论部分，作者指出，越来越多的研究表明转录因子聚集在决定细胞命运中起关键作用，然而，转录因子聚集在基因调控中的定量作用仍然不太清楚。在这项工作中，作者通过合理设计一个具有化学可调转录因子聚集的自下而上的合成基因调控系统，阐明了转录因子聚集在激活和放大基因转录以及调节基因调控功能中的作用。</li>
<li>作者的合成系统为建立转录因子聚集倾向和聚集转录因子在基因调控作用之间的因果关系提供了独特的机会。他们通过调整转录因子的聚集倾向，并同时测量相关的目标基因的反应变化，发现转录因子聚集可以显著影响转录动态。</li>
<li>作者通过系统的解剖，证明了基因调控功能的参数可以通过转录因子聚集进行定量调节。他们发现，转录因子聚集对基因调控功能的敏感性的影响大于对超敏感性的影响。</li>
<li>作者证明了基因调控功能的调节可以在高转录因子聚集倾向下导致系统出现新的行为，包括双模态和持续响应行为。他们发现，转录因子聚集可以显著调节有效的转录因子结合亲和力，这可能为许多在转录组中双峰分布的基因提供了见解。</li>
</ul>
]]></content>
      <categories>
        <category>文献阅读类</category>
      </categories>
  </entry>
  <entry>
    <title>文献阅读：davidBaker组RFdiffusion</title>
    <url>/2023/07/24/Baker%E7%BB%84RFdiffusion/</url>
    <content><![CDATA[<h1 id="文章总述"><a href="#文章总述" class="headerlink" title="文章总述"></a>文章总述</h1><ul>
<li><p>论文介绍了一种基于深度学习的蛋白质设计框架，称为 RFdiffusion，它利用了 RoseTTAFold 结构预测网络的能力，通过逆向去噪过程生成多样化和精确的蛋白质结构。</p>
</li>
<li><p>论文展示了 RFdiffusion 在无条件和有条件的蛋白质单体设计、蛋白质结合物设计、对称寡聚体设计、酶活性位点支架设计和对称功能基团支架设计等多个方面的优异性能。</p>
</li>
<li><p>论文通过实验验证了数百个设计的对称组装体、金属结合蛋白和蛋白质结合物的结构和功能，并通过冷冻电镜解析了一个设计的结合物与流感血凝素的复合物结构，证明了 RFdiffusion 可以以原子级精度设计功能性蛋白质。</p>
</li>
<li><p>论文讨论了 RFdiffusion 的优势和局限，以及未来可能的扩展方向，例如将其应用于核酸和配体结合蛋白的设计，以及利用外部势能和微调技术来定制特定的设计挑战。</p>
<h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><ul>
<li><p>原理：RFdiffusion 的基本思想是利用一个去噪网络将随机状态映射到一个低维流形上，该流形包含了所有可能的合理的蛋白质结构。通过在流形上进行随机游走，RFdiffusion 可以探索出各种不同的蛋白质结构，并通过条件信息来指导生成过程，以满足特定的设计目标。去噪网络的作用是将输入的状态修正为更接近真实蛋白质结构的状态，从而消除噪声和不合理的结构。条件信息的作用是提供一些额外的约束或指示，例如拓扑类型、对称性、功能基团、结合伙伴等，从而使生成的结构符合设计要求。RFdiffusion 的核心假设是存在一个从随机状态到真实蛋白质结构的映射关系，该映射关系可以由去噪网络来近似学习和表示。通过逆向扩散过程，RFdiffusion 可以从任意初始状态出发，沿着梯度方向逐步接近真实蛋白质结构，同时保持多样性和创新性。</p>
</li>
<li><p>实现方法：RFdiffusion 的实现方法主要包括以下几个步骤：</p>
<ul>
<li>建立去噪网络：RFdiffusion 使用了 RoseTTAFold 结构预测网络作为去噪网络，因为它已经被证明能够根据蛋白质序列预测出其三维结构，并达到接近实验水平的精度。RoseTTAFold 结构预测网络由三个模块组成：一个序列编码器、一个三维距离预测器和一个三维坐标预测器。序列编码器将蛋白质序列转换为一个高维特征向量，三维距离预测器将特征向量转换为一个距离矩阵，三维坐标预测器将距离矩阵转换为一个坐标矩阵。RFdiffusion 只使用了 RoseTTAFold 的最后一个模块作为去噪网络，即三维坐标预测器，它可以将任意输入的坐标矩阵修正为更接近真实蛋白质结构的坐标矩阵。</li>
<li>训练去噪网络：RFdiffusion 使用了与 RoseTTAFold 相同的数据集来训练去噪网络，即 Protein Data Bank（PDB）中的所有已知蛋白质结构。训练过程中，RFdiffusion 对每个蛋白质结构添加一些随机扰动或噪声，然后将扰动后的坐标矩阵作为输入，将原始的坐标矩阵作为输出，计算两者之间的均方误差（MSE）作为损失函数，并使用反向传播算法更新去噪网络的参数。训练目标是使去噪网络能够尽可能地恢复原始的蛋白质结构，并消除输入中的噪声和不合理性。</li>
<li>生成蛋白质结构：RFdiffusion 使用了一种基于 Langevin 动力学（LD）的生成过程来生成蛋白质结构。LD 是一种描述粒子在随机力场中运动的物理模型，它可以用来模拟蛋白质的折叠过程。RFdiffusion 将去噪网络的输出视为一个势能函数，将输入的坐标矩阵视为一个粒子状态，然后使用 LD 方程来更新粒子状态，从而实现在流形上的随机游走。LD 方程的形式如下：</li>
</ul>
<p>!LD equation</p>
<p>其中，!x_t 是粒子状态，!U(x_t) 是势能函数，!T 是温度参数，!\xi_t 是高斯白噪声。RFdiffusion 使用了 Euler-Maruyama 方法来离散化 LD 方程，并使用梯度下降法来计算势能函数的梯度。生成过程中，RFdiffusion 从一个随机初始化的坐标矩阵开始，然后重复以下步骤：</p>
<ul>
<li><p>将当前的坐标矩阵输入去噪网络，得到修正后的坐标矩阵；</p>
</li>
<li><p>计算修正后的坐标矩阵和当前的坐标矩阵之间的梯度；</p>
</li>
<li><p>根据 LD 方程更新当前的坐标矩阵，并添加一些随机扰动；</p>
</li>
<li><p>如果有条件信息，则将当前的坐标矩阵与条件信息进行对齐或匹配；</p>
</li>
<li><p>如果满足停止条件，则结束生成过程，否则继续重复以上步骤。</p>
</li>
<li><p>设计蛋白质序列：RFdiffusion 使用了另一个基于深度学习的网络，称为 ProteinMPNN，来设计与生成的蛋白质结构匹配的蛋白质序列。ProteinMPNN 是一种基于消息传递神经网络（MPNN）的序列设计网络，它可以根据蛋白质结构和条件信息预测出最优的蛋白质序列。ProteinMPNN 的输入是一个由残基类型、距离、角度、二级结构等特征组成的特征矩阵，以及一个由拓扑类型、对称性、功能基团等特征组成的条件向量。ProteinMPNN 的输出是一个由残基类型概率分布组成的序列矩阵。ProteinMPNN 的训练数据是从 PDB 中提取的所有已知蛋白质结构和序列，以及一些人工合成的条件信息。ProteinMPNN 的训练目标是使预测出的序列与给定的结构和条件信息尽可能地匹配，并具有高度的可行性和稳定性。设计过程中，RFdiffusion 将生成的蛋白质结构和条件信息输入 ProteinMPNN，得到预测出的序列矩阵，然后从每个位置上选择最大概率的残基类型作为最终的序列。如果需要，RFdiffusion 还可以对预测出的序列进行一些后处理，例如添加或删除一些残基，以满足一些额外的约束或优化一些性能指标。</p>
<h2 id="蛋白设计示例"><a href="#蛋白设计示例" class="headerlink" title="蛋白设计示例"></a>蛋白设计示例</h2><p>论文展示了RFdiffusion在以下几个方面的优异性能：</p>
<ul>
<li>无条件的蛋白质单体设计：RFdiffusion可以从随机噪声开始，生成多种复杂的蛋白质结构，包括α螺旋、β折叠和混合α&#x2F;β拓扑，这些结构与AlphaFold2和ESMFold预测的结构非常接近，而且与已知蛋白质结构有很大差异，表明了模型的创新能力。论文还实验验证了一些设计的蛋白质具有高度稳定性和正确的二级结构。</li>
<li>有条件的蛋白质单体设计：RFdiffusion可以根据用户指定的二级结构和&#x2F;或拓扑信息生成符合要求的蛋白质结构，例如TIM桶或NTF2折叠。论文也实验验证了一些设计的TIM桶具有高度稳定性和正确的二级结构。</li>
<li>蛋白质结合物设计：RFdiffusion可以根据目标蛋白质结构信息和界面热点残基生成新颖的高亲和力结合物。论文展示了对五个不同目标（流感病毒血凝素、IL-7Rα、PD-L1、胰岛素受体和TrkA）设计并实验筛选出具有纳摩尔级亲和力的结合物，并且通过冷冻电镜解析了一个流感病毒血凝素结合物与目标复合物的高分辨率结构，与设计模型几乎完全一致。</li>
<li>对称寡聚体设计：RFdiffusion可以根据用户指定的对称性生成具有任意点群对称性（如循环、二面角、四面体、八面体或二十面体）的寡聚体结构。论文展示了多种复杂而新颖的对称寡聚体结构，并且通过电镜验证了它们具有正确的寡聚化状态和形状。</li>
<li>酶活性位点支架设计：RFdiffusion可以根据用户指定的酶活性位点（包括多个残基和背景原子）生成能够精确固定这些位点在空间中位置和取向的支架蛋白质。论文展示了对多种酶类别（如水解酶、转移酶、还原酶等）进行活性位点支架设计，并且通过AlphaFold2验证了设计模型的准确性。</li>
<li>对称功能基团支架设计：RFdiffusion可以根据用户指定的对称功能基团（如金属配位位点或病毒抗原表位）生成能够精确固定这些基团在空间中位置和取向的对称寡聚体结构。论文展示了对四面体Ni2+配位位点和三聚体SARS-CoV-2刺突蛋白结合位点进行对称功能基团支架设计，并且通过热力学和电镜验证了设计蛋白质的功能和结构。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>文献阅读类</category>
      </categories>
  </entry>
  <entry>
    <title>文献阅读：ECnet</title>
    <url>/2023/07/30/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AECnet/</url>
    <content><![CDATA[<ul>
<li><p>本文介绍了一种新的机器学习算法，可以利用蛋白质序列的进化信息来预测蛋白质的功能适应性，从而辅助蛋白质设计和定向进化。</p>
</li>
<li><p>摘要概述了ECNet的主要特点和优势：</p>
<p>结合局部进化上下文和全局进化上下文。ECNet使用两种类型的表示来捕捉蛋白质序列中的进化信息，<strong>一种是基于特定蛋白质的同源序列计算的局部进化上下文表示，另一种是基于大规模蛋白质序列数据库学习的全局进化上下文表示。</strong>这两种表示分别反映了蛋白质序列中残基之间的协变关系和表现力，以及序列中的语义和结构特征。通过将这两种表示结合起来，ECNet能够更准确地捕捉蛋白质功能的细微差异。</p>
<p>使用深度神经网络来学习序列到功能的映射。<strong>ECNet使用基于长短期记忆网络（LSTM）和注意力机制（Attention）的深度神经网络来建立预测模型，该网络能够学习序列到功能之间的复杂非线性映射关系</strong>。通过使用<strong>深度突变扫描或随机突变</strong>等实验方法测量得到的功能适应性数据作为监督信号，ECNet能够<strong>通过反向传播算法来更新网络参数</strong>，从而提高预测性能。</p>
<p>在多个数据集和实验中表现出色。ECNet在多个基准实验中都取得了优异的性能，包括与其他方法在~50个深度突变扫描数据集上的比较，以及在组合突变数据集上的泛化能力。此外，ECNet还在指导TEM-1 β-内酰胺酶工程中得到了应用和验证，证明了其在蛋白质工程中的有效性和实用性。，以及在多个数据集和实验中的表现和应用。</p>
</li>
<li><p>文件的正文分为以下几个部分：</p>
<ul>
<li>引言部分介绍了蛋白质工程的背景和挑战，以及现有的机器学习方法的局限性和不足。然后提出了ECNet的主要思想和创新点，即结合了针对特定蛋白质的局部进化上下文和从大规模蛋白质序列数据库中学习的全局进化上下文，以及使用深度神经网络来学习序列到功能的映射。</li>
<li>方法部分详细描述了ECNet的技术细节，包括如何构建蛋白质序列的表示，如何使用语言模型和直接耦合分析模型来捕捉进化上下文，如何使用循环神经网络和注意力机制来建立预测模型，以及如何使用深度突变扫描或随机突变数据来训练和评估模型。</li>
<li>结果部分展示了ECNet在多个基准实验中的性能，包括与其他方法在~50个深度突变扫描数据集上的比较，以及在组合突变数据集上的泛化能力。此外，还展示了ECNet在指导TEM-1 β-内酰胺酶工程中的应用和验证，证明了ECNet能够成功地发现具有改善抗生素抗性的变体。</li>
<li>讨论部分总结了ECNet的主要贡献和优势，分析了影响ECNet性能的因素，讨论了ECNet在蛋白质工程中的实际应用场景和潜在价值，以及未来可能的改进方向。</li>
</ul>
</li>
</ul>
<p><strong>方法部分主要包括以下几个步骤</strong>：</p>
<ul>
<li>构建蛋白质序列的表示。为了捕捉蛋白质序列中的进化信息，ECNet使用了两种类型的表示，<strong>一种是局部进化上下文表示，另一种是全局进化上下文表示</strong>。局部进化上下文表示是基于特定蛋白质的同源序列，使用直接耦合分析模型（DCA）来计算序列中每对残基之间的相互作用强度，从而反映出残基之间的协变关系和表现力。全局进化上下文表示是基于大规模的蛋白质序列数据库，使用语言模型（LM）来学习序列中每个残基出现的概率，从而反映出序列中的语义和结构特征。这两种表示都是使用神经网络来实现的，具体细节可以参考原文中的公式和图示。</li>
<li>使用语言模型和直接耦合分析模型来捕捉进化上下文。语言模型是一种无监督的机器学习模型，它可以从大量的文本数据中学习语言的规律和统计特性。在这里，作者将蛋白质序列视为一种特殊的语言，<strong>并使用UniProt或Pfam等数据库中的蛋白质序列作为训练数据，来训练一个基于长短期记忆网络（LSTM）的语言模型</strong>。该模型的目标是根据给定的上下文，预测序列中某个位置出现某个氨基酸的概率。通过这样的训练，语言模型可以学习到蛋白质序列中隐含的语法和语义信息，以及与结构和稳定性相关的信息。<strong>直接耦合分析模型是一种基于马尔可夫随机场（MRF）的生成模型，它可以从特定蛋白质的同源序列中学习残基之间的相互作用关系。</strong>该模型使用多重序列比对（MSA）作为输入，定义一个能量函数来描述序列生成的概率，其中能量函数由单点项和双点项组成，分别表示每个残基和每对残基对能量函数的贡献。通过最大似然估计或近似推断方法，可以得到单点项和双点项的参数，从而反映出残基之间的协变程度和表达力。</li>
<li>使用深度神经网络来建立预测模型。将局部进化上下文表示和全局进化上下文表示结合起来，就得到了蛋白质序列的完整表示。该表示作为输入，被送入一个基于LSTM和注意力机制（Attention）的深度神经网络中，该网络可以学习序列到功能之间的复杂非线性映射关系。该网络使用深度突变扫描或随机突变等实验方法测量得到的功能适应性数据作为监督信号，通过反向传播算法来更新网络参数，从而提高预测性能。</li>
</ul>
<p><strong>结果部分主要展示了ECNet在多个数据集和实验中的表现和应用，包括以下几个方面：</strong></p>
<ul>
<li>ECNet在Envision数据集上与其他蛋白质变体表示方法进行了比较，包括Yang et al.提出的基于Doc2Vec模型的固定长度向量表示，以及Envision模型提出的基于27个生物学、结构学和物理化学特征的变体表示。结果表明，ECNet在所有12个数据集上都取得了更高的斯皮尔曼相关系数和AUROC分数，说明ECNet的序列表示方法更能捕捉蛋白质功能的细微差异。</li>
<li>ECNet在DeepSequence数据集上与其他序列建模方法进行了比较，包括三种无监督的生成模型（EVmutation，DeepSequence和Autoregressive），以及两种有监督的模型（UniRep和TAPE），后者都使用了预训练的蛋白质语言模型来学习序列表示。结果表明，ECNet在几乎所有的数据集上都取得了最高的斯皮尔曼相关系数，说明ECNet的序列建模方法更能准确地预测蛋白质功能适应性。</li>
<li>ECNet在组合突变数据集上展示了其泛化能力，即使用低阶突变数据来训练模型，然后用来预测高阶突变的功能适应性。结果表明，ECNet在六个蛋白质的双突变数据集上都优于其他方法，并且能够从单突变数据中推断出四突变的功能适应性。此外，ECNet还能够准确地捕捉残基之间的表现力效应，并与实验观察到的表现力效应呈正相关。</li>
<li>ECNet在TEM-1 β-内酰胺酶工程中的应用和验证。作者使用ECNet来指导TEM-1 β-内酰胺酶的定向进化，从单突变和连续双突变数据中训练模型，并用来预测高阶突变的抗生素抗性。作者从ECNet的预测结果中选择了37个新颖的高阶突变，并在实验室中构建和筛选它们。结果表明，大多数由ECNet预测的突变都比野生型TEM-1具有更高的抗生素抗性，并且有些突变甚至比训练数据中最好的突变还要好。这些结果证明了ECNet在蛋白质工程中的有效性和实用性。</li>
</ul>
]]></content>
      <categories>
        <category>文献阅读类</category>
      </categories>
  </entry>
  <entry>
    <title>文献阅读：人工智能下的酶工程</title>
    <url>/2023/07/30/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8B%E7%9A%84%E9%85%B6%E5%B7%A5%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h1><p>​		在重组蛋白技术发展成熟之后，蛋白质一级序列中的氨基酸可以被精准控制和编辑。在此基础上，蛋白质层面有关工作机理和理化性质的先验知识可以被转化成蛋白质序列设计方案。理性设计方法便是依赖这些知识判断具体氨基酸替换后是否会增强蛋白质的特定性质，或者改造蛋白质的特异性功能，<strong>但这种方法不适用于工作机理或结构未知的蛋白质</strong>  。</p>
<p>​		定向进化策略跨越了理性设计的知识壁垒，该方法通过随机突变和高通量筛选加速蛋白质向特定指标的进化过程，研究人员不再需要了解蛋白质的结构和工作机理  。<strong>由于酶工程的实验结果可以按照统一的标准被收集</strong>，随着实验结果的累积，大量的数据推动了数据驱动的酶工程的发展。人工智能为酶工程提供了新的工具，机器学习方法与深度神经网络在该领域得到了有效利用与发展。  </p>
<h2 id="定向进化与半理性设计"><a href="#定向进化与半理性设计" class="headerlink" title="定向进化与半理性设计"></a>定向进化与半理性设计</h2><p>​	定向进化技术的核心思路可以被分为两步，先构建大规模随机突变文库，再通过高通量实验筛选得到有益突变体。<strong>这样的过程往往会被迭代实施很多轮</strong>，直到有益突变位点积累到使蛋白质性质满足预期的数量。另一个具有代表性意义的工作是Stemmer在1994年提出的利用DNA重组构建随机突变文库，<strong>这项技术利用PCR扩增目标蛋白的同源基因文库并将它们剪切成大量基因片段</strong>，通过无引物PCR技术重组后，基因片段会组成杂交基因并被克隆到表达载体中供后续筛选，得到的突变体会被用于构建新的DNA片段文库，有益的突变会在如此反复的筛选过程中累积。</p>
<p>​		Liebeton 团队将多种定向进化策略结合在一起来改造铜绿假单胞菌（Pseudomonas aeruginosa）中的细菌脂肪酶（bacterial lipase）。该团队先利用<strong>易错 PCR 技术</strong>，在多轮迭代过程中找到数个对蛋白质产物选择性影响较大的阳性单点突变体。<strong>然 后 在 这 些 阳 性 突 变 所 在 位 置 进 行 饱 和 突 变(saturation mutagenesis)，得到了之前随机突变过</strong><br><strong>程中漏选的更好的阳性突变</strong>。在这些结果的基础上，<strong>再利用定点突变技术（site-specific mutagenesis）重新设计多点突变</strong>，最终得到的突变体在特定产物的选择性上比野生型提高了 23.5 倍。这种将多个定向进化策略结合起来的方法降低了随机突变漏选优秀突变体的概率，同时为饱和突变技术提供了关键的氨基酸位点 。</p>
<p>​		共识序列（consensus sequence）是半理性设计中具有代表性的方法。蛋白质一级序列中氨基酸之间具有高度的进化相关性，<strong>从进化角度来看，对酶活性和稳定性产生重要影响的氨基酸很可能是保守的</strong>。在给定蛋白质家族的多重序列比对（multiple sequence alignment）中 ，某个位置上的保守氨基酸具有更高的频率，这些残基被认为是共识残基（consensus residue）。共识序列的核心思想是氨基酸频率反映了某些生物特性的相对重要性，在给定位置上用共识残基代替非共识残基往往能优化蛋白质性质 。<strong>这些计算设计方法着眼于单个或者多个突变给蛋白质结构和功能带来的具体影响</strong>，可以构建相比于定向进化更小也更合理的突变文库，大幅度降低了定向进化方案中筛选突变体的工作量，具体进行实验时不再需要高通量筛选方法。  </p>
<h2 id="人工智能助力酶工程"><a href="#人工智能助力酶工程" class="headerlink" title="人工智能助力酶工程"></a>人工智能助力酶工程</h2><p>​		机器学习的方法是将大量蛋白质信息按照一定方式编码，使计算机产生可以执行复杂决策的算法。  Capriotti等在 2004年利用 1615个单点突变数据训练单层感知机并预测蛋白质突变对热稳定性造成的影响，<strong>他们将测量蛋白质突变稳定性变化时的温度、pH 值、单点突变内容、溶液可及性以及单点突变周围氨基酸频率分布编码并输入到模型中</strong>，使模型在预测精度上超过了之前利用能量函数计算热稳定性变化的方法。这种编码方案只利用突变周围的氨基酸频率分布将蛋白质结构信息纳入考虑，该团队在 2005 年推出了基于支持向量机（SVM）的 I-Mutant2.0，在结构信息之外又成功编码了蛋白质序列信息 。</p>
<p>​		Laimer等不仅增加了编码内容，还通过整合的方法丰富了模型架构，使用的数据包括统计模型的打分，蛋白质残基数目、二级结构、溶液可及面积、质量、亲疏水性和等电点等理化性质。这些数据会被编码输入到3个模块中，3个模块包括具备单个隐藏层的人工神经网络（artificial neural networks）、支持向量机（SVM）和多重线性回归（multiple linear regression）。经过测试，该整合模型被证明具有超越以往模型的精度。</p>
<p>​		Dehouck 等选择利用多种统计势能的线性组合来预测突变带来的热稳定性变化，该方法在预测速度上相比其他方法有巨大的提升。Pires等则是利用支持向量机（SVM）<strong>整合了突变体阈值扫描矩阵 （mutation cutoff scanning matrix，mCSM）和定点诱变（site directed mutator，SDM）两种属性互补的方法，</strong>其中 mCSM 是一种利用结构特征预测错义突变（missense mutation）的机器学习方法，SDM 则是一种包含了同源蛋白进化信息的统计函数。 </p>
<p>​		蛋白质语言模型是这类语言模型在生物化学领域的迁移应用，它将 20 种天然氨基酸当作词汇，学习蛋白质一级序列中的语义和语序规则，以完成预测蛋白质功能、结构等下游任务。Facebook AI团队［30］在Transformer架构基础上开发了可以直接对蛋白质突变体进行非监督学习 （unsupervised learning）的蛋白质语言模型ESM-1v。该模型使用的训练集包括 9800 万条蛋白质序列，<strong>使用的训练方法为随机遮掩（masked training），即输入经过随机遮掩处理的残缺蛋白质序列，令模型通过未遮掩部分来预测被遮掩部分的残基类型。这样的训练方法可以让模型具备评估蛋白质中氨基酸保守性的能力</strong>，即某个序列中特定残基类型是否符合自然界中蛋白质语言的语义和语序规则。若突变体相比于野生型更加符合模型学习到的规则，模型就会对该突变体给出阳性打分。</p>
<p>​		蛋白质三级结构比一级序列包含更多的信息，尤其是蛋白内部氨基酸三维空间互作信息，目前通过实验解出的结构约 20 万条，远少于目前已知的序列数量。AlphaFold2作为深度学习模型，能够以极高的准确度根据序列预测蛋白质三级结构。ESM-IF1 模型使用经AlphaFold2 预测的 1200 万条蛋白质序列的结构进行训练，根据蛋白质骨架坐标预测其序列<strong>。模型架构方面 ESM-IF1 使用几何向量感知机（GVP）来编码蛋白质三维结构</strong>，该模块可以保证编码信息向量的等变性以及标量的不变性。  </p>
<p>​		ESM-1v、 ESM-IF1 和 MSA-Transformer 等 无监督模型不需要经过额外训练即可直接在特定蛋白质上执行突变体的预测任务，但打分的规则并不是蛋白质活性或者稳定性等具体指标，而是突变体相比于野生型是否更加符合模型学习到的规则。</p>
<p>​		</p>
<p>​		相比无监督学习，监督学习（supervised learning）通过学习某个特定蛋白的突变数据（序列和性质的对应关系）可以更加准确地预测该蛋白突变体的性质。<strong>无监督模型已经通过训练学习到了蛋白质的编码方式，因此有监督模型引入无监督模型作为编码模块可以在准确预测特定蛋白质突变体性质的同时保证预测结果符合自然规律。</strong>ESM-1b模型使用34层的Transformer在UR50&#x2F;S数据库上进行预训练，然后使用特定蛋白的突变数据对模型进行微调（fine-tune），得到了相比以往方法更高的精度。ECNet 是利用进化环境预测特定蛋白质的突变效果的有监督模型，该模型使用无监督模型 TAPE 编码蛋白质序列特征，并且从 MSA 中学习了残基之间的进化约束。ECNet在多个数据集上表现出高于 TAPE 模型的预测精度，证明 MSA中包含的进化信息对预测蛋白质突变效果有正向作用。SESNet是整合了蛋白质序列、MSA和结构信息的有监督模型，在多个数据集上的预测精度超过了现有的监督学习模型。</p>
<p>​		Rosetta的核心是一整套基于物理参数的分子势能和统计势能的分子力场，其包含了结构生物学中经常提到的氢键、盐桥、溶液可及面积、亲疏水性等作用项。  </p>
<p>​		整体来说，基于结构的蛋白质从头设计方法更加具有创新性和新颖性，但同时成功率也更低，一般需要在万这个数量级的设计序列库上做筛选才能找到阳性序列。这需要有针对性的高通量筛选实验方法，且同时受限于 Oligo合成技术的限制，不能在更长的蛋白质序列上做设计合成（一般不超过200个氨基酸）。</p>
<p>​		蛋白质语言模型首先在公共蛋白质序列数据库上进行预训练，模型学习到了蛋白质序列中氨基酸的排列规则（类似于蛋白质的一种语言规则），之后其可以对任何序列是否接近自然序列做出判断。一般来说，更符合自然序列特征的序列，意味着其具有更好的结构折叠能力和更好的表达能力以及水溶性。在需要对特定功能的蛋白质做设计之前，将预训练模型在这些特定家族的蛋白质序列上进行微调（finetune），然后其对特定功能的蛋白质序列具有更准确的生成和预测能力。  </p>
<p>​	使用计算机代替高通量筛选方法去探索庞大的序列空间可以大幅度缩小实验成本，相比定向进化方法，高精度的模型可以更快地找到最优突变体，从而减少实验周期。<strong>但是蛋白质多点位突变的序列空间非常庞大，即便使用计算方法也无法完全遍历，因此需要按照一定方法对序列空间进行采样。传统采样方法包括随机突变、贪婪算法和蒙特卡洛模拟退火等</strong>。其中随机突变方法即在序列空间中随机采样，采样结果将被计算方法筛选。这种采样方法效率较低，并且找到最优突变的概率严重依赖采样数量。贪婪算法先选择一批表现较好的突变体作为亲本（parent sequences），然后迭代组合这些突变生成子本（children sequences）。该方法可以有效探索高维突变的序列空间，但是探索内容受到亲本限制，无法在整个蛋白质的序列空间中进行有效检索。蒙特卡洛方法即在一个不具有物理意义的玻尔兹曼分布中采样。  </p>
]]></content>
      <categories>
        <category>文献阅读类</category>
      </categories>
  </entry>
  <entry>
    <title>文献阅读：人类抗体从一般蛋白质语言模型的有效进化</title>
    <url>/2023/07/30/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-1/</url>
    <content><![CDATA[<ul>
<li>这篇文章的主要研究目的是利用一种称为语言模型的算法，来高效地改进人类抗体的亲和力，即抗体与特定抗原结合的强度。亲和力是抗体治疗的重要指标，因为它影响了抗体对病原体的中和能力和耐受性。</li>
<li>语言模型是一种基于人工智能的算法，可以从大量的自然蛋白质序列中学习氨基酸排列的规律，从而预测哪些突变更有可能发生在自然进化中。作者假设，这些突变也更有可能提高抗体的亲和力，即使算法没有提供任何关于抗原、结合特异性或蛋白质结构的信息。</li>
<li>作者使用了两种通用的语言模型（ESM-1b和ESM-1v），分别在UniRef50和UniRef90这两个包含了数百万种自然蛋白质序列的数据集上训练。这些数据集只包含了少量与抗体相关的序列，并且早于SARS-CoV-2等新型病毒的发现和变异。因此，语言模型不能利用疾病特定的偏差来指导抗体进化，而必须依靠更一般的进化规律。</li>
<li>作者使用这些语言模型来计算所有单个氨基酸替换对抗体变量区（包括重链和轻链）的可能性，并选择了六个语言模型共同认可的比野生型更有可能发生的替换。作者在实验室中进行了两轮定向进化，分别测量了每种抗体8到14个单个替换变体和1到11个组合替换变体与抗原结合的强度。作者使用了生物层干涉仪（BLI）来测量单价片段抗原结合（Fab）区域或双价免疫球蛋白G（IgG）与抗原之间的解离常数（Kd），Kd越小表示亲和力越高。</li>
<li>作者选择了七种人类IgG抗体作为研究对象，它们分别针对冠状病毒、埃博拉病毒和甲型流感病毒等不同的病原体表面蛋白。这些抗体具有不同的成熟度和临床相关性，有些是从患者血清中分离出来的，有些是经过人工进化优化过的。作者使用不同类型的流感A病毒血凝素（HA）、埃博拉病毒糖蛋白（GP）或SARS-CoV-2刺突蛋白（Spike）作为筛选用的抗原。</li>
<li>作者发现，所有七种抗体都在两轮定向进化后获得了亲和力提高的变体，其中四种临床相关、高度成熟的抗体亲和力提高了最多七倍，三种未成熟或低成熟的抗体亲和力提高了最多160倍。许多设计的变体还显示出更好的热稳定性和对埃博拉病毒和SARS-CoV-2伪病毒的中和活性。语言模型推荐的变体中，有一部分是在自然进化或传统的亲和力成熟过程中很少出现的，表明语言模型能够探索出不同的进化路径。</li>
<li>作者还对变体进行了其他方面的表征，包括多特异性、免疫原性、结构稳定性等，发现变体没有显著增加与非目标蛋白的结合或T细胞介导的免疫反应，而且大多数变体保持了高于70°C的熔点。作者还与其他基于序列信息的方法进行了比较，发现通用的语言模型在推荐提高亲和力的变体方面优于专门针对抗体序列训练的语言模型或基于突变频率的方法。</li>
<li>作者最后测试了通用的语言模型是否能够指导其他类型蛋白质的进化，包括抗生素抗性、癌症药物抗性、酶活性或病毒复制适应性等不同的适应性压力。作者使用了高通量扫描突变实验的数据来验证语言模型推荐的变体，并发现在六个数据集中有五个能够显著富集高适应性值的变体。这些结果表明，语言模型能够学习到普遍适用于多种蛋白质家族和选择压力的进化规则。</li>
</ul>
]]></content>
      <categories>
        <category>文献阅读</category>
      </categories>
  </entry>
  <entry>
    <title>ECnet代码解读（一）</title>
    <url>/2023/08/10/ECnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<h1 id="数据处理部分代码"><a href="#数据处理部分代码" class="headerlink" title="数据处理部分代码"></a>数据处理部分代码</h1><p>#data.py#</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> Bio <span class="keyword">import</span> SeqIO</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, ShuffleSplit</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ecnet <span class="keyword">import</span> vocab</span><br><span class="line"><span class="keyword">from</span> ecnet.local_feature <span class="keyword">import</span> CCMPredEncoder</span><br><span class="line"><span class="keyword">from</span> ecnet.global_feature <span class="keyword">import</span> TAPEEncoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SequenceData</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequences, labels</span>):</span><br><span class="line">        self.sequences = sequences</span><br><span class="line">        self.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.sequences[index], self.labels[index]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MetagenesisData</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">index_encoding</span>(<span class="params">sequences</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Modified from https://github.com/openvax/mhcflurry/blob/master/mhcflurry/amino_acid.py#L110-L130</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    sequences: list of equal-length sequences</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    np.array with shape (#sequences, length of sequences)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    df = pd.DataFrame(<span class="built_in">iter</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> sequences)</span><br><span class="line">    encoding = df.replace(vocab.AMINO_ACID_INDEX)</span><br><span class="line">    encoding = encoding.values.astype(np.<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            train_tsv=<span class="literal">None</span>, test_tsv=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            fasta=<span class="literal">None</span>, ccmpred_output=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            use_loc_feat=<span class="literal">True</span>, use_glob_feat=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            split_ratio=[<span class="number">0.9</span>, <span class="number">0.1</span>],</span></span><br><span class="line"><span class="params">            random_seed=<span class="number">42</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        split_ratio: [train, valid] or [train, valid, test]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        self.train_tsv = train_tsv</span><br><span class="line">        self.test_tsv = test_tsv</span><br><span class="line">        self.fasta = fasta</span><br><span class="line">        self.use_loc_feat = use_loc_feat</span><br><span class="line">        self.use_glob_feat = use_glob_feat</span><br><span class="line">        self.split_ratio = split_ratio</span><br><span class="line">        self.rng = np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line">        self.native_sequence = self._read_native_sequence()</span><br><span class="line">        <span class="keyword">if</span> train_tsv <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.full_df = self._read_mutation_df(train_tsv)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.full_df = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_tsv <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(split_ratio) != <span class="number">3</span>:</span><br><span class="line">                split_ratio = [<span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]</span><br><span class="line">                warnings.warn(<span class="string">&quot;\nsplit_ratio should have 3 elements if test_tsv is None.&quot;</span> + \</span><br><span class="line">                    <span class="string">f&quot;Changing split_ratio to <span class="subst">&#123;split_ratio&#125;</span>. &quot;</span> + \</span><br><span class="line">                    <span class="string">&quot;Set to other values using --split_ratio.&quot;</span>)</span><br><span class="line">            self.train_df, self.valid_df, self.test_df = \</span><br><span class="line">                self._split_dataset_df(self.full_df, split_ratio)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(split_ratio) != <span class="number">2</span>:</span><br><span class="line">                split_ratio = [<span class="number">0.9</span>, <span class="number">0.1</span>]</span><br><span class="line">                warnings.warn(<span class="string">&quot;\nsplit_ratio should have 2 elements if test_tsv is provided. &quot;</span> + \</span><br><span class="line">                    <span class="string">f&quot;Changing split_ratio to <span class="subst">&#123;split_ratio&#125;</span>. &quot;</span> + \</span><br><span class="line">                    <span class="string">&quot;Set to other values using --split_ratio.&quot;</span>)</span><br><span class="line">            self.test_df = self._read_mutation_df(test_tsv)</span><br><span class="line">            <span class="keyword">if</span> self.full_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.train_df, self.valid_df, _ = \</span><br><span class="line">                    self._split_dataset_df(self.full_df, split_ratio)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.full_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.train_valid_df = pd.concat(</span><br><span class="line">                [self.train_df, self.valid_df]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">            self.ccmpred_encoder = CCMPredEncoder(</span><br><span class="line">                ccmpred_output=ccmpred_output, seq_len=<span class="built_in">len</span>(self.native_sequence))</span><br><span class="line">        <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">            self.tape_encoder = TAPEEncoder()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_read_native_sequence</span>(<span class="params">self</span>):</span><br><span class="line">        fasta = SeqIO.read(self.fasta, <span class="string">&#x27;fasta&#x27;</span>)</span><br><span class="line">        native_sequence = <span class="built_in">str</span>(fasta.seq)</span><br><span class="line">        <span class="keyword">return</span> native_sequence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_check_split_ratio</span>(<span class="params">self, split_ratio</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Modified from: https://github.com/pytorch/text/blob/3d28b1b7c1fb2ddac4adc771207318b0a0f4e4f9/torchtext/data/dataset.py#L284-L311</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        test_ratio = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(split_ratio, <span class="built_in">float</span>):</span><br><span class="line">            <span class="keyword">assert</span> <span class="number">0.</span> &lt; split_ratio &lt; <span class="number">1.</span>, (</span><br><span class="line">                <span class="string">&quot;Split ratio &#123;&#125; not between 0 and 1&quot;</span>.<span class="built_in">format</span>(split_ratio))</span><br><span class="line">            valid_ratio = <span class="number">1.</span> - split_ratio</span><br><span class="line">            <span class="keyword">return</span> (split_ratio, valid_ratio, test_ratio)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(split_ratio, <span class="built_in">list</span>):</span><br><span class="line">            length = <span class="built_in">len</span>(split_ratio)</span><br><span class="line">            <span class="keyword">assert</span> length == <span class="number">2</span> <span class="keyword">or</span> length == <span class="number">3</span>, (</span><br><span class="line">                <span class="string">&quot;Length of split ratio list should be 2 or 3, got &#123;&#125;&quot;</span>.<span class="built_in">format</span>(split_ratio))</span><br><span class="line">            ratio_sum = <span class="built_in">sum</span>(split_ratio)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> ratio_sum == <span class="number">1.</span>:</span><br><span class="line">                split_ratio = [<span class="built_in">float</span>(ratio) / ratio_sum <span class="keyword">for</span> ratio <span class="keyword">in</span> split_ratio]</span><br><span class="line">            <span class="keyword">if</span> length == <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">tuple</span>(split_ratio + [test_ratio])</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">tuple</span>(split_ratio)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Split ratio must be float or a list, got &#123;&#125;&#x27;</span></span><br><span class="line">                            .<span class="built_in">format</span>(<span class="built_in">type</span>(split_ratio)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_split_dataset_df</span>(<span class="params">self, input_df, split_ratio, resample_split=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Modified from:</span></span><br><span class="line"><span class="string">        https://github.com/pytorch/text/blob/3d28b1b7c1fb2ddac4adc771207318b0a0f4e4f9/torchtext/data/dataset.py#L86-L136</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        _rng = self.rng.randint(<span class="number">512</span>) <span class="keyword">if</span> resample_split <span class="keyword">else</span> self.rng</span><br><span class="line">        df = input_df.copy()</span><br><span class="line">        df = df.sample(frac=<span class="number">1</span>, random_state=_rng).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        N = <span class="built_in">len</span>(df)</span><br><span class="line">        train_ratio, valid_ratio, test_ratio = self._check_split_ratio(split_ratio)</span><br><span class="line">        train_len = <span class="built_in">int</span>(<span class="built_in">round</span>(train_ratio * N))</span><br><span class="line">        valid_len = N - train_len <span class="keyword">if</span> <span class="keyword">not</span> test_ratio <span class="keyword">else</span> <span class="built_in">int</span>(<span class="built_in">round</span>(valid_ratio * N))</span><br><span class="line"></span><br><span class="line">        train_df = df.iloc[:train_len].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        valid_df = df.iloc[train_len:train_len + valid_len].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        test_df = df.iloc[train_len + valid_len:].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> train_df, valid_df, test_df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mutation_to_sequence</span>(<span class="params">self, mutation</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        mutation: &#x27;;&#x27;.join(WiM) (wide-type W at position i mutated to M)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        sequence = self.native_sequence</span><br><span class="line">        <span class="keyword">for</span> mut <span class="keyword">in</span> mutation.split(<span class="string">&#x27;;&#x27;</span>):</span><br><span class="line">            wt_aa = mut[<span class="number">0</span>]</span><br><span class="line">            mt_aa = mut[-<span class="number">1</span>]</span><br><span class="line">            pos = <span class="built_in">int</span>(mut[<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">assert</span> wt_aa == sequence[pos - <span class="number">1</span>],\</span><br><span class="line">                    <span class="string">&quot;%s: %s-&gt;%s (fasta WT: %s)&quot;</span>%(pos, wt_aa, mt_aa, sequence[pos - <span class="number">1</span>])</span><br><span class="line">            sequence = sequence[:(pos - <span class="number">1</span>)] + mt_aa + sequence[pos:]</span><br><span class="line">        <span class="keyword">return</span> sequence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mutations_to_sequences</span>(<span class="params">self, mutations</span>):</span><br><span class="line">        <span class="keyword">return</span> [self._mutation_to_sequence(m) <span class="keyword">for</span> m <span class="keyword">in</span> mutations]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_drop_invalid_mutation</span>(<span class="params">self, df</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Drop mutations WiM where</span></span><br><span class="line"><span class="string">        - W is incosistent with the i-th AA in native_sequence</span></span><br><span class="line"><span class="string">        - M is ambiguous, e.g., &#x27;X&#x27;</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        flags = []</span><br><span class="line">        <span class="keyword">for</span> mutation <span class="keyword">in</span> df[<span class="string">&#x27;mutation&#x27;</span>].values:</span><br><span class="line">            <span class="keyword">for</span> mut <span class="keyword">in</span> mutation.split(<span class="string">&#x27;;&#x27;</span>):</span><br><span class="line">                wt_aa = mut[<span class="number">0</span>]</span><br><span class="line">                mt_aa = mut[-<span class="number">1</span>]</span><br><span class="line">                pos = <span class="built_in">int</span>(mut[<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">                valid = <span class="literal">True</span> <span class="keyword">if</span> wt_aa == self.native_sequence[pos - <span class="number">1</span>] <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">                valid = valid <span class="keyword">and</span> (mt_aa <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;X&#x27;</span>])</span><br><span class="line">            flags.append(valid)</span><br><span class="line">        df = df[flags].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_read_mutation_df</span>(<span class="params">self, tsv</span>):</span><br><span class="line">        df = pd.read_table(tsv)</span><br><span class="line">        df = self._drop_invalid_mutation(df)</span><br><span class="line">        df[<span class="string">&#x27;sequence&#x27;</span>] = self._mutations_to_sequences(df[<span class="string">&#x27;mutation&#x27;</span>].values)</span><br><span class="line">        <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_seq_enc</span>(<span class="params">self, sequences</span>):</span><br><span class="line">        seq_enc = index_encoding(sequences)</span><br><span class="line">        seq_enc = torch.from_numpy(seq_enc.astype(np.<span class="built_in">int</span>))</span><br><span class="line">        <span class="keyword">return</span> seq_enc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_loc_feat</span>(<span class="params">self, sequences</span>):</span><br><span class="line">        feat = self.ccmpred_encoder.encode(sequences)</span><br><span class="line">        feat = torch.from_numpy(feat).<span class="built_in">float</span>()</span><br><span class="line">        <span class="keyword">return</span> feat</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_glob_feat</span>(<span class="params">self, sequences</span>):</span><br><span class="line">        feat = self.tape_encoder.encode(sequences)</span><br><span class="line">        feat = torch.from_numpy(feat).<span class="built_in">float</span>()</span><br><span class="line">        <span class="keyword">return</span> feat</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_data</span>(<span class="params">self, mode, return_df=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            df = self.train_df.copy()</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&#x27;valid&#x27;</span>:</span><br><span class="line">            df = self.valid_df.copy()</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">            df = self.test_df.copy()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        sequences = df[<span class="string">&#x27;sequence&#x27;</span>].values</span><br><span class="line">        seq_enc = self.encode_seq_enc(sequences)</span><br><span class="line">        <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">            loc_feat = self.encode_loc_feat(sequences)</span><br><span class="line">        <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">            glob_feat = self.encode_glob_feat(sequences)</span><br><span class="line"></span><br><span class="line">        labels = df[<span class="string">&#x27;score&#x27;</span>].values</span><br><span class="line">        labels = torch.from_numpy(labels.astype(np.float32))</span><br><span class="line"></span><br><span class="line">        samples = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df)):</span><br><span class="line">            sample = &#123;</span><br><span class="line">                <span class="string">&#x27;sequence&#x27;</span>:sequences[i],</span><br><span class="line">                <span class="string">&#x27;label&#x27;</span>:labels[i],</span><br><span class="line">                <span class="string">&#x27;seq_enc&#x27;</span>: seq_enc[i],</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">                sample[<span class="string">&#x27;loc_feat&#x27;</span>] = loc_feat[i]</span><br><span class="line">            <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">                sample[<span class="string">&#x27;glob_feat&#x27;</span>] = glob_feat[i]</span><br><span class="line">            samples.append(sample)</span><br><span class="line">        data = MetagenesisData(samples)</span><br><span class="line">        <span class="keyword">if</span> return_df:</span><br><span class="line">            <span class="keyword">return</span> data, df</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_dataloader</span>(<span class="params">self, mode, batch_size=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">            return_df=<span class="literal">False</span>, resample_train_valid=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> resample_train_valid:</span><br><span class="line">            self.train_df, self.valid_df, _ = \</span><br><span class="line">                self._split_dataset_df(</span><br><span class="line">                    self.train_valid_df, self.split_ratio[:<span class="number">2</span>], resample_split=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&#x27;train_valid&#x27;</span>:</span><br><span class="line">            train_data, train_df = self.build_data(<span class="string">&#x27;train&#x27;</span>, return_df=<span class="literal">True</span>)</span><br><span class="line">            valid_data, valid_df = self.build_data(<span class="string">&#x27;valid&#x27;</span>, return_df=<span class="literal">True</span>)</span><br><span class="line">            train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)</span><br><span class="line">            valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)</span><br><span class="line">            <span class="keyword">if</span> return_df:</span><br><span class="line">                <span class="keyword">return</span> (train_loader, train_df), (valid_loader, valid_df)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> train_loader, valid_loader</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">            test_data, test_df = self.build_data(<span class="string">&#x27;test&#x27;</span>, return_df=<span class="literal">True</span>)</span><br><span class="line">            test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)</span><br><span class="line">            <span class="keyword">if</span> return_df:</span><br><span class="line">                <span class="keyword">return</span> test_loader, test_df</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> test_loader</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    protein_name = <span class="string">&#x27;gb1&#x27;</span></span><br><span class="line">    dataset_name = <span class="string">&#x27;Envision_Gray2018&#x27;</span></span><br><span class="line">    dataset = Dataset(</span><br><span class="line">        train_tsv=<span class="string">f&#x27;../../output/mutagenesis/<span class="subst">&#123;dataset_name&#125;</span>/<span class="subst">&#123;protein_name&#125;</span>/data.tsv&#x27;</span>,</span><br><span class="line">        fasta=<span class="string">f&#x27;../../output/mutagenesis/<span class="subst">&#123;dataset_name&#125;</span>/<span class="subst">&#123;protein_name&#125;</span>/native_sequence.fasta&#x27;</span>,</span><br><span class="line">        ccmpred_output=<span class="string">f&#x27;../../output/homologous/<span class="subst">&#123;dataset_name&#125;</span>/<span class="subst">&#123;protein_name&#125;</span>/hhblits/ccmpred/<span class="subst">&#123;protein_name&#125;</span>.braw&#x27;</span>,</span><br><span class="line">        split_ratio=[<span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0.2</span>],</span><br><span class="line">        use_loc_feat=<span class="literal">False</span>, use_glob_feat=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># dataset.build_data(&#x27;train&#x27;)</span></span><br><span class="line">    (loader, df), (_, _) = dataset.get_dataloader(<span class="string">&#x27;train_valid&#x27;</span>,</span><br><span class="line">        batch_size=<span class="number">32</span>, return_df=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(df.head())</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(loader.__iter__()))</span><br><span class="line">    (loader, df), (_, _) = dataset.get_dataloader(<span class="string">&#x27;train_valid&#x27;</span>,</span><br><span class="line">        batch_size=<span class="number">32</span>, return_df=<span class="literal">True</span>, resample_train_valid=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(df.head())</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(loader.__iter__()))</span><br><span class="line">    loader, df = dataset.get_dataloader(<span class="string">&#x27;test&#x27;</span>,</span><br><span class="line">        batch_size=<span class="number">32</span>, return_df=<span class="literal">True</span>, resample_train_valid=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">next</span>(loader.__iter__()))</span><br></pre></td></tr></table></figure>

<h2 id="数据加载和预处理"><a href="#数据加载和预处理" class="headerlink" title="数据加载和预处理"></a>数据加载和预处理</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SequenceData</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sequences, labels</span>):</span><br><span class="line">        self.sequences = sequences</span><br><span class="line">        self.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.labels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.sequences[index], self.labels[index]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MetagenesisData</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br></pre></td></tr></table></figure>

<ol>
<li><strong><code>SequenceData</code> 类</strong>：<ul>
<li>**<code>__init__(self, sequences, labels)</code>**：构造函数接受序列和标签作为输入，并将它们存储为类的属性。序列可以是一系列的特征向量，标签通常是相应的目标值或类别标签。</li>
<li>**<code>__len__(self)</code>**：此方法返回数据集的大小，即标签的数量。这对于迭代和批量处理数据集非常重要。</li>
<li>**<code>__getitem__(self, index)</code>**：此方法允许通过索引访问数据集中的特定项。返回的是给定索引处的序列和标签。</li>
</ul>
</li>
<li><strong><code>MetagenesisData</code> 类</strong>：<ul>
<li>**<code>__init__(self, data)</code>**：构造函数接受数据作为输入，并将其存储为类的属性。这些数据可能是一个复杂的结构，包括特征和标签。</li>
<li>**<code>__len__(self)</code>**：和上面一样，此方法返回数据集的大小。</li>
<li>**<code>__getitem__(self, index)</code>**：此方法允许通过索引访问数据集中的特定项。返回的是给定索引处的数据项。</li>
</ul>
</li>
</ol>
<p>这两个类的主要作用是封装数据，并提供一种标准化的方式来访问和迭代数据。这对于训练和评估机器学习模型非常重要，因为它允许模型以一致的方式处理数据，而无需关心数据的底层表示和结构。</p>
<p>通过将数据封装在这些类中，可以更容易地与 PyTorch 的其他组件（例如 DataLoader）集成，从而实现数据的批量加载、随机抽样和多线程加载等功能。这有助于提高训练和评估过程的效率和灵活性。</p>
<h2 id="编码和数据准备"><a href="#编码和数据准备" class="headerlink" title="编码和数据准备"></a>编码和数据准备</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">index_encoding</span>(<span class="params">sequences</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Modified from https://github.com/openvax/mhcflurry/blob/master/mhcflurry/amino_acid.py#L110-L130</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    sequences: list of equal-length sequences</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    np.array with shape (#sequences, length of sequences)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    df = pd.DataFrame(<span class="built_in">iter</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> sequences)</span><br><span class="line">    encoding = df.replace(vocab.AMINO_ACID_INDEX)</span><br><span class="line">    encoding = encoding.values.astype(np.<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dataset</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            train_tsv=<span class="literal">None</span>, test_tsv=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            fasta=<span class="literal">None</span>, ccmpred_output=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            use_loc_feat=<span class="literal">True</span>, use_glob_feat=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            split_ratio=[<span class="number">0.9</span>, <span class="number">0.1</span>],</span></span><br><span class="line"><span class="params">            random_seed=<span class="number">42</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        split_ratio: [train, valid] or [train, valid, test]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        self.train_tsv = train_tsv</span><br><span class="line">        self.test_tsv = test_tsv</span><br><span class="line">        self.fasta = fasta</span><br><span class="line">        self.use_loc_feat = use_loc_feat</span><br><span class="line">        self.use_glob_feat = use_glob_feat</span><br><span class="line">        self.split_ratio = split_ratio</span><br><span class="line">        self.rng = np.random.RandomState(random_seed)</span><br><span class="line"></span><br><span class="line">        self.native_sequence = self._read_native_sequence()</span><br><span class="line">        <span class="keyword">if</span> train_tsv <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.full_df = self._read_mutation_df(train_tsv)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.full_df = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_tsv <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(split_ratio) != <span class="number">3</span>:</span><br><span class="line">                split_ratio = [<span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]</span><br><span class="line">                warnings.warn(<span class="string">&quot;\nsplit_ratio should have 3 elements if test_tsv is None.&quot;</span> + \</span><br><span class="line">                    <span class="string">f&quot;Changing split_ratio to <span class="subst">&#123;split_ratio&#125;</span>. &quot;</span> + \</span><br><span class="line">                    <span class="string">&quot;Set to other values using --split_ratio.&quot;</span>)</span><br><span class="line">            self.train_df, self.valid_df, self.test_df = \</span><br><span class="line">                self._split_dataset_df(self.full_df, split_ratio)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(split_ratio) != <span class="number">2</span>:</span><br><span class="line">                split_ratio = [<span class="number">0.9</span>, <span class="number">0.1</span>]</span><br><span class="line">                warnings.warn(<span class="string">&quot;\nsplit_ratio should have 2 elements if test_tsv is provided. &quot;</span> + \</span><br><span class="line">                    <span class="string">f&quot;Changing split_ratio to <span class="subst">&#123;split_ratio&#125;</span>. &quot;</span> + \</span><br><span class="line">                    <span class="string">&quot;Set to other values using --split_ratio.&quot;</span>)</span><br><span class="line">            self.test_df = self._read_mutation_df(test_tsv)</span><br><span class="line">            <span class="keyword">if</span> self.full_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.train_df, self.valid_df, _ = \</span><br><span class="line">                    self._split_dataset_df(self.full_df, split_ratio)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.full_df <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.train_valid_df = pd.concat(</span><br><span class="line">                [self.train_df, self.valid_df]).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">            self.ccmpred_encoder = CCMPredEncoder(</span><br><span class="line">                ccmpred_output=ccmpred_output, seq_len=<span class="built_in">len</span>(self.native_sequence))</span><br><span class="line">        <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">            self.tape_encoder = TAPEEncoder()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_read_native_sequence</span>(<span class="params">self</span>):</span><br><span class="line">        fasta = SeqIO.read(self.fasta, <span class="string">&#x27;fasta&#x27;</span>)</span><br><span class="line">        native_sequence = <span class="built_in">str</span>(fasta.seq)</span><br><span class="line">        <span class="keyword">return</span> native_sequence</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="index-encoding-函数"><a href="#index-encoding-函数" class="headerlink" title="index_encoding 函数"></a><code>index_encoding</code> 函数</h3><p>这个函数用于将氨基酸序列转换为整数索引编码。</p>
<ul>
<li><strong>参数</strong>：<code>sequences</code> - 等长序列的列表。</li>
<li><strong>返回值</strong>：整数编码的 NumPy 数组，形状为 (#序列, 序列长度)。</li>
</ul>
<p>流程：</p>
<ol>
<li>使用 Pandas 的 DataFrame 将序列转换为表格形式。</li>
<li>使用 <code>vocab.AMINO_ACID_INDEX</code> 替换 DataFrame 中的值，将氨基酸转换为对应的索引。</li>
<li>将结果转换为整数类型的 NumPy 数组并返回。</li>
</ol>
<h3 id="Dataset-类"><a href="#Dataset-类" class="headerlink" title="Dataset 类"></a><code>Dataset</code> 类</h3><p>这个类是一个数据集的容器，用于存储和管理数据。</p>
<ul>
<li><strong>构造函数</strong>：接受许多参数，包括训练和测试的 TSV 文件路径、FASTA 文件路径、CCMPred 输出、本地特征和全局特征的使用、数据集的划分比例以及随机种子。</li>
</ul>
<p>流程：</p>
<ol>
<li>初始化参数和随机数生成器。</li>
<li>读取本地序列（通过 <code>_read_native_sequence</code> 方法）。</li>
<li>根据提供的 TSV 文件读取训练和测试数据。</li>
<li>根据 <code>split_ratio</code> 对数据进行划分。</li>
<li>如果使用本地特征，则初始化 CCMPred 编码器；如果使用全局特征，则初始化 TAPE 编码器。</li>
</ol>
<h4 id="read-native-sequence-方法"><a href="#read-native-sequence-方法" class="headerlink" title="_read_native_sequence 方法"></a><code>_read_native_sequence</code> 方法</h4><p>这个私有方法用于从 FASTA 文件中读取本地序列，并将其转换为字符串格式。</p>
<h2 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_split_dataset_df</span>(<span class="params">self, input_df, split_ratio, resample_split=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Modified from:</span></span><br><span class="line"><span class="string">    https://github.com/pytorch/text/blob/3d28b1b7c1fb2ddac4adc771207318b0a0f4e4f9/torchtext/data/dataset.py#L86-L136</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    _rng = self.rng.randint(<span class="number">512</span>) <span class="keyword">if</span> resample_split <span class="keyword">else</span> self.rng</span><br><span class="line">    df = input_df.copy()</span><br><span class="line">    df = df.sample(frac=<span class="number">1</span>, random_state=_rng).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    N = <span class="built_in">len</span>(df)</span><br><span class="line">    train_ratio, valid_ratio, test_ratio = self._check_split_ratio(split_ratio)</span><br><span class="line">    train_len = <span class="built_in">int</span>(<span class="built_in">round</span>(train_ratio * N))</span><br><span class="line">    valid_len = N - train_len <span class="keyword">if</span> <span class="keyword">not</span> test_ratio <span class="keyword">else</span> <span class="built_in">int</span>(<span class="built_in">round</span>(valid_ratio * N))</span><br><span class="line"></span><br><span class="line">    train_df = df.iloc[:train_len].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    valid_df = df.iloc[train_len:train_len + valid_len].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    test_df = df.iloc[train_len + valid_len:].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_df, valid_df, test_df</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>参数</strong>：<ul>
<li><code>input_df</code>：要划分的 Pandas DataFrame。</li>
<li><code>split_ratio</code>：一个包含划分比例的列表，如 <code>[0.7, 0.1, 0.2]</code>。</li>
<li><code>resample_split</code>：一个布尔值，如果为 <code>True</code>，则在划分数据之前重新采样随机种子。</li>
</ul>
</li>
<li><strong>流程</strong>：<ol>
<li>通过调用 <code>self.rng.randint(512)</code> 创建一个随机数生成器 <code>_rng</code>，或者使用现有的随机数生成器 <code>self.rng</code>。</li>
<li>复制输入的 DataFrame。</li>
<li>使用 <code>_rng</code> 随机化 DataFrame 的行顺序。</li>
<li>计算训练、验证和测试集的大小。首先通过调用 <code>_check_split_ratio</code> 方法验证和解析 <code>split_ratio</code>。然后，基于这些比例计算每个子集的长度。</li>
<li>使用 Pandas 的 <code>iloc</code> 方法，根据计算出的长度从随机化的 DataFrame 中切分训练、验证和测试集。</li>
<li>重置每个子集的索引并返回。</li>
</ol>
</li>
<li><strong>返回值</strong>：三个 DataFrame，分别代表训练集、验证集和测试集。</li>
</ul>
<h2 id="突变的写入和读取"><a href="#突变的写入和读取" class="headerlink" title="突变的写入和读取"></a>突变的写入和读取</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_mutation_to_sequence</span>(<span class="params">self, mutation</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    mutation: &#x27;;&#x27;.join(WiM) (wide-type W at position i mutated to M)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    sequence = self.native_sequence</span><br><span class="line">    <span class="keyword">for</span> mut <span class="keyword">in</span> mutation.split(<span class="string">&#x27;;&#x27;</span>):</span><br><span class="line">        wt_aa = mut[<span class="number">0</span>]</span><br><span class="line">        mt_aa = mut[-<span class="number">1</span>]</span><br><span class="line">        pos = <span class="built_in">int</span>(mut[<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">assert</span> wt_aa == sequence[pos - <span class="number">1</span>],\</span><br><span class="line">                <span class="string">&quot;%s: %s-&gt;%s (fasta WT: %s)&quot;</span>%(pos, wt_aa, mt_aa, sequence[pos - <span class="number">1</span>])</span><br><span class="line">        sequence = sequence[:(pos - <span class="number">1</span>)] + mt_aa + sequence[pos:]</span><br><span class="line">    <span class="keyword">return</span> sequence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_mutations_to_sequences</span>(<span class="params">self, mutations</span>):</span><br><span class="line">    <span class="keyword">return</span> [self._mutation_to_sequence(m) <span class="keyword">for</span> m <span class="keyword">in</span> mutations]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_drop_invalid_mutation</span>(<span class="params">self, df</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Drop mutations WiM where</span></span><br><span class="line"><span class="string">    - W is incosistent with the i-th AA in native_sequence</span></span><br><span class="line"><span class="string">    - M is ambiguous, e.g., &#x27;X&#x27;</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    flags = []</span><br><span class="line">    <span class="keyword">for</span> mutation <span class="keyword">in</span> df[<span class="string">&#x27;mutation&#x27;</span>].values:</span><br><span class="line">        <span class="keyword">for</span> mut <span class="keyword">in</span> mutation.split(<span class="string">&#x27;;&#x27;</span>):</span><br><span class="line">            wt_aa = mut[<span class="number">0</span>]</span><br><span class="line">            mt_aa = mut[-<span class="number">1</span>]</span><br><span class="line">            pos = <span class="built_in">int</span>(mut[<span class="number">1</span>:-<span class="number">1</span>])</span><br><span class="line">            valid = <span class="literal">True</span> <span class="keyword">if</span> wt_aa == self.native_sequence[pos - <span class="number">1</span>] <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">            valid = valid <span class="keyword">and</span> (mt_aa <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;X&#x27;</span>])</span><br><span class="line">        flags.append(valid)</span><br><span class="line">    df = df[flags].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_read_mutation_df</span>(<span class="params">self, tsv</span>):</span><br><span class="line">    df = pd.read_table(tsv)</span><br><span class="line">    df = self._drop_invalid_mutation(df)</span><br><span class="line">    df[<span class="string">&#x27;sequence&#x27;</span>] = self._mutations_to_sequences(df[<span class="string">&#x27;mutation&#x27;</span>].values)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>



<h3 id="1-mutation-to-sequence-方法"><a href="#1-mutation-to-sequence-方法" class="headerlink" title="1. _mutation_to_sequence 方法:"></a>1. <code>_mutation_to_sequence</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：将一个突变描述转换为一个完整的氨基酸序列。</p>
</li>
<li><p><strong>参数</strong>：<code>mutation</code> - 描述单个或多个突变的字符串，例如 “A5T;F10Y” 表示位置5的A突变为T，位置10的F突变为Y。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li><p>从原始的 <code>native_sequence</code> 开始。</p>
</li>
<li><p>对于 </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mutation</span><br></pre></td></tr></table></figure>

<p> 中的每个突变：</p>
<ul>
<li>检查突变是否与 <code>native_sequence</code> 中的氨基酸相匹配。</li>
<li>如果相匹配，则在序列中应用突变。</li>
</ul>
</li>
<li><p>返回突变后的序列。</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-mutations-to-sequences-方法"><a href="#2-mutations-to-sequences-方法" class="headerlink" title="2. _mutations_to_sequences 方法:"></a>2. <code>_mutations_to_sequences</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：将多个突变描述转换为多个氨基酸序列。</p>
</li>
<li><p><strong>参数</strong>：<code>mutations</code> - 描述多个突变的字符串列表。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>对于每个突变描述，调用 <code>_mutation_to_sequence</code> 方法并收集结果。</li>
</ul>
</li>
</ul>
<h3 id="3-drop-invalid-mutation-方法"><a href="#3-drop-invalid-mutation-方法" class="headerlink" title="3. _drop_invalid_mutation 方法:"></a>3. <code>_drop_invalid_mutation</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：从 DataFrame 中删除无效的突变。</p>
</li>
<li><p><strong>参数</strong>：<code>df</code> - 包含突变描述的 DataFrame。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>对于每个突变描述：<ul>
<li>检查突变是否与 <code>native_sequence</code> 中的氨基酸相匹配。</li>
<li>检查突变是否为不明确的氨基酸，例如 “X”。</li>
</ul>
</li>
<li>基于上述检查结果，保留有效的突变，并从 DataFrame 中删除无效的突变。</li>
</ul>
</li>
</ul>
<h3 id="4-read-mutation-df-方法"><a href="#4-read-mutation-df-方法" class="headerlink" title="4. _read_mutation_df 方法:"></a>4. <code>_read_mutation_df</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：从 TSV 文件中读取突变数据，并转换为相应的氨基酸序列。</p>
</li>
<li><p><strong>参数</strong>：<code>tsv</code> - TSV 文件的路径。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>使用 Pandas 从文件中读取数据。</li>
<li>调用 <code>_drop_invalid_mutation</code> 方法，去除无效的突变。</li>
<li>调用 <code>_mutations_to_sequences</code> 方法，将突变转换为相应的氨基酸序列，并将其添加到 DataFrame 中。</li>
</ul>
</li>
</ul>
<h2 id="特征编码和数据载入"><a href="#特征编码和数据载入" class="headerlink" title="特征编码和数据载入"></a>特征编码和数据载入</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">encode_seq_enc</span>(<span class="params">self, sequences</span>):</span><br><span class="line">    seq_enc = index_encoding(sequences)</span><br><span class="line">    seq_enc = torch.from_numpy(seq_enc.astype(np.<span class="built_in">int</span>))</span><br><span class="line">    <span class="keyword">return</span> seq_enc</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_loc_feat</span>(<span class="params">self, sequences</span>):</span><br><span class="line">    feat = self.ccmpred_encoder.encode(sequences)</span><br><span class="line">    feat = torch.from_numpy(feat).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> feat</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode_glob_feat</span>(<span class="params">self, sequences</span>):</span><br><span class="line">    feat = self.tape_encoder.encode(sequences)</span><br><span class="line">    feat = torch.from_numpy(feat).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> feat</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_data</span>(<span class="params">self, mode, return_df=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        df = self.train_df.copy()</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;valid&#x27;</span>:</span><br><span class="line">        df = self.valid_df.copy()</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        df = self.test_df.copy()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    sequences = df[<span class="string">&#x27;sequence&#x27;</span>].values</span><br><span class="line">    seq_enc = self.encode_seq_enc(sequences)</span><br><span class="line">    <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">        loc_feat = self.encode_loc_feat(sequences)</span><br><span class="line">    <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">        glob_feat = self.encode_glob_feat(sequences)</span><br><span class="line"></span><br><span class="line">    labels = df[<span class="string">&#x27;score&#x27;</span>].values</span><br><span class="line">    labels = torch.from_numpy(labels.astype(np.float32))</span><br><span class="line"></span><br><span class="line">    samples = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df)):</span><br><span class="line">        sample = &#123;</span><br><span class="line">            <span class="string">&#x27;sequence&#x27;</span>:sequences[i],</span><br><span class="line">            <span class="string">&#x27;label&#x27;</span>:labels[i],</span><br><span class="line">            <span class="string">&#x27;seq_enc&#x27;</span>: seq_enc[i],</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">            sample[<span class="string">&#x27;loc_feat&#x27;</span>] = loc_feat[i]</span><br><span class="line">        <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">            sample[<span class="string">&#x27;glob_feat&#x27;</span>] = glob_feat[i]</span><br><span class="line">        samples.append(sample)</span><br><span class="line">    data = MetagenesisData(samples)</span><br><span class="line">    <span class="keyword">if</span> return_df:</span><br><span class="line">        <span class="keyword">return</span> data, df</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader</span>(<span class="params">self, mode, batch_size=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">        return_df=<span class="literal">False</span>, resample_train_valid=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> resample_train_valid:</span><br><span class="line">        self.train_df, self.valid_df, _ = \</span><br><span class="line">            self._split_dataset_df(</span><br><span class="line">                self.train_valid_df, self.split_ratio[:<span class="number">2</span>], resample_split=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;train_valid&#x27;</span>:</span><br><span class="line">        train_data, train_df = self.build_data(<span class="string">&#x27;train&#x27;</span>, return_df=<span class="literal">True</span>)</span><br><span class="line">        valid_data, valid_df = self.build_data(<span class="string">&#x27;valid&#x27;</span>, return_df=<span class="literal">True</span>)</span><br><span class="line">        train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)</span><br><span class="line">        valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size)</span><br><span class="line">        <span class="keyword">if</span> return_df:</span><br><span class="line">            <span class="keyword">return</span> (train_loader, train_df), (valid_loader, valid_df)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> train_loader, valid_loader</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">        test_data, test_df = self.build_data(<span class="string">&#x27;test&#x27;</span>, return_df=<span class="literal">True</span>)</span><br><span class="line">        test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)</span><br><span class="line">        <span class="keyword">if</span> return_df:</span><br><span class="line">            <span class="keyword">return</span> test_loader, test_df</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> test_loader</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>



<h3 id="1-encode-seq-enc-方法"><a href="#1-encode-seq-enc-方法" class="headerlink" title="1. encode_seq_enc 方法:"></a>1. <code>encode_seq_enc</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：将氨基酸序列转换为整数索引编码。</p>
</li>
<li><p><strong>参数</strong>：<code>sequences</code> - 氨基酸序列的列表。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>调用之前定义的 <code>index_encoding</code> 函数来获取整数编码。</li>
<li>将 NumPy 数组转换为 PyTorch 张量并返回。</li>
</ul>
</li>
</ul>
<h3 id="2-encode-loc-feat-和-encode-glob-feat-方法"><a href="#2-encode-loc-feat-和-encode-glob-feat-方法" class="headerlink" title="2. encode_loc_feat 和 encode_glob_feat 方法:"></a>2. <code>encode_loc_feat</code> 和 <code>encode_glob_feat</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：使用 CCMPred 编码器和 TAPE 编码器对序列进行本地和全局特征编码。</p>
</li>
<li><p><strong>参数</strong>：<code>sequences</code> - 氨基酸序列的列表。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>调用编码器的 <code>encode</code> 方法来获取特征。</li>
<li>将特征转换为 PyTorch 张量并返回。</li>
</ul>
</li>
</ul>
<h3 id="3-build-data-方法"><a href="#3-build-data-方法" class="headerlink" title="3. build_data 方法:"></a>3. <code>build_data</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：根据给定的模式（训练、验证或测试）构建数据集。</p>
</li>
<li><p><strong>参数</strong>：<code>mode</code> - 指定要构建的数据集类型；<code>return_df</code> - 如果为 <code>True</code>，则返回原始 DataFrame。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>根据模式复制相应的 DataFrame。</li>
<li>对序列进行整数索引编码。</li>
<li>如果使用，对序列进行本地和全局特征编码。</li>
<li>从 DataFrame 中提取标签，并转换为 PyTorch 张量。</li>
<li>创建样本字典并收集。</li>
<li>使用 <code>MetagenesisData</code> 类创建数据集。</li>
<li>返回数据集（和可选的 DataFrame）。</li>
</ul>
</li>
</ul>
<h3 id="4-get-dataloader-方法"><a href="#4-get-dataloader-方法" class="headerlink" title="4. get_dataloader 方法:"></a>4. <code>get_dataloader</code> 方法:</h3><ul>
<li><p><strong>目的</strong>：根据给定的模式和批量大小构建数据加载器。</p>
</li>
<li><p><strong>参数</strong>：<code>mode</code>, <code>batch_size</code>, <code>return_df</code>, <code>resample_train_valid</code>。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>如果重新采样，重新划分训练和验证集。</li>
<li>根据模式构建数据集。</li>
<li>使用 PyTorch 的 <code>DataLoader</code> 创建数据加载器。</li>
<li>返回数据加载器（和可选的 DataFrame）。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>ECnet代码解读（二）</title>
    <url>/2023/08/10/ECnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p>#ecnet.py#</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ECNet</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">            output_dir=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            train_tsv=<span class="literal">None</span>, test_tsv=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            fasta=<span class="literal">None</span>, ccmpred_output=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">            use_loc_feat=<span class="literal">True</span>, use_glob_feat=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            split_ratio=[<span class="number">0.9</span>, <span class="number">0.1</span>],</span></span><br><span class="line"><span class="params">            random_seed=<span class="number">42</span>,</span></span><br><span class="line"><span class="params">            nn_name=<span class="string">&#x27;lstm&#x27;</span>, n_ensembles=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">            d_embed=<span class="number">20</span>, d_model=<span class="number">128</span>, d_h=<span class="number">128</span>, nlayers=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">            batch_size=<span class="number">128</span>, save_log=<span class="literal">False</span></span>):</span><br><span class="line"></span><br><span class="line">        self.dataset = Dataset(</span><br><span class="line">            train_tsv=train_tsv, test_tsv=test_tsv,</span><br><span class="line">            fasta=fasta, ccmpred_output=ccmpred_output,</span><br><span class="line">            use_loc_feat=use_loc_feat, use_glob_feat=use_glob_feat,</span><br><span class="line">            split_ratio=split_ratio,</span><br><span class="line">            random_seed=random_seed)</span><br><span class="line">        self.saver = Saver(output_dir=output_dir)</span><br><span class="line">        self.logger = Logger(logfile=self.saver.save_dir/<span class="string">&#x27;exp.log&#x27;</span> <span class="keyword">if</span> save_log <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        self.use_loc_feat = use_loc_feat</span><br><span class="line">        self.use_glob_feat = use_glob_feat</span><br><span class="line">        vocab_size = <span class="built_in">len</span>(vocab.AMINO_ACIDS)</span><br><span class="line">        seq_len = <span class="built_in">len</span>(self.dataset.native_sequence)</span><br><span class="line">        proj_loc_config = &#123;</span><br><span class="line">            <span class="string">&#x27;layer&#x27;</span>: nn.Linear,</span><br><span class="line">            <span class="string">&#x27;d_in&#x27;</span>: seq_len + <span class="number">1</span>,</span><br><span class="line">            <span class="string">&#x27;d_out&#x27;</span>: <span class="built_in">min</span>(<span class="number">128</span>, seq_len)</span><br><span class="line">        &#125;</span><br><span class="line">        proj_glob_config = &#123;</span><br><span class="line">            <span class="string">&#x27;layer&#x27;</span>: nn.Identity,</span><br><span class="line">            <span class="string">&#x27;d_in&#x27;</span>: <span class="number">768</span>,</span><br><span class="line">            <span class="string">&#x27;d_out&#x27;</span>: <span class="number">768</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        self.device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> nn_name <span class="keyword">in</span> [<span class="string">&#x27;lstm&#x27;</span>, <span class="string">&#x27;blstm&#x27;</span>]:</span><br><span class="line">            self.models = [LSTMPredictor(</span><br><span class="line">                d_embed=d_embed, d_model=d_model, d_h=d_h, nlayers=nlayers,</span><br><span class="line">                vocab_size=vocab_size, seq_len=seq_len,</span><br><span class="line">                bidirectional=<span class="literal">True</span> <span class="keyword">if</span> nn_name == <span class="string">&#x27;blstm&#x27;</span> <span class="keyword">else</span> <span class="literal">False</span>,</span><br><span class="line">                use_loc_feat=use_loc_feat, use_glob_feat=use_glob_feat,</span><br><span class="line">                proj_loc_config=proj_loc_config, proj_glob_config=proj_glob_config</span><br><span class="line">            ).to(self.device) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_ensembles)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">        self.criterion = F.mse_loss</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.optimizers = [optim.Adam(model.parameters()) <span class="keyword">for</span> model <span class="keyword">in</span> self.models]</span><br><span class="line">        self._test_pack = <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="init-方法"><a href="#init-方法" class="headerlink" title="__init__ 方法"></a><code>__init__</code> 方法</h3><p>这个构造函数负责初始化 <code>ECNet</code> 类的所有必要组件。</p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><ul>
<li><code>output_dir</code>：保存输出的目录路径。</li>
<li><code>train_tsv</code>, <code>test_tsv</code>：训练和测试数据的 TSV 文件路径。</li>
<li><code>fasta</code>, <code>ccmpred_output</code>：FASTA 文件路径和 CCMPred 输出路径。</li>
<li><code>use_loc_feat</code>, <code>use_glob_feat</code>：是否使用本地和全局特征。</li>
<li><code>split_ratio</code>：数据划分比例。</li>
<li><code>random_seed</code>：随机种子，用于确保可重现性。</li>
<li><code>nn_name</code>：神经网络类型，例如 LSTM 或双向 LSTM。</li>
<li><code>n_ensembles</code>：模型集合的大小。</li>
<li><code>d_embed</code>, <code>d_model</code>, <code>d_h</code>, <code>nlayers</code>：模型的维度和层数参数。</li>
<li><code>batch_size</code>：训练批次的大小。</li>
<li><code>save_log</code>：是否保存日志。</li>
</ul>
<h4 id="主要组件和流程"><a href="#主要组件和流程" class="headerlink" title="主要组件和流程"></a>主要组件和流程</h4><ol>
<li><strong>数据集</strong>：使用 <code>Dataset</code> 类创建数据集对象，包括所有训练、验证和测试数据。</li>
<li><strong>保存器</strong>：使用 <code>Saver</code> 类创建保存器对象，用于管理输出目录和文件保存。</li>
<li><strong>日志记录器</strong>：使用 <code>Logger</code> 类创建日志记录器，可选择保存到文件。</li>
<li><strong>模型参数</strong>：计算和设置模型的特定参数，如词汇表大小、序列长度和投影层配置。</li>
<li><strong>设备</strong>：检测是否有可用的 GPU，并据此设置 PyTorch 设备。</li>
<li><strong>模型创建</strong>：根据指定的网络类型和参数创建一个或多个 LSTM 预测器模型。</li>
<li><strong>损失函数</strong>：设置均方误差损失函数。</li>
<li><strong>优化器</strong>：为每个模型创建 Adam 优化器。</li>
</ol>
<h2 id="模型的加载点和重用接口设置"><a href="#模型的加载点和重用接口设置" class="headerlink" title="模型的加载点和重用接口设置"></a>模型的加载点和重用接口设置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@property</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">test_pack</span>(<span class="params">self</span>):</span><br><span class="line">     <span class="keyword">if</span> self._test_pack <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">         test_loader, test_df = self.dataset.get_dataloader(</span><br><span class="line">             <span class="string">&#x27;test&#x27;</span>, batch_size=self.batch_size, return_df=<span class="literal">True</span>)</span><br><span class="line">         self._test_pack = (test_loader, test_df)</span><br><span class="line">     <span class="keyword">return</span> self._test_pack</span><br><span class="line"></span><br><span class="line"><span class="meta"> @property</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">test_loader</span>(<span class="params">self</span>):</span><br><span class="line">     <span class="keyword">return</span> self.test_pack[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta"> @property</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">test_df</span>(<span class="params">self</span>):</span><br><span class="line">     <span class="keyword">return</span> self.test_pack[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">load_checkpoint</span>(<span class="params">self, checkpoint_dir</span>):</span><br><span class="line">     checkpoint_dir = pathlib.Path(checkpoint_dir)</span><br><span class="line">     <span class="keyword">if</span> <span class="keyword">not</span> checkpoint_dir.is_dir():</span><br><span class="line">         <span class="keyword">raise</span> ValueError(<span class="string">f&#x27;<span class="subst">&#123;checkpoint_dir&#125;</span> is not a directory&#x27;</span>)</span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.models)):</span><br><span class="line">         checkpoint_path = <span class="string">f&#x27;<span class="subst">&#123;checkpoint_dir&#125;</span>/model_<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>.pt&#x27;</span></span><br><span class="line">         self.logger.info(<span class="string">&#x27;Load pretrained model from &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(checkpoint_path))</span><br><span class="line">         pt = torch.load(checkpoint_path)</span><br><span class="line">         model_dict = self.models[i].state_dict()</span><br><span class="line">         model_pretrained_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pt[<span class="string">&#x27;model_state_dict&#x27;</span>].items() <span class="keyword">if</span> k <span class="keyword">in</span> model_dict&#125;</span><br><span class="line">         model_dict.update(model_pretrained_dict)</span><br><span class="line">         self.models[i].load_state_dict(model_dict)</span><br><span class="line">         self.optimizers[i].load_state_dict(pt[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">load_single_pretrained_model</span>(<span class="params">self, checkpoint_path, model=<span class="literal">None</span>, optimizer=<span class="literal">None</span>, is_resume=<span class="literal">False</span></span>):</span><br><span class="line">     self.logger.info(<span class="string">&#x27;Load pretrained model from &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(checkpoint_path))</span><br><span class="line">     pt = torch.load(checkpoint_path)</span><br><span class="line">     model_dict = model.state_dict()</span><br><span class="line">     model_pretrained_dict = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> pt[<span class="string">&#x27;model_state_dict&#x27;</span>].items() <span class="keyword">if</span> k <span class="keyword">in</span> model_dict&#125;</span><br><span class="line">     model_dict.update(model_pretrained_dict)</span><br><span class="line">     model.load_state_dict(model_dict)</span><br><span class="line">     optimizer.load_state_dict(pt[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">     <span class="keyword">return</span> (model, optimizer, pt[<span class="string">&#x27;log_info&#x27;</span>]) <span class="keyword">if</span> is_resume <span class="keyword">else</span> (model, optimizer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">save_checkpoint</span>(<span class="params">self, ckp_name=<span class="literal">None</span>, model_dict=<span class="literal">None</span>, opt_dict=<span class="literal">None</span>, log_info=<span class="literal">None</span></span>):</span><br><span class="line">     ckp = &#123;<span class="string">&#x27;model_state_dict&#x27;</span>: model_dict,</span><br><span class="line">            <span class="string">&#x27;optimizer_state_dict&#x27;</span>: opt_dict&#125;</span><br><span class="line">     ckp[<span class="string">&#x27;log_info&#x27;</span>] = log_info</span><br><span class="line">     self.saver.save_ckp(ckp, ckp_name)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="test-pack-属性"><a href="#test-pack-属性" class="headerlink" title="test_pack 属性"></a><code>test_pack</code> 属性</h3><p>这个属性返回一个包含测试数据加载器和测试 DataFrame 的元组。</p>
<ul>
<li><p>流程</p>
<p>：</p>
<ul>
<li>如果 <code>_test_pack</code> 尚未设置，则使用 <code>get_dataloader</code> 方法从数据集中获取测试数据加载器和测试 DataFrame。</li>
<li>将这些值存储在 <code>_test_pack</code> 中，并返回。</li>
</ul>
</li>
</ul>
<h3 id="test-loader-和-test-df-属性"><a href="#test-loader-和-test-df-属性" class="headerlink" title="test_loader 和 test_df 属性"></a><code>test_loader</code> 和 <code>test_df</code> 属性</h3><p>这两个属性是 <code>test_pack</code> 属性的便捷访问器，分别返回测试数据加载器和测试 DataFrame。</p>
<h3 id="load-checkpoint-方法"><a href="#load-checkpoint-方法" class="headerlink" title="load_checkpoint 方法"></a><code>load_checkpoint</code> 方法</h3><ul>
<li><p><strong>目的</strong>：从给定的目录加载预训练的模型检查点。</p>
</li>
<li><p><strong>参数</strong>：<code>checkpoint_dir</code> - 包含模型检查点文件的目录路径。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>验证给定的路径是否为目录。</li>
<li>对于每个模型：<ul>
<li>构建检查点文件的路径。</li>
<li>从文件加载检查点。</li>
<li>更新模型的状态字典并加载到模型中。</li>
<li>加载优化器的状态字典。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="load-single-pretrained-model-方法"><a href="#load-single-pretrained-model-方法" class="headerlink" title="load_single_pretrained_model 方法"></a><code>load_single_pretrained_model</code> 方法</h3><ul>
<li><p><strong>目的</strong>：从给定的文件路径加载单个预训练模型。</p>
</li>
<li><p><strong>参数</strong>：<code>checkpoint_path</code> - 检查点文件的路径；<code>model</code> - 要加载的模型；<code>optimizer</code> - 要加载的优化器；<code>is_resume</code> - 是否返回日志信息。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>从文件加载检查点。</li>
<li>更新模型的状态字典并加载到模型中。</li>
<li>加载优化器的状态字典。</li>
<li>返回模型、优化器和可选的日志信息。</li>
</ul>
</li>
</ul>
<h3 id="save-checkpoint-方法"><a href="#save-checkpoint-方法" class="headerlink" title="save_checkpoint 方法"></a><code>save_checkpoint</code> 方法</h3><ul>
<li><p><strong>目的</strong>：保存模型的检查点。</p>
</li>
<li><p><strong>参数</strong>：<code>ckp_name</code> - 检查点的名称；<code>model_dict</code> - 模型的状态字典；<code>opt_dict</code> - 优化器的状态字典；<code>log_info</code> - 日志信息。</p>
</li>
<li><p>流程</p>
<p>：</p>
<ul>
<li>创建一个包含状态字典和日志信息的检查点字典。</li>
<li>使用 <code>saver</code> 的 <code>save_ckp</code> 方法保存检查点。</li>
</ul>
</li>
</ul>
<h2 id="模型训练和测试"><a href="#模型训练和测试" class="headerlink" title="模型训练和测试"></a>模型训练和测试</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, epochs=<span class="number">1000</span>, log_freq=<span class="number">100</span>, eval_freq=<span class="number">50</span>,</span></span><br><span class="line"><span class="params">             patience=<span class="number">500</span>, save_checkpoint=<span class="literal">False</span>, resume_path=<span class="literal">None</span></span>):</span><br><span class="line">     <span class="keyword">assert</span> eval_freq &lt;= log_freq</span><br><span class="line">     monitoring_score = <span class="string">&#x27;corr&#x27;</span></span><br><span class="line">     <span class="keyword">for</span> midx, (model, optimizer) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(self.models, self.optimizers), start=<span class="number">1</span>):</span><br><span class="line">         (train_loader, train_df), (valid_loader, valid_df) = \</span><br><span class="line">             self.dataset.get_dataloader(</span><br><span class="line">                 <span class="string">&#x27;train_valid&#x27;</span>, self.batch_size,</span><br><span class="line">                 return_df=<span class="literal">True</span>, resample_train_valid=<span class="literal">True</span>)</span><br><span class="line">         <span class="keyword">if</span> resume_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">             model, optimizer, log_info = self.load_single_pretrained_model(</span><br><span class="line">                 <span class="string">&#x27;&#123;&#125;/model_&#123;&#125;.pt&#x27;</span>.<span class="built_in">format</span>(resume_path, midx),</span><br><span class="line">                 model=model, optimizer=optimizer, is_resume=<span class="literal">True</span>)</span><br><span class="line">             start_epoch = log_info[<span class="string">&#x27;epoch&#x27;</span>] + <span class="number">1</span></span><br><span class="line">             best_score = log_info[<span class="string">&#x27;best_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(monitoring_score)]</span><br><span class="line">         <span class="keyword">else</span>:</span><br><span class="line">             start_epoch = <span class="number">1</span></span><br><span class="line">             best_score = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">         best_model_state_dict = <span class="literal">None</span></span><br><span class="line">         stopper = EarlyStopping(patience=patience, eval_freq=eval_freq, best_score=best_score)</span><br><span class="line">         model.train()</span><br><span class="line">         <span class="keyword">try</span>:</span><br><span class="line">             <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(start_epoch, epochs + <span class="number">1</span>):</span><br><span class="line">                 time_start = time.time()</span><br><span class="line">                 tot_loss = <span class="number">0</span></span><br><span class="line">                 <span class="keyword">for</span> step, batch <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(train_loader, <span class="number">1</span>),</span><br><span class="line">                     leave=<span class="literal">False</span>, desc=<span class="string">f&#x27;M-<span class="subst">&#123;midx&#125;</span> E-<span class="subst">&#123;epoch&#125;</span>&#x27;</span>, total=<span class="built_in">len</span>(train_loader)):</span><br><span class="line">                     y = batch[<span class="string">&#x27;label&#x27;</span>].to(self.device)</span><br><span class="line">                     X = batch[<span class="string">&#x27;seq_enc&#x27;</span>].to(self.device)</span><br><span class="line">                     <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">                         loc_feat = batch[<span class="string">&#x27;loc_feat&#x27;</span>].to(self.device)</span><br><span class="line">                     <span class="keyword">else</span>:</span><br><span class="line">                         loc_feat = <span class="literal">None</span></span><br><span class="line">                     <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">                         glob_feat = batch[<span class="string">&#x27;glob_feat&#x27;</span>].to(self.device)</span><br><span class="line">                     <span class="keyword">else</span>:</span><br><span class="line">                         glob_feat = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">                     optimizer.zero_grad()</span><br><span class="line">                     output = model(X, glob_feat=glob_feat, loc_feat=loc_feat)</span><br><span class="line">                     output = output.view(-<span class="number">1</span>)</span><br><span class="line">                     loss = self.criterion(output, y)</span><br><span class="line"></span><br><span class="line">                     loss.backward()</span><br><span class="line">                     optimizer.step()</span><br><span class="line">                     tot_loss += loss.item()</span><br><span class="line"></span><br><span class="line">                 <span class="keyword">if</span> epoch % eval_freq == <span class="number">0</span>:</span><br><span class="line">                     val_results = self.test(test_model=model, test_loader=valid_loader,</span><br><span class="line">                         test_df=valid_df, mode=<span class="string">&#x27;val&#x27;</span>)</span><br><span class="line">                     model.train()</span><br><span class="line">                     is_best = stopper.update(val_results[<span class="string">&#x27;metric&#x27;</span>][monitoring_score])</span><br><span class="line">                     <span class="keyword">if</span> is_best:</span><br><span class="line">                         best_model_state_dict = copy.deepcopy(model.state_dict())</span><br><span class="line">                         <span class="keyword">if</span> save_checkpoint:</span><br><span class="line">                             self.save_checkpoint(ckp_name=<span class="string">&#x27;model_&#123;&#125;.pt&#x27;</span>.<span class="built_in">format</span>(midx),</span><br><span class="line">                                 model_dict=model.state_dict(),</span><br><span class="line">                                 opt_dict=optimizer.state_dict(),</span><br><span class="line">                                 log_info=&#123;</span><br><span class="line">                                     <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">                                     <span class="string">&#x27;best_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(monitoring_score): stopper.best_score,</span><br><span class="line">                                     <span class="string">&#x27;val_loss&#x27;</span>:val_results[<span class="string">&#x27;loss&#x27;</span>],</span><br><span class="line">                                     <span class="string">&#x27;val_results&#x27;</span>:val_results[<span class="string">&#x27;metric&#x27;</span>]</span><br><span class="line">                                 &#125;)</span><br><span class="line"></span><br><span class="line">                 <span class="keyword">if</span> epoch % log_freq == <span class="number">0</span>:</span><br><span class="line">                     train_results = self.test(test_model=model, test_loader=train_loader,</span><br><span class="line">                             test_df=train_df, mode=<span class="string">&#x27;val&#x27;</span>)</span><br><span class="line">                     <span class="keyword">if</span> (log_freq &lt;= eval_freq) <span class="keyword">or</span> (log_freq % eval_freq != <span class="number">0</span>):</span><br><span class="line">                         val_results = self.test(test_model=model, test_loader=valid_loader,</span><br><span class="line">                             test_df=valid_df, mode=<span class="string">&#x27;val&#x27;</span>)</span><br><span class="line">                     model.train()</span><br><span class="line">                     self.logger.info(</span><br><span class="line">                         <span class="string">&#x27;Model: &#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(midx, <span class="built_in">len</span>(self.models))</span><br><span class="line">                         + <span class="string">&#x27;\tEpoch: &#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, epochs)</span><br><span class="line">                         + <span class="string">&#x27;\tTrain loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(tot_loss / step)</span><br><span class="line">                         + <span class="string">&#x27;\tVal loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(val_results[<span class="string">&#x27;loss&#x27;</span>])                         </span><br><span class="line">                         + <span class="string">&#x27;\t&#x27;</span> + <span class="string">&#x27;\t&#x27;</span>.join([<span class="string">&#x27;Val &#123;&#125;: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(k, v) \</span><br><span class="line">                                 <span class="keyword">for</span> (k, v) <span class="keyword">in</span> val_results[<span class="string">&#x27;metric&#x27;</span>].items()])</span><br><span class="line">                         + <span class="string">&#x27;\tBest &#123;n&#125;: &#123;b:.4f&#125;\t&#x27;</span>.<span class="built_in">format</span>(n=monitoring_score, b=stopper.best_score)</span><br><span class="line">                         + <span class="string">&#x27;\t&#123;:.1f&#125; s/epoch&#x27;</span>.<span class="built_in">format</span>(time.time() - time_start)</span><br><span class="line">                         )</span><br><span class="line">                     time_start = time.time()</span><br><span class="line"></span><br><span class="line">                 <span class="keyword">if</span> stopper.early_stop:</span><br><span class="line">                     self.logger.info(<span class="string">&#x27;Eearly stop at epoch &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">                     <span class="keyword">break</span></span><br><span class="line">         <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">             self.logger.info(<span class="string">&#x27;Exiting model training from keyboard interrupt&#x27;</span>)</span><br><span class="line">         <span class="keyword">if</span> best_model_state_dict <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">             model.load_state_dict(best_model_state_dict)</span><br><span class="line"></span><br><span class="line">         test_results = self.test(test_model=model, model_label=<span class="string">&#x27;model_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(midx))</span><br><span class="line">         test_res_msg = <span class="string">&#x27;Testing Model &#123;&#125;: Loss: &#123;:.4f&#125;\t&#x27;</span>.<span class="built_in">format</span>(midx, test_results[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">         test_res_msg += <span class="string">&#x27;\t&#x27;</span>.join([<span class="string">&#x27;Test &#123;&#125;: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(k, v) \</span><br><span class="line">                             <span class="keyword">for</span> (k, v) <span class="keyword">in</span> test_results[<span class="string">&#x27;metric&#x27;</span>].items()])</span><br><span class="line">         self.logger.info(test_res_msg + <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h3 id="train-方法"><a href="#train-方法" class="headerlink" title="train 方法"></a><code>train</code> 方法</h3><ul>
<li><p><strong>目的</strong>：训练模型的整个过程。</p>
</li>
<li><p>参数</p>
<p>：</p>
<ul>
<li><code>epochs</code>：训练周期的数量。</li>
<li><code>log_freq</code>：记录训练信息的频率。</li>
<li><code>eval_freq</code>：进行验证和检查早期停止的频率。</li>
<li><code>patience</code>：早期停止的耐心参数，即在验证得分没有改进时允许的连续周期数。</li>
<li><code>save_checkpoint</code>：是否在找到更好的验证得分时保存检查点。</li>
<li><code>resume_path</code>：从先前保存的检查点恢复训练的路径。</li>
</ul>
</li>
</ul>
<h4 id="主要流程"><a href="#主要流程" class="headerlink" title="主要流程"></a>主要流程</h4><ol>
<li><p><strong>模型循环</strong>：遍历所有模型和对应的优化器。</p>
</li>
<li><p><strong>加载或恢复检查点</strong>：如果提供了 <code>resume_path</code>，则从检查点加载或恢复模型。</p>
</li>
<li><p><strong>准备训练和验证数据加载器</strong>：从数据集中获取训练和验证的数据加载器。</p>
</li>
<li><p><strong>早期停止</strong>：初始化一个早期停止对象来监视验证得分。</p>
</li>
<li><p>训练循环</p>
<p>：</p>
<ul>
<li>将模型设置为训练模式。</li>
<li>对于每个周期：<ul>
<li>初始化时间和损失计数器。</li>
<li>遍历训练加载器中的批次：<ul>
<li>将批次数据移动到适当的设备上。</li>
<li>执行前向传递和损失计算。</li>
<li>执行反向传递和优化器步骤。</li>
<li>累积批次损失。</li>
</ul>
</li>
<li>如果到达验证频率，则在验证集上评估模型，并使用早期停止对象更新得分。</li>
<li>如果到达日志频率，则在训练和验证集上评估模型，并记录结果。</li>
<li>检查早期停止条件，如果满足，则中断训练。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>异常处理</strong>：捕获键盘中断，允许用户手动停止训练。</p>
</li>
<li><p><strong>加载最佳模型</strong>：如果找到更好的模型，则加载最佳状态字典。</p>
</li>
<li><p><strong>测试</strong>：在测试集上测试模型并记录结果。</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">self, test_model=<span class="literal">None</span>, test_loader=<span class="literal">None</span>, test_df=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               checkpoint_dir=<span class="literal">None</span>, save_prediction=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">               calc_metric=<span class="literal">True</span>, calc_loss=<span class="literal">True</span>, model_label=<span class="literal">None</span>, mode=<span class="string">&#x27;test&#x27;</span></span>):</span><br><span class="line">       <span class="keyword">if</span> checkpoint_dir <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">           self.load_pretrained_model(checkpoint_dir)</span><br><span class="line">       <span class="keyword">if</span> test_loader <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> test_df <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">           test_loader = self.test_loader</span><br><span class="line">           test_df = self.test_df</span><br><span class="line">       test_models = self.models <span class="keyword">if</span> test_model <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> [test_model]</span><br><span class="line">       esb_ypred, esb_yprob = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">       esb_loss = <span class="number">0</span></span><br><span class="line">       <span class="keyword">for</span> model <span class="keyword">in</span> test_models:</span><br><span class="line">           model.<span class="built_in">eval</span>()</span><br><span class="line">           y_true, y_pred, y_prob = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">           tot_loss = <span class="number">0</span></span><br><span class="line">           <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">               <span class="keyword">for</span> step, batch <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(test_loader, <span class="number">1</span>),</span><br><span class="line">                       desc=mode, leave=<span class="literal">False</span>, total=<span class="built_in">len</span>(test_loader)):</span><br><span class="line">                   X = batch[<span class="string">&#x27;seq_enc&#x27;</span>].to(self.device)</span><br><span class="line">                   <span class="keyword">if</span> self.use_loc_feat:</span><br><span class="line">                       loc_feat = batch[<span class="string">&#x27;loc_feat&#x27;</span>].to(self.device)</span><br><span class="line">                   <span class="keyword">else</span>:</span><br><span class="line">                       loc_feat = <span class="literal">None</span></span><br><span class="line">                   <span class="keyword">if</span> self.use_glob_feat:</span><br><span class="line">                       glob_feat = batch[<span class="string">&#x27;glob_feat&#x27;</span>].to(self.device)</span><br><span class="line">                   <span class="keyword">else</span>:</span><br><span class="line">                       glob_feat = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">                   output = model(X, glob_feat=glob_feat, loc_feat=loc_feat)</span><br><span class="line">                   output = output.view(-<span class="number">1</span>)</span><br><span class="line">                   <span class="keyword">if</span> calc_loss:</span><br><span class="line">                       y = batch[<span class="string">&#x27;label&#x27;</span>].to(self.device)</span><br><span class="line">                       loss = self.criterion(output, y)</span><br><span class="line">                       tot_loss += loss.item()</span><br><span class="line">                   y_pred = output <span class="keyword">if</span> y_pred <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> torch.cat((y_pred, output), dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">           y_pred = y_pred.detach().cpu() <span class="keyword">if</span> self.device == torch.device(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">else</span> y_pred.detach()</span><br><span class="line">           esb_ypred = y_pred.view(-<span class="number">1</span>, <span class="number">1</span>) <span class="keyword">if</span> esb_ypred <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> torch.cat((esb_ypred, y_pred.view(-<span class="number">1</span>, <span class="number">1</span>)), dim=<span class="number">1</span>)</span><br><span class="line">           esb_loss += tot_loss / step</span><br><span class="line"></span><br><span class="line">       esb_ypred = esb_ypred.mean(axis=<span class="number">1</span>).numpy()</span><br><span class="line">       esb_loss /= <span class="built_in">len</span>(test_models)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> calc_metric:</span><br><span class="line">           y_fitness = test_df[<span class="string">&#x27;score&#x27;</span>].values</span><br><span class="line">           eval_results = scipy.stats.spearmanr(y_fitness, esb_ypred)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">       test_results = &#123;&#125;</span><br><span class="line">       results_df = test_df.copy()</span><br><span class="line">       results_df = results_df.drop(columns=[<span class="string">&#x27;sequence&#x27;</span>])</span><br><span class="line">       results_df[<span class="string">&#x27;prediction&#x27;</span>] = esb_ypred</span><br><span class="line">       test_results[<span class="string">&#x27;df&#x27;</span>] = results_df</span><br><span class="line">       <span class="keyword">if</span> save_prediction:</span><br><span class="line">           self.saver.save_df(results_df, <span class="string">&#x27;prediction.tsv&#x27;</span>)</span><br><span class="line">       test_results[<span class="string">&#x27;loss&#x27;</span>] = esb_loss</span><br><span class="line">       <span class="keyword">if</span> calc_metric:</span><br><span class="line">           test_results[<span class="string">&#x27;metric&#x27;</span>] = &#123;<span class="string">&#x27;corr&#x27;</span>: eval_results&#125;</span><br><span class="line">       <span class="keyword">return</span> test_results</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="test-方法"><a href="#test-方法" class="headerlink" title="test 方法"></a><code>test</code> 方法</h3><ul>
<li><p><strong>目的</strong>：对给定的测试模型在测试加载器上的性能进行评估。</p>
</li>
<li><p>参数</p>
<p>：</p>
<ul>
<li><code>test_model</code>：要测试的模型（可选）。</li>
<li><code>test_loader</code>：包含测试数据的数据加载器（可选）。</li>
<li><code>test_df</code>：包含测试数据的 DataFrame（可选）。</li>
<li><code>checkpoint_dir</code>：从检查点加载预训练模型的目录（可选）。</li>
<li><code>save_prediction</code>：是否保存预测结果到文件（可选）。</li>
<li><code>calc_metric</code>：是否计算评估指标（例如相关性）（可选）。</li>
<li><code>calc_loss</code>：是否计算损失（可选）。</li>
<li><code>model_label</code>：模型标签（可选，未在代码中使用）。</li>
<li><code>mode</code>：描述测试模式的字符串（例如 ‘test’ 或 ‘val’）。</li>
</ul>
</li>
</ul>
<h4 id="主要流程-1"><a href="#主要流程-1" class="headerlink" title="主要流程"></a>主要流程</h4><ol>
<li><p><strong>加载预训练模型</strong>：如果提供了 <code>checkpoint_dir</code>，则从检查点加载预训练模型。</p>
</li>
<li><p><strong>设置测试加载器和 DataFrame</strong>：如果没有提供，使用默认的测试加载器和测试 DataFrame。</p>
</li>
<li><p><strong>选择测试模型</strong>：如果没有提供 <code>test_model</code>，则使用所有模型进行测试。</p>
</li>
<li><p>模型循环</p>
<p>：遍历要测试的模型。</p>
<ul>
<li>将模型设置为评估模式。</li>
<li>初始化真实值和预测值的变量。</li>
<li>遍历测试加载器中的批次：<ul>
<li>将批次数据移动到适当的设备上。</li>
<li>执行前向传递。</li>
<li>如果 <code>calc_loss</code> 为 True，则计算损失。</li>
<li>收集预测值。</li>
</ul>
</li>
<li>累积预测值和损失。</li>
</ul>
</li>
<li><p><strong>计算集成预测和损失</strong>：通过平均所有模型的预测和损失来计算集成结果。</p>
</li>
<li><p><strong>计算评估指标</strong>：如果 <code>calc_metric</code> 为 True，则计算相关性等评估指标。</p>
</li>
<li><p><strong>保存预测</strong>：如果 <code>save_prediction</code> 为 True，则将预测结果保存到文件中。</p>
</li>
<li><p><strong>返回结果</strong>：返回包含预测、损失和评估指标的结果字典。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>ECnet代码解读（三）</title>
    <url>/2023/08/10/ECnet%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataFrameDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, seq_df</span>):</span><br><span class="line">        self._cache = []</span><br><span class="line">        <span class="keyword">for</span> seq_id, seq <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            seq_df[<span class="string">&#x27;ID&#x27;</span>].values,</span><br><span class="line">            seq_df[<span class="string">&#x27;sequence&#x27;</span>].values</span><br><span class="line">        ):</span><br><span class="line">            self._cache.append(&#123;</span><br><span class="line">                <span class="string">&#x27;id&#x27;</span>: seq_id,</span><br><span class="line">                <span class="string">&#x27;primary&#x27;</span>: seq,</span><br><span class="line">                <span class="string">&#x27;protein_length&#x27;</span>: <span class="built_in">len</span>(seq)</span><br><span class="line">            &#125;)</span><br><span class="line">        self._num_examples = <span class="built_in">len</span>(self._cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self._num_examples</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0</span> &lt;= index &lt; self._num_examples:</span><br><span class="line">            <span class="keyword">raise</span> IndexError(index)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self._cache[index]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_pad_sequences</span>(<span class="params">sequences: <span class="type">Sequence</span>[np.ndarray], constant_value=<span class="number">0</span></span>) -&gt; np.ndarray:</span><br><span class="line">    batch_size = <span class="built_in">len</span>(sequences)</span><br><span class="line">    shape = [batch_size] + np.<span class="built_in">max</span>([seq.shape <span class="keyword">for</span> seq <span class="keyword">in</span> sequences], <span class="number">0</span>).tolist()</span><br><span class="line">    array = np.zeros(shape, sequences[<span class="number">0</span>].dtype) + constant_value</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> arr, seq <span class="keyword">in</span> <span class="built_in">zip</span>(array, sequences):</span><br><span class="line">        arrslice = <span class="built_in">tuple</span>(<span class="built_in">slice</span>(dim) <span class="keyword">for</span> dim <span class="keyword">in</span> seq.shape)</span><br><span class="line">        arr[arrslice] = seq</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> array</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EmbedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 data,</span></span><br><span class="line"><span class="params">                 tokenizer: <span class="type">Union</span>[<span class="built_in">str</span>, TAPETokenizer] = <span class="string">&#x27;iupac&#x27;</span>,</span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(tokenizer, <span class="built_in">str</span>):</span><br><span class="line">            tokenizer = TAPETokenizer(vocab=tokenizer)</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, pd.DataFrame):</span><br><span class="line">            self.data = DataFrameDataset(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index: <span class="built_in">int</span></span>):</span><br><span class="line">        item = self.data[index]</span><br><span class="line">        token_ids = self.tokenizer.encode(item[<span class="string">&#x27;primary&#x27;</span>])</span><br><span class="line">        input_mask = np.ones_like(token_ids)</span><br><span class="line">        <span class="keyword">return</span> item[<span class="string">&#x27;id&#x27;</span>], token_ids, input_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">self, batch: <span class="type">List</span>[<span class="type">Tuple</span>[<span class="type">Any</span>, ...]]</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        ids, tokens, input_mask = <span class="built_in">zip</span>(*batch)</span><br><span class="line">        ids = <span class="built_in">list</span>(ids)</span><br><span class="line">        tokens = torch.from_numpy(_pad_sequences(tokens))</span><br><span class="line">        input_mask = torch.from_numpy(_pad_sequences(input_mask))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;ids&#x27;</span>: ids, <span class="string">&#x27;input_ids&#x27;</span>: tokens, <span class="string">&#x27;input_mask&#x27;</span>: input_mask&#125;  <span class="comment"># type: ignore</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="训练数据预处理"><a href="#训练数据预处理" class="headerlink" title="训练数据预处理"></a>训练数据预处理</h2><ol>
<li><p><strong><code>DataFrameDataset</code> 类</strong>：</p>
<ul>
<li><p><strong>目的</strong>：将数据框（通常包含蛋白质ID和相应的蛋白质序列）转换为易于访问和使用的数据集格式。</p>
</li>
<li><p>方法</p>
<p>：</p>
<ul>
<li><code>__init__</code>: 初始化数据集并将数据存储在缓存中。</li>
<li><code>__len__</code>: 返回数据集中的项数。</li>
<li><code>__getitem__</code>: 根据索引返回数据集中的一个项。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong><code>_pad_sequences</code> 函数</strong>：</p>
<ul>
<li><p><strong>目的</strong>：给定一系列不等长的序列，使用常数值填充它们，使得所有序列具有相同的长度。</p>
</li>
<li><p>参数</p>
<p>：</p>
<ul>
<li><code>sequences</code>: 要填充的序列列表。</li>
<li><code>constant_value</code>: 用于填充的常数值。</li>
</ul>
</li>
<li><p><strong>返回</strong>：一个填充后的 NumPy 数组，其中每个序列都有相同的长度。</p>
</li>
</ul>
</li>
<li><p><strong><code>EmbedDataset</code> 类</strong>：</p>
<ul>
<li><p><strong>目的</strong>：对蛋白质序列进行嵌入编码，并为训练和评估模型准备适当的输入格式。</p>
</li>
<li><p>方法</p>
<p>：</p>
<ul>
<li><code>__init__</code>: 初始化数据集并设置分词器（用于将蛋白质序列转换为嵌入向量）。</li>
<li><code>__len__</code>: 返回数据集中的项数。</li>
<li><code>__getitem__</code>: 根据索引返回数据集中的一个项。它将蛋白质序列转换为嵌入向量。</li>
<li><code>collate_fn</code>: 一个用于数据加载器的收集函数，它将一批数据组合成一个单一的输入张量。这是为了处理不同长度的蛋白质序列。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="TAPE编码"><a href="#TAPE编码" class="headerlink" title="TAPE编码"></a>TAPE编码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TAPEEncoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">        batch_size: <span class="built_in">int</span> = <span class="number">128</span>,</span></span><br><span class="line"><span class="params">        model_config_file: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        full_sequence_embed: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        no_cuda: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        seed: <span class="built_in">int</span> = <span class="number">42</span>,</span></span><br><span class="line"><span class="params">        tokenizer: <span class="built_in">str</span> = <span class="string">&#x27;iupac&#x27;</span>,</span></span><br><span class="line"><span class="params">        num_workers: <span class="built_in">int</span> = <span class="number">4</span>,</span></span><br><span class="line"><span class="params">        log_level: <span class="type">Union</span>[<span class="built_in">str</span>, <span class="built_in">int</span>] = logging.INFO,</span></span><br><span class="line"><span class="params">        progress_bar: <span class="built_in">bool</span> = <span class="literal">True</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        seq_df: pd.DataFrame object. Two columns are requried `ID` and `sequence`.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        local_rank = -<span class="number">1</span>  <span class="comment"># TAPE does not support torch.distributed.launch for embedding</span></span><br><span class="line">        device, n_gpu, is_master = utils.setup_distributed(local_rank, no_cuda)</span><br><span class="line">        utils.setup_logging(local_rank, save_path=<span class="literal">None</span>, log_level=log_level)</span><br><span class="line">        utils.set_random_seeds(seed, n_gpu)</span><br><span class="line"></span><br><span class="line">        model = ProteinBertModel.from_pretrained(<span class="string">&#x27;bert-base&#x27;</span>)</span><br><span class="line">        model = model.to(device)</span><br><span class="line">        runner = ForwardRunner(model, device, n_gpu)</span><br><span class="line">        runner.initialize_distributed_model()</span><br><span class="line">        runner.<span class="built_in">eval</span>()</span><br><span class="line">        self.local_rank = local_rank</span><br><span class="line">        self.n_gpu = n_gpu</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.num_workers = num_workers</span><br><span class="line">        self.runner = runner</span><br><span class="line">        self.full_sequence_embed = full_sequence_embed</span><br><span class="line">        self.progress_bar = progress_bar</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, sequences: [<span class="built_in">str</span>]</span>) -&gt; np.ndarray:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        sequences: list of equal-length sequences</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        np array with shape (#sequences, length of sequences, embedding dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        seq_df = pd.DataFrame(&#123;<span class="string">&#x27;ID&#x27;</span>: sequences, <span class="string">&#x27;sequence&#x27;</span>: sequences&#125;)</span><br><span class="line">        embed_dict = self.tape_embed(seq_df)</span><br><span class="line">        encoding = np.array([embed_dict[s].numpy() <span class="keyword">for</span> s <span class="keyword">in</span> sequences])</span><br><span class="line">        <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tape_embed</span>(<span class="params">self, seq_df: pd.DataFrame</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        seq_df: pd.DataFrame with at least two columns `ID` and `sequence`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        dict with ID as keys and embeddings as values. Embeddings are </span></span><br><span class="line"><span class="string">        pytorch tensor with shape (#sequences, length of sequences, embedding dim)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        local_rank = self.local_rank</span><br><span class="line">        n_gpu = self.n_gpu</span><br><span class="line">        batch_size = self.batch_size</span><br><span class="line">        num_workers = self.num_workers</span><br><span class="line">        runner = self.runner</span><br><span class="line">        full_sequence_embed = self.full_sequence_embed</span><br><span class="line">        progress_bar = self.progress_bar</span><br><span class="line"></span><br><span class="line">        dataset = EmbedDataset(seq_df)</span><br><span class="line">        valid_loader = utils.setup_loader(dataset, batch_size, local_rank, n_gpu, <span class="number">1</span>, num_workers)</span><br><span class="line"></span><br><span class="line">        embed_dict = &#123;&#125;</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">with</span> utils.wrap_cuda_oom_error(local_rank, batch_size, n_gpu):</span><br><span class="line">                <span class="keyword">for</span> batch <span class="keyword">in</span> (tqdm(valid_loader, total=<span class="built_in">len</span>(valid_loader),</span><br><span class="line">                        desc=<span class="string">&#x27;encode&#x27;</span>, leave=<span class="literal">False</span>) <span class="keyword">if</span> progress_bar <span class="keyword">else</span> valid_loader):</span><br><span class="line">                    outputs = runner.forward(batch, no_loss=<span class="literal">True</span>)</span><br><span class="line">                    ids = batch[<span class="string">&#x27;ids&#x27;</span>]</span><br><span class="line">                    sequence_embed = outputs[<span class="number">0</span>]</span><br><span class="line">                    pooled_embed = outputs[<span class="number">1</span>]</span><br><span class="line">                    sequence_lengths = batch[<span class="string">&#x27;input_mask&#x27;</span>].<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">                    sequence_embed = sequence_embed.cpu()</span><br><span class="line">                    sequence_lengths = sequence_lengths.cpu()</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">for</span> seqembed, length, protein_id <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">                            sequence_embed, sequence_lengths, ids):</span><br><span class="line">                        seqembed = seqembed[:length]</span><br><span class="line">                        seqembed = seqembed[<span class="number">1</span>:-<span class="number">1</span>] <span class="comment"># remove &lt;cls&gt; &lt;sep&gt;</span></span><br><span class="line">                        <span class="keyword">if</span> <span class="keyword">not</span> full_sequence_embed:</span><br><span class="line">                            seqembed = seqembed.mean(<span class="number">0</span>)</span><br><span class="line">                        embed_dict[protein_id] = seqembed</span><br><span class="line">        <span class="keyword">return</span> embed_dict</span><br></pre></td></tr></table></figure>

<h3 id="1-初始化方法-init"><a href="#1-初始化方法-init" class="headerlink" title="1. 初始化方法 (__init__):"></a>1. <strong>初始化方法 (<code>__init__</code>):</strong></h3><ul>
<li><p>参数</p>
<p>：</p>
<ul>
<li><code>batch_size</code>: 批次大小。</li>
<li><code>model_config_file</code>: 模型配置文件（如果有）。</li>
<li><code>full_sequence_embed</code>: 是否返回整个序列的嵌入。</li>
<li><code>no_cuda</code>: 是否禁用 CUDA。</li>
<li><code>seed</code>: 随机种子。</li>
<li><code>tokenizer</code>: 分词器类型。</li>
<li><code>num_workers</code>: 用于数据加载的工作进程数。</li>
<li><code>log_level</code>: 日志级别。</li>
<li><code>progress_bar</code>: 是否显示进度条。</li>
</ul>
</li>
<li><p>功能</p>
<p>：</p>
<ul>
<li>设置分布式计算环境和随机种子。</li>
<li>从预训练的权重中加载 ProteinBertModel。</li>
<li>初始化 <code>ForwardRunner</code>，用于模型推理。</li>
<li>存储相关参数和对象。</li>
</ul>
</li>
</ul>
<h3 id="2-编码方法-encode"><a href="#2-编码方法-encode" class="headerlink" title="2. 编码方法 (encode):"></a>2. <strong>编码方法 (<code>encode</code>):</strong></h3><ul>
<li><strong>参数</strong>：<code>sequences</code>，要编码的等长序列列表。</li>
<li><strong>返回</strong>：嵌入编码的 NumPy 数组，形状为 (#sequences, length of sequences, embedding dim)。</li>
<li><strong>功能</strong>：将给定的蛋白质序列转换为嵌入向量。</li>
</ul>
<h3 id="3-TAPE-嵌入方法-tape-embed"><a href="#3-TAPE-嵌入方法-tape-embed" class="headerlink" title="3. TAPE 嵌入方法 (tape_embed):"></a>3. <strong>TAPE 嵌入方法 (<code>tape_embed</code>):</strong></h3><ul>
<li><p><strong>参数</strong>：<code>seq_df</code>，一个包含至少两列（<code>ID</code> 和 <code>sequence</code>）的 pandas 数据框。</p>
</li>
<li><p><strong>返回</strong>：一个字典，其中键是 ID，值是嵌入向量。嵌入向量是具有形状 (#sequences, length of sequences, embedding dim) 的 PyTorch 张量。</p>
</li>
<li><p>功能</p>
<p>：</p>
<ul>
<li>创建一个嵌入数据集并设置数据加载器。</li>
<li>通过 TAPE 的 ProteinBertModel 运行前向传播来生成嵌入。</li>
<li>根据需要，可以选择返回整个序列的嵌入或返回序列的平均嵌入。</li>
</ul>
</li>
</ul>
<h2 id="CCMPred编码局部变量"><a href="#CCMPred编码局部变量" class="headerlink" title="CCMPred编码局部变量"></a>CCMPred编码局部变量</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@numba.njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">all_sequence_pairwise_profile</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    e: 4-d (L, L, 21, 21) np.array where e(i, j, a, b) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion and `b` at the j-th position.</span></span><br><span class="line"><span class="string">           Amino acids are encoded by 0-20 using the same index in CCMPred.</span></span><br><span class="line"><span class="string">    index_encoded: (N, L) array of index-encoded sequence using CCMPred index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    encoding: (N, L, L) array of sequence encoding. Each amino acid in a sequence</span></span><br><span class="line"><span class="string">    is encoded by values in the pairwise epsilon value table of CCMPred.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    e, index_encoded = args</span><br><span class="line">    N, L = index_encoded.shape</span><br><span class="line">    encoding = np.zeros((N, L, L))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L - <span class="number">1</span>):</span><br><span class="line">            a = index_encoded[k, i]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, L):</span><br><span class="line">                b = index_encoded[k, j]</span><br><span class="line">                encoding[k, i, j] = e[i, j, a, b]</span><br><span class="line">        encoding[k] += encoding[k].T</span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"><span class="meta">@numba.njit</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">all_sequence_singleton_profile</span>(<span class="params">args</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    ei: 2-d (L, 20) np.array where e(i, a) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion.</span></span><br><span class="line"><span class="string">           Amino acids are encoded by 0-20 using the same index in CCMPred.</span></span><br><span class="line"><span class="string">    index_encoded: (N, L) array of index-encoded sequence using CCMPred index</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    encoding: (N, L, 1) array of sequence encoding. Each amino acid in a sequence</span></span><br><span class="line"><span class="string">    is encoded by values in the singleton epsilon value table of CCMPred.    </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    e, index_encoded = args</span><br><span class="line">    N, L = index_encoded.shape</span><br><span class="line">    encoding = np.zeros((N, L, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L):</span><br><span class="line">            a = index_encoded[k, i]</span><br><span class="line">            encoding[k, i] = e[i, a]</span><br><span class="line">    <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CCMPredEncoder</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ccmpred_output=<span class="literal">None</span>, seq_len=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        brawfile: path to msgpack file storing the (L, L, 21, 21) table of</span></span><br><span class="line"><span class="string">                  CCMPred epsilon values of a target sequence</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.seq_len = seq_len</span><br><span class="line">        self.vocab_index = vocab.CCMPRED_AMINO_ACID_INDEX</span><br><span class="line">        brawfile = pathlib.Path(ccmpred_output)</span><br><span class="line">        self.eij, self.ei = self.load_data(brawfile)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_data</span>(<span class="params">self, brawfile</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        eij: 4-d (L, L, 21, 21) np.array where e(i, j, a, b) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion and `b` at the j-th position.</span></span><br><span class="line"><span class="string">        ei: 2-d (L, 20) np.array where e(i, a) is the epsilon values in CCMPred for</span></span><br><span class="line"><span class="string">           `a` at the i-th postion</span></span><br><span class="line"><span class="string">           Amino acids are encoded by 0-20 using the same index in CCMPred.</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> brawfile.exists():</span><br><span class="line">            <span class="keyword">raise</span> FileNotFoundError(brawfile)</span><br><span class="line">        data = msgpack.unpack(<span class="built_in">open</span>(brawfile, <span class="string">&#x27;rb&#x27;</span>))</span><br><span class="line">        L = self.seq_len</span><br><span class="line">        V = <span class="built_in">len</span>(self.vocab_index)</span><br><span class="line">        eij = np.zeros((L, L, V, V))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(L - <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, L):</span><br><span class="line">                arr = np.array(data[<span class="string">b&#x27;x_pair&#x27;</span>][<span class="string">b&#x27;%d/%d&#x27;</span>%(i, j)][<span class="string">b&#x27;x&#x27;</span>]).reshape(V, V)</span><br><span class="line">                eij[i, j] = arr</span><br><span class="line">                eij[j, i] = arr.T</span><br><span class="line"></span><br><span class="line">        ei = np.array(data[<span class="string">b&#x27;x_single&#x27;</span>]).reshape(L, V - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> eij, ei</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">index_encoding</span>(<span class="params">self, sequences, letter_to_index_dict</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Modified from https://github.com/openvax/mhcflurry/blob/master/mhcflurry/amino_acid.py#L110-L130</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        sequences: list of equal-length sequences</span></span><br><span class="line"><span class="string">        letter_to_index_dict: char -&gt; int</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        np.array with shape (#sequences, length of sequences)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        df = pd.DataFrame(<span class="built_in">iter</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> sequences)</span><br><span class="line">        encoding = df.replace(letter_to_index_dict)</span><br><span class="line">        encoding = encoding.values.astype(np.<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ccmpred_encoding</span>(<span class="params">self, index_encoded, profile=<span class="string">&#x27;pair&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">if</span> profile == <span class="string">&#x27;pair&#x27;</span>:</span><br><span class="line">            encoding = all_sequence_pairwise_profile((self.eij, index_encoded))</span><br><span class="line">        <span class="keyword">elif</span> profile == <span class="string">&#x27;single&#x27;</span>:</span><br><span class="line">            encoding = all_sequence_singleton_profile((self.ei, index_encoded))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        <span class="keyword">return</span> encoding</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, sequences, mode=<span class="string">&#x27;train&#x27;</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        encoding: (N, L, L + 1) array of sequence encoding. Each amino acid in a sequence</span></span><br><span class="line"><span class="string">        is encoded by values in the singble and pairwise epsilon value tables of CCMPred</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        index_encoded = self.index_encoding(sequences, self.vocab_index)        </span><br><span class="line">        single = self.ccmpred_encoding(index_encoded, profile=<span class="string">&#x27;single&#x27;</span>)</span><br><span class="line">        pair = self.ccmpred_encoding(index_encoded, profile=<span class="string">&#x27;pair&#x27;</span>)</span><br><span class="line">        self.ccmpred_encoded = np.concatenate([single, pair], axis=<span class="number">2</span>)        </span><br><span class="line">        <span class="keyword">return</span> self.ccmpred_encoded</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="1-函数-all-sequence-pairwise-profile："><a href="#1-函数-all-sequence-pairwise-profile：" class="headerlink" title="1. 函数 all_sequence_pairwise_profile："></a>1. <strong>函数 <code>all_sequence_pairwise_profile</code>：</strong></h3><ul>
<li><strong>参数</strong>：一个包含 CCMPred 的 �<em>ε</em> 值和索引编码序列的元组。</li>
<li><strong>返回</strong>：一个编码数组，形状为 (�,�,�)(<em>N</em>,<em>L</em>,<em>L</em>)。</li>
<li><strong>功能</strong>：计算所有序列的两两配对特征。</li>
</ul>
<h3 id="2-函数-all-sequence-singleton-profile："><a href="#2-函数-all-sequence-singleton-profile：" class="headerlink" title="2. 函数 all_sequence_singleton_profile："></a>2. <strong>函数 <code>all_sequence_singleton_profile</code>：</strong></h3><ul>
<li><strong>参数</strong>：一个包含 CCMPred 的 �<em>ε</em> 值和索引编码序列的元组。</li>
<li><strong>返回</strong>：一个编码数组，形状为 (�,�,1)(<em>N</em>,<em>L</em>,1)。</li>
<li><strong>功能</strong>：计算所有序列的单体特征。</li>
</ul>
<h3 id="3-类-CCMPredEncoder："><a href="#3-类-CCMPredEncoder：" class="headerlink" title="3. 类 CCMPredEncoder："></a>3. <strong>类 <code>CCMPredEncoder</code>：</strong></h3><ul>
<li>方法 <code>__init__</code>：<ul>
<li>初始化类，加载 CCMPred 输出文件并设置词汇表索引。</li>
</ul>
</li>
<li>方法 <code>load_data</code>：<ul>
<li>从给定的文件中加载 CCMPred 数据并返回两个数组，分别表示两两配对和单体特征。</li>
</ul>
</li>
<li>方法 <code>index_encoding</code>：<ul>
<li>将给定的序列列表转换为整数索引编码。</li>
</ul>
</li>
<li>方法 <code>ccmpred_encoding</code>：<ul>
<li>根据 CCMPred 的两两配对或单体特征计算序列的编码。</li>
</ul>
</li>
<li>方法 <code>encode</code>：<ul>
<li>返回序列的 CCMPred 编码，将单体和两两配对特征组合在一起。</li>
</ul>
</li>
</ul>
<p>该代码部分的核心目标是使用 CCMPred 的共进化信息为蛋白质序列生成特征编码。共进化信息可以捕获蛋白质残基之间的相互作用和依赖性，对于许多下游分析和预测任务（例如，三维结构预测）可能是有用的。通过使用 Numba（一个用于编译 Python 的高性能库）进行即时编译，相关的数值计算得到了优化。</p>
<h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Modified from Annotated Transformer</span></span><br><span class="line"><span class="string">    http://nlp.seas.harvard.edu/2018/04/03/attention.html</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">1024</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEmbedding, self).__init__()</span><br><span class="line">        pe = torch.zeros((max_len, d_model), requires_grad=<span class="literal">False</span>).<span class="built_in">float</span>()</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).<span class="built_in">float</span>().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InputPositionEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size=<span class="literal">None</span>, embed_dim=<span class="literal">None</span>, dropout=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                init_weight=<span class="literal">None</span>, seq_len=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InputPositionEmbedding, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.position_embed = PositionalEmbedding(embed_dim, max_len=seq_len)</span><br><span class="line">        self.reproject = nn.Identity()</span><br><span class="line">        <span class="keyword">if</span> init_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.embed = nn.Embedding.from_pretrained(init_weight)</span><br><span class="line">            self.reproject = nn.Linear(init_weight.size(<span class="number">1</span>), embed_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        x = self.embed(inputs)</span><br><span class="line">        x = x + self.position_embed(inputs)</span><br><span class="line">        x = self.reproject(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AggregateLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="literal">None</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(AggregateLayer, self).__init__()        </span><br><span class="line">        self.attn = nn.Sequential(collections.OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;layernorm&#x27;</span>, nn.LayerNorm(d_model)),</span><br><span class="line">            (<span class="string">&#x27;fc&#x27;</span>, nn.Linear(d_model, <span class="number">1</span>, bias=<span class="literal">False</span>)),</span><br><span class="line">            (<span class="string">&#x27;dropout&#x27;</span>, nn.Dropout(dropout)),</span><br><span class="line">            (<span class="string">&#x27;softmax&#x27;</span>, nn.Softmax(dim=<span class="number">1</span>))</span><br><span class="line">        ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, context</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        context: token embedding from encoder (Transformer/LSTM)</span></span><br><span class="line"><span class="string">                (batch_size, seq_len, embed_dim)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">        weight = self.attn(context)</span><br><span class="line">        <span class="comment"># (batch_size, seq_len, embed_dim).T * (batch_size, seq_len, 1) *  -&gt;</span></span><br><span class="line">        <span class="comment"># (batch_size, embed_dim, 1)</span></span><br><span class="line">        output = torch.bmm(context.transpose(<span class="number">1</span>, <span class="number">2</span>), weight)</span><br><span class="line">        output = output.squeeze(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GlobalPredictor</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="literal">None</span>, d_h=<span class="literal">None</span>, d_out=<span class="literal">None</span>, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GlobalPredictor, self).__init__()</span><br><span class="line">        self.predict_layer = nn.Sequential(collections.OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;batchnorm&#x27;</span>, nn.BatchNorm1d(d_model)),</span><br><span class="line">            (<span class="string">&#x27;fc1&#x27;</span>, nn.Linear(d_model, d_h)),</span><br><span class="line">            (<span class="string">&#x27;tanh&#x27;</span>, nn.Tanh()),</span><br><span class="line">            (<span class="string">&#x27;dropout&#x27;</span>, nn.Dropout(dropout)),</span><br><span class="line">            (<span class="string">&#x27;fc2&#x27;</span>, nn.Linear(d_h, d_out))</span><br><span class="line">        ]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.predict_layer(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SequenceLSTM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Container module with an encoder, a recurrent module, and a decoder.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_input=<span class="literal">None</span>, d_embed=<span class="number">20</span>, d_model=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                vocab_size=<span class="literal">None</span>, seq_len=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                dropout=<span class="number">0.1</span>, lstm_dropout=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                nlayers=<span class="number">1</span>, bidirectional=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                proj_loc_config=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SequenceLSTM, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embed = InputPositionEmbedding(vocab_size=vocab_size,</span><br><span class="line">                    seq_len=seq_len, embed_dim=d_embed)</span><br><span class="line"></span><br><span class="line">        self.lstm = nn.LSTM(input_size=d_input,</span><br><span class="line">                            hidden_size=d_model//<span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> d_model,</span><br><span class="line">                            num_layers=nlayers, dropout=lstm_dropout,</span><br><span class="line">                            bidirectional=bidirectional)</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.proj_loc_layer = proj_loc_config[<span class="string">&#x27;layer&#x27;</span>](</span><br><span class="line">                    proj_loc_config[<span class="string">&#x27;d_in&#x27;</span>], proj_loc_config[<span class="string">&#x27;d_out&#x27;</span>]</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, loc_feat=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.embed(x)</span><br><span class="line">        <span class="keyword">if</span> loc_feat <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loc_feat = self.proj_loc_layer(loc_feat)</span><br><span class="line">            x = torch.cat([x, loc_feat], dim=<span class="number">2</span>)</span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        x, _ = self.lstm(x)</span><br><span class="line">        x = x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        x = self.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMPredictor</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_embed=<span class="number">20</span>, d_model=<span class="number">128</span>, d_h=<span class="number">128</span>, d_out=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                vocab_size=<span class="literal">None</span>, seq_len=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                dropout=<span class="number">0.1</span>, lstm_dropout=<span class="number">0</span>, nlayers=<span class="number">1</span>, bidirectional=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                use_loc_feat=<span class="literal">True</span>, use_glob_feat=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                proj_loc_config=<span class="literal">None</span>, proj_glob_config=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMPredictor, self).__init__()</span><br><span class="line">        self.seq_lstm = SequenceLSTM(</span><br><span class="line">            d_input=d_embed + (proj_loc_config[<span class="string">&#x27;d_out&#x27;</span>] <span class="keyword">if</span> use_loc_feat <span class="keyword">else</span> <span class="number">0</span>),</span><br><span class="line">            d_embed=d_embed, d_model=d_model,</span><br><span class="line">            vocab_size=vocab_size, seq_len=seq_len,</span><br><span class="line">            dropout=dropout, lstm_dropout=lstm_dropout,</span><br><span class="line">            nlayers=nlayers, bidirectional=bidirectional,</span><br><span class="line">            proj_loc_config=proj_loc_config)</span><br><span class="line">        self.proj_glob_layer = proj_glob_config[<span class="string">&#x27;layer&#x27;</span>](</span><br><span class="line">            proj_glob_config[<span class="string">&#x27;d_in&#x27;</span>], proj_glob_config[<span class="string">&#x27;d_out&#x27;</span>]</span><br><span class="line">        )</span><br><span class="line">        self.aggragator = AggregateLayer(</span><br><span class="line">            d_model = d_model + (proj_glob_config[<span class="string">&#x27;d_out&#x27;</span>] <span class="keyword">if</span> use_glob_feat <span class="keyword">else</span> <span class="number">0</span>))</span><br><span class="line">        self.predictor = GlobalPredictor(</span><br><span class="line">            d_model = d_model + (proj_glob_config[<span class="string">&#x27;d_out&#x27;</span>] <span class="keyword">if</span> use_glob_feat <span class="keyword">else</span> <span class="number">0</span>),</span><br><span class="line">            d_h=d_h, d_out=d_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, glob_feat=<span class="literal">None</span>, loc_feat=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.seq_lstm(x, loc_feat=loc_feat)</span><br><span class="line">        <span class="keyword">if</span> glob_feat <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            glob_feat = self.proj_glob_layer(glob_feat)</span><br><span class="line">            x = torch.cat([x, glob_feat], dim=<span class="number">2</span>)</span><br><span class="line">        x = self.aggragator(x)</span><br><span class="line">        output = self.predictor(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="1-PositionalEmbedding-类"><a href="#1-PositionalEmbedding-类" class="headerlink" title="1. PositionalEmbedding 类"></a>1. <code>PositionalEmbedding</code> 类</h3><ul>
<li><p><strong>功能</strong>：创建了一个位置嵌入矩阵，用于向序列的每个位置添加独特的嵌入。</p>
</li>
<li><p>属性</p>
<p>：</p>
<ul>
<li><code>pe</code>：存储位置嵌入的张量。</li>
</ul>
</li>
<li><p><strong>原理</strong>：使用 sine 和 cosine 函数生成位置嵌入，使得每个位置的嵌入都是唯一的。</p>
</li>
</ul>
<h3 id="2-InputPositionEmbedding-类"><a href="#2-InputPositionEmbedding-类" class="headerlink" title="2. InputPositionEmbedding 类"></a>2. <code>InputPositionEmbedding</code> 类</h3><ul>
<li><p><strong>功能</strong>：将输入的词汇表索引转换为连续的嵌入表示，并添加位置嵌入。</p>
</li>
<li><p>属性和组件</p>
<p>：</p>
<ul>
<li><code>embed</code>：一个嵌入层，用于将输入的词汇表索引转换为连续的向量表示。</li>
<li><code>position_embed</code>：一个 <code>PositionalEmbedding</code> 层，用于添加位置信息。</li>
<li><code>reproject</code>：一个线性层，用于重新投影嵌入空间（如果需要）。</li>
<li><code>dropout</code>：Dropout 层，用于正则化。</li>
</ul>
</li>
</ul>
<h3 id="3-AggregateLayer-类"><a href="#3-AggregateLayer-类" class="headerlink" title="3. AggregateLayer 类"></a>3. <code>AggregateLayer</code> 类</h3><ul>
<li><p><strong>功能</strong>：聚合编码后的序列表示。</p>
</li>
<li><p>组件</p>
<p>：</p>
<ul>
<li><code>attn</code>：一个包括层归一化、全连接层、Dropout 层和 Softmax 层的序列，用于计算每个位置的权重，然后通过加权平均进行聚合。</li>
</ul>
</li>
</ul>
<h3 id="4-GlobalPredictor-类"><a href="#4-GlobalPredictor-类" class="headerlink" title="4. GlobalPredictor 类"></a>4. <code>GlobalPredictor</code> 类</h3><ul>
<li><p><strong>功能</strong>：从聚合后的表示生成最终的预测输出。</p>
</li>
<li><p>组件</p>
<p>：</p>
<ul>
<li><code>predict_layer</code>：一个包括批归一化、全连接层、激活函数和 Dropout 层的序列，用于最终的预测。</li>
</ul>
</li>
</ul>
<h3 id="5-SequenceLSTM-类"><a href="#5-SequenceLSTM-类" class="headerlink" title="5. SequenceLSTM 类"></a>5. <code>SequenceLSTM</code> 类</h3><ul>
<li><p><strong>功能</strong>：使用 LSTM 处理序列输入并生成连续的序列表示。</p>
</li>
<li><p>属性和组件</p>
<p>：</p>
<ul>
<li><code>embed</code>：一个 <code>InputPositionEmbedding</code> 层，用于输入和位置嵌入。</li>
<li><code>lstm</code>：LSTM 层，用于编码序列。</li>
<li><code>drop</code>：Dropout 层，用于正则化。</li>
<li><code>proj_loc_layer</code>：可选的全连接层，用于投影局部特征。</li>
</ul>
</li>
</ul>
<h3 id="6-LSTMPredictor-类"><a href="#6-LSTMPredictor-类" class="headerlink" title="6. LSTMPredictor 类"></a>6. <code>LSTMPredictor</code> 类</h3><ul>
<li><p><strong>功能</strong>：整个 LSTM 预测模型的容器。</p>
</li>
<li><p>属性和组件</p>
<p>：</p>
<ul>
<li><code>seq_lstm</code>：一个 <code>SequenceLSTM</code> 层，用于序列编码。</li>
<li><code>proj_glob_layer</code>：全连接层，用于投影全局特征。</li>
<li><code>aggregator</code>：一个 <code>AggregateLayer</code> 层，用于聚合编码后的序列表示。</li>
<li><code>predictor</code>：一个 <code>GlobalPredictor</code> 层，用于最终的预测。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>Python入门1</title>
    <url>/2023/08/18/Python%E5%85%A5%E9%97%A81/</url>
    <content><![CDATA[<h1 id="Python入门"><a href="#Python入门" class="headerlink" title="Python入门"></a>Python入门</h1><h2 id="为什么学python"><a href="#为什么学python" class="headerlink" title="为什么学python"></a>为什么学python</h2><p>这里引用一下鱼皮的话，以python为主作为开发语言确实不太现实，因为较慢的运行速度和性能无法较好的满足应用开发的需求。但是用python作为编程入门，了解编程的敲门砖，辅助理解编程思想，理解编程逻辑是个不错的选择。</p>
<p><strong>作为面向生信入门的Python学习Blog，我们从数据科学的角度来入门切入</strong></p>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>Leetcode每日一题1</title>
    <url>/2023/09/17/Leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%981/</url>
    <content><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a><strong>题目</strong></h1><p>给你一个下标从 <strong>0</strong> 开始的整数数组 <code>mapping</code> ，它表示一个十进制数的映射规则，<code>mapping[i] = j</code> 表示这个规则下将数位 <code>i</code> 映射为数位 <code>j</code> 。</p>
<p>一个整数 <strong>映射后的值</strong> 为将原数字每一个数位 <code>i</code> （<code>0 &lt;= i &lt;= 9</code>）映射为 <code>mapping[i]</code> 。</p>
<p>另外给你一个整数数组 <code>nums</code> ，请你将数组 <code>nums</code> 中每个数按照它们映射后对应数字非递减顺序排序后返回。</p>
<p><strong>注意：</strong></p>
<ul>
<li>如果两个数字映射后对应的数字大小相同，则将它们按照输入中的 <strong>相对顺序</strong> 排序。</li>
<li><code>nums</code> 中的元素只有在排序的时候需要按照映射后的值进行比较，返回的值应该是输入的元素本身。</li>
</ul>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：mapping = [8,9,4,0,2,1,3,5,7,6], nums = [991,338,38]</span><br><span class="line">输出：[338,38,991]</span><br><span class="line">解释：</span><br><span class="line">将数字 991 按照如下规则映射：</span><br><span class="line">1. mapping[9] = 6 ，所有数位 9 都会变成 6 。</span><br><span class="line">2. mapping[1] = 9 ，所有数位 1 都会变成 8 。</span><br><span class="line">所以，991 映射的值为 669 。</span><br><span class="line">338 映射为 007 ，去掉前导 0 后得到 7 。</span><br><span class="line">38 映射为 07 ，去掉前导 0 后得到 7 。</span><br><span class="line">由于 338 和 38 映射后的值相同，所以它们的前后顺序保留原数组中的相对位置关系，338 在 38 的前面。</span><br><span class="line">所以，排序后的数组为 [338,38,991] 。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：mapping = [0,1,2,3,4,5,6,7,8,9], nums = [789,456,123]</span><br><span class="line">输出：[123,456,789]</span><br><span class="line">解释：789 映射为 789 ，456 映射为 456 ，123 映射为 123 。所以排序后数组为 [123,456,789] 。</span><br></pre></td></tr></table></figure>

<h2 id="我的解法："><a href="#我的解法：" class="headerlink" title="我的解法："></a>我的解法：</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortJumbled</span>(<span class="params">self, mapping: <span class="type">List</span>[<span class="built_in">int</span>], nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 定义一个函数，用于获取每个数映射后的值</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">get_mapped_num</span>(<span class="params">num: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">            digits = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">str</span>(num)))  <span class="comment"># 将每一个数拆解成各个数位</span></span><br><span class="line">            mapped_digits = [mapping[digit] <span class="keyword">for</span> digit <span class="keyword">in</span> digits]  <span class="comment"># 应用映射规则</span></span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">int</span>(<span class="string">&#x27;&#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, mapped_digits)))  <span class="comment"># 将映射后的各数位组合成一个整数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用 sorted 函数和自定义的 key 函数进行排序</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sorted</span>(nums, key=get_mapped_num)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="取整数每个数位的方法："><a href="#取整数每个数位的方法：" class="headerlink" title="取整数每个数位的方法："></a>取整数每个数位的方法：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法1：使用 str 转换和 join 函数</span></span><br><span class="line">digits1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">combined_num1 = <span class="built_in">int</span>(<span class="string">&#x27;&#x27;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, digits1)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：使用数学运算</span></span><br><span class="line">digits2 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">combined_num2 = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> digit <span class="keyword">in</span> digits2:</span><br><span class="line">    combined_num2 = combined_num2 * <span class="number">10</span> + digit</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="函数解释"><a href="#函数解释" class="headerlink" title="函数解释"></a>函数解释</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>(<span class="built_in">str</span>, digits1)</span><br><span class="line">函数：<span class="built_in">map</span></span><br><span class="line">参数：一个函数 <span class="built_in">str</span> 和一个列表 digits1</span><br><span class="line">作用：将 <span class="built_in">str</span> 函数应用于 digits1 列表中的每一个元素。这会将所有的整数转换为字符串。</span><br><span class="line">返回值：一个 <span class="built_in">map</span> 对象，需要通过 <span class="built_in">list</span> 或其他方式转换为列表进行查看。</span><br><span class="line">例如，如果 digits1 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]，那么 <span class="built_in">map</span>(<span class="built_in">str</span>, digits1) 会返回一个 <span class="built_in">map</span> 对象，该对象相当于 [<span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>]。</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;</span>.join(iterable)</span><br><span class="line">方法：<span class="built_in">str</span>.join</span><br><span class="line">参数：一个可迭代对象 iterable，通常是一个字符串列表。</span><br><span class="line">作用：将 iterable 中的所有字符串连接成一个单一的字符串。字符串之间用调用该方法的字符串（这里是空字符串 <span class="string">&#x27;&#x27;</span>）进行连接。</span><br><span class="line">返回值：一个新的字符串。</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>(iterable, *, key=<span class="literal">None</span>, reverse=<span class="literal">False</span>)</span><br><span class="line">函数：<span class="built_in">sorted</span></span><br><span class="line">参数：</span><br><span class="line">iterable: 要排序的可迭代对象。</span><br><span class="line">key: （可选）一个接受单个参数的函数，用于从每个元素中提取一个用于排序的键。</span><br><span class="line">reverse: （可选）布尔值。如果设置为 <span class="literal">True</span>，则元素将以降序排列。</span><br><span class="line"></span><br><span class="line">key 参数</span><br><span class="line">key 参数接受一个函数（在这里是 get_mapped_num），该函数应当接受一个参数（从可迭代对象 iterable 中取出）并返回一个值，该值将用于排序。</span><br><span class="line"></span><br><span class="line">例如，假设 nums = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>]，并且 key 函数是 get_mapped_num。在排序过程中，<span class="built_in">sorted</span> 函数会用 get_mapped_num 函数来获取每个元素的“键”（或排序依据），然后根据这些“键”来排序。</span><br></pre></td></tr></table></figure>

<h2 id="Leetcode推荐解法"><a href="#Leetcode推荐解法" class="headerlink" title="Leetcode推荐解法"></a>Leetcode推荐解法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sortJumbled</span>(<span class="params">self, mapping: <span class="type">List</span>[<span class="built_in">int</span>], nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="type">List</span>[<span class="built_in">int</span>]:</span><br><span class="line">        <span class="comment"># 制作翻译表</span></span><br><span class="line">        tab = <span class="built_in">str</span>.maketrans(<span class="string">&quot;0123456789&quot;</span>, <span class="string">&quot;&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, mapping)))</span><br><span class="line">        <span class="comment"># 自定义排序: (num2map,idx)</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sorted</span>(nums, key=<span class="keyword">lambda</span> x: <span class="built_in">int</span>(<span class="built_in">str</span>(x).translate(tab)))</span><br><span class="line"></span><br><span class="line">作者：Bollie</span><br><span class="line">链接：https://leetcode.cn/problems/sort-the-jumbled-numbers/</span><br><span class="line">来源：力扣（LeetCode）</span><br></pre></td></tr></table></figure>

<h3 id="函数解释-1"><a href="#函数解释-1" class="headerlink" title="函数解释"></a>函数解释</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">str</span>.maketrans(x, y=<span class="literal">None</span>, z=<span class="literal">None</span>)</span><br><span class="line">方法：<span class="built_in">str</span>.maketrans</span><br><span class="line">参数：两个或三个字符串参数（这里只用了两个）。</span><br><span class="line">作用：返回一个用于字符串转换的映射表。</span><br><span class="line">返回值：一个转换表，该表可以用于 <span class="built_in">str</span>.translate 方法。</span><br><span class="line">在这里，<span class="built_in">str</span>.maketrans(<span class="string">&quot;0123456789&quot;</span>, <span class="string">&quot;&quot;</span>.join(<span class="built_in">map</span>(<span class="built_in">str</span>, mapping))) 创建了一个将数字字符 <span class="string">&#x27;0&#x27;</span> 到 <span class="string">&#x27;9&#x27;</span> 映射到 mapping 中相应元素的转换表。</span><br><span class="line"></span><br><span class="line"><span class="built_in">str</span>.translate(table)</span><br><span class="line">方法：<span class="built_in">str</span>.translate</span><br><span class="line">参数：一个转换表。</span><br><span class="line">作用：使用转换表来替换字符串中的字符。</span><br><span class="line">返回值：一个新的字符串。</span><br><span class="line">在这里，<span class="built_in">str</span>(x).translate(tab) 使用 tab 转换表将字符串 x 中的字符替换为映射后的字符。</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>Leetcode每日一题2</title>
    <url>/2023/09/18/Leetcode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982/</url>
    <content><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>表: <code>NPV</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------------+---------+</span><br><span class="line">| Column Name   | Type    |</span><br><span class="line">+---------------+---------+</span><br><span class="line">| id            | int     |</span><br><span class="line">| year          | int     |</span><br><span class="line">| npv           | int     |</span><br><span class="line">+---------------+---------+</span><br><span class="line">(id, year) 是该表主键(具有唯一值的列的组合).</span><br><span class="line">该表有每一笔存货的年份, id 和对应净现值的信息.</span><br></pre></td></tr></table></figure>

<p>表: <code>Queries</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">+---------------+---------+</span><br><span class="line">| Column Name   | Type    |</span><br><span class="line">+---------------+---------+</span><br><span class="line">| id            | int     |</span><br><span class="line">| year          | int     |</span><br><span class="line">+---------------+---------+</span><br><span class="line">(id, year) 是该表主键(具有唯一值的列的组合).</span><br><span class="line">该表有每一次查询所对应存货的 id 和年份的信息.</span><br></pre></td></tr></table></figure>

<p>编写解决方案，找到 Queries 表中每一次查询的净现值。</p>
<p>结果表 <strong>没有顺序要求</strong> 。</p>
<p>查询结果的格式如下所示:</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入：</span><br><span class="line">NPV 表:</span><br><span class="line">+------+--------+--------+</span><br><span class="line">| id   | year   | npv    |</span><br><span class="line">+------+--------+--------+</span><br><span class="line">| 1    | 2018   | 100    |</span><br><span class="line">| 7    | 2020   | 30     |</span><br><span class="line">| 13   | 2019   | 40     |</span><br><span class="line">| 1    | 2019   | 113    |</span><br><span class="line">| 2    | 2008   | 121    |</span><br><span class="line">| 3    | 2009   | 12     |</span><br><span class="line">| 11   | 2020   | 99     |</span><br><span class="line">| 7    | 2019   | 0      |</span><br><span class="line">+------+--------+--------+</span><br><span class="line"></span><br><span class="line">Queries 表:</span><br><span class="line">+------+--------+</span><br><span class="line">| id   | year   |</span><br><span class="line">+------+--------+</span><br><span class="line">| 1    | 2019   |</span><br><span class="line">| 2    | 2008   |</span><br><span class="line">| 3    | 2009   |</span><br><span class="line">| 7    | 2018   |</span><br><span class="line">| 7    | 2019   |</span><br><span class="line">| 7    | 2020   |</span><br><span class="line">| 13   | 2019   |</span><br><span class="line">+------+--------+</span><br><span class="line">输出：</span><br><span class="line">+------+--------+--------+</span><br><span class="line">| id   | year   | npv    |</span><br><span class="line">+------+--------+--------+</span><br><span class="line">| 1    | 2019   | 113    |</span><br><span class="line">| 2    | 2008   | 121    |</span><br><span class="line">| 3    | 2009   | 12     |</span><br><span class="line">| 7    | 2018   | 0      |</span><br><span class="line">| 7    | 2019   | 0      |</span><br><span class="line">| 7    | 2020   | 30     |</span><br><span class="line">| 13   | 2019   | 40     |</span><br><span class="line">+------+--------+--------+</span><br><span class="line">解释：</span><br><span class="line">(7, 2018)的净现值不在 NPV 表中, 我们把它看作是 0.</span><br><span class="line">所有其它查询的净现值都能在 NPV 表中找到.</span><br></pre></td></tr></table></figure>

<h2 id="我的解法"><a href="#我的解法" class="headerlink" title="我的解法"></a>我的解法</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Write your MySQL query statement below</span><br><span class="line">select</span><br><span class="line">    q.id,</span><br><span class="line">    q.year,</span><br><span class="line">    ifnull(npv,0) npv</span><br><span class="line">from</span><br><span class="line">    Queries q</span><br><span class="line">    left join</span><br><span class="line">    NPV n using(id,year);</span><br></pre></td></tr></table></figure>

<h2 id="函数解释"><a href="#函数解释" class="headerlink" title="函数解释"></a>函数解释</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">这段代码是一个 SQL 查询，用于从两个表（Queries 和 NPV）中选取数据，并且还使用了一些特定的函数和关键字。让我们逐一解释每一部分。</span><br><span class="line"></span><br><span class="line">总体结构</span><br><span class="line">这是一个 SELECT 查询，用于从一个或多个表中选取数据。在这个例子中，它从 Queries 和 NPV 这两个表中选取数据。</span><br><span class="line"></span><br><span class="line">SELECT 子句</span><br><span class="line">sql</span><br><span class="line">Copy code</span><br><span class="line">select</span><br><span class="line">    q.id,</span><br><span class="line">    q.year,</span><br><span class="line">    ifnull(npv,0) npv</span><br><span class="line">这部分指定了要从查询结果中选取哪些字段：</span><br><span class="line"></span><br><span class="line">q.id：从 Queries 表中选取 id 字段。</span><br><span class="line">q.year：从 Queries 表中选取 year 字段。</span><br><span class="line">ifnull(npv,0) npv：使用 IFNULL 函数来检查 npv 字段是否为 NULL。如果是 NULL，则返回 0；否则返回 npv 的实际值。查询结果中的这一列被命名为 npv。</span><br><span class="line">FROM 和 JOIN 子句</span><br><span class="line">sql</span><br><span class="line">Copy code</span><br><span class="line">from</span><br><span class="line">    Queries q</span><br><span class="line">    left join</span><br><span class="line">    NPV n using(id,year);</span><br><span class="line">这部分指定了查询的数据来源：</span><br><span class="line"></span><br><span class="line">Queries q：数据首先来自 Queries 表，并用别名 q 表示。</span><br><span class="line">left join NPV n using(id,year)：执行左连接（Left Join）操作，连接 Queries 表和 NPV 表。连接条件是 id 和 year 字段必须匹配，而且这两个字段在两个表中都存在。</span><br><span class="line">左连接（Left Join）</span><br><span class="line">左连接意味着，即使在 NPV 表中没有匹配的记录，Queries 表中的所有记录也会出现在结果集中。对于 NPV 表中没有的记录，npv 字段将为 NULL。</span><br><span class="line"></span><br><span class="line">IFNULL 函数</span><br><span class="line">IFNULL 是一个 MySQL 函数，用于检查第一个参数是否为 NULL。如果是 NULL，则返回第二个参数的值；否则返回第一个参数的值。</span><br><span class="line"></span><br><span class="line">总结</span><br><span class="line">这个查询的目的是从 Queries 表中选取所有的 id 和 year，并试图找到与之匹配的 NPV 表中的 npv 值。如果找不到匹配的 npv 值，结果集中的 npv 将为 0。</span><br><span class="line"></span><br><span class="line">这样做通常是为了确保查询结果中没有 NULL 值，这可能会在后续数据处理中造成问题。</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
</search>
