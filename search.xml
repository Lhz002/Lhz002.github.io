<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2023/07/20/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
  </entry>
  <entry>
    <title>MIT线性代数学习（一）</title>
    <url>/2023/07/21/MIT%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="用几何理解本质，用代数运用方法"><a href="#用几何理解本质，用代数运用方法" class="headerlink" title="用几何理解本质，用代数运用方法"></a>用几何理解本质，用代数运用方法</h1><h2 id="行图像（Raw-picture）"><a href="#行图像（Raw-picture）" class="headerlink" title="行图像（Raw picture）"></a><strong>行图像（Raw picture）</strong></h2><p>将未知数的系数作为矩阵的行，构成系数矩阵。有m组方程则构成m行，有n个未知数则作为n列。第二个矩阵则是由未知数构成。矩阵方程等号右侧则为方程右侧的数或者参数</p>
<p>在图像上理解为按行来讲，第一个方程到第m个方程在图像上相交的点作为方程组的解（从几何上理解求解的过程和本质）。而当三个未知数时，则可以看作平面在坐标系上的相交，相交的一点则可以作为方程式的解。</p>
<h2 id="列图像（Column-picture）"><a href="#列图像（Column-picture）" class="headerlink" title="列图像（Column picture）"></a><strong>列图像（Column picture）</strong></h2><p>按照参数，有m组方程，则写为未知数×m个未知数参数作为行的矩阵方程。这样的写法可以理解成为以m维向量n个基底向量来组成等号右侧的向量。</p>
<p>理论来说，当(x,y)取任意值的时候，可以表示坐标系上任何一个向量。而当参数从两个，即二维向量不断上升，到三维或者更高维度时，则看作数学意义上的高维向量来组成等号右侧的向量。</p>
]]></content>
      <categories>
        <category>数理基础类</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>AntiPre基于抗原的辅助抗体生成模型</title>
    <url>/2023/07/22/AntiPre%E5%9F%BA%E4%BA%8E%E6%8A%97%E5%8E%9F%E7%9A%84%E8%BE%85%E5%8A%A9%E6%8A%97%E4%BD%93%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="模型构建思路"><a href="#模型构建思路" class="headerlink" title="模型构建思路"></a>模型构建思路</h1><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>处理蛋白质序列数据并使用 PyTorch 进行模型训练的任务包含了以下几个步骤：</p>
<p>首先，从 Protein Data Bank (PDB) 获取蛋白质序列数据。下为一个通常用于获取 PDB 数据的 python 代码示例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># The protein id</span></span><br><span class="line">protein_id = <span class="string">&#x27;1abc&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the url</span></span><br><span class="line">url = <span class="string">f&#x27;https://files.rcsb.org/download/<span class="subst">&#123;protein_id&#125;</span>.pdb&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the local filename</span></span><br><span class="line">filename = <span class="string">f&#x27;<span class="subst">&#123;protein_id&#125;</span>.pdb&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If the file doesn&#x27;t exist, download it</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(filename):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Downloading PDB file <span class="subst">&#123;protein_id&#125;</span>...&#x27;</span>)</span><br><span class="line">    urllib.request.urlretrieve(url, filename)</span><br></pre></td></tr></table></figure>

<p>然后，需要将蛋白质序列数据转换成适合用于模型训练的形式。这通常包括将蛋白质序列数据进行 one-hot 编码或者转换为词嵌入向量等。下面是一个如何进行 one-hot 编码的代码示例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the protein sequence</span></span><br><span class="line">protein_sequence = <span class="string">&#x27;ACDEFGHIKLMNPQRSTVWY&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape the protein sequence to be 2D</span></span><br><span class="line">protein_sequence = np.array(<span class="built_in">list</span>(protein_sequence)).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the one-hot encoder</span></span><br><span class="line">encoder = OneHotEncoder(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit and transform the protein sequence</span></span><br><span class="line">one_hot_encoded_sequence = encoder.fit_transform(protein_sequence)</span><br></pre></td></tr></table></figure>

<p>最后，可以使用 PyTorch 来定义和训练模型。如果正在进行分类任务，可能需要使用一个卷积神经网络 (CNN) 或循环神经网络 (RNN)。如果正在进行序列生成任务，可能需要使用一个生成对抗网络 (GAN) 或变分自编码器 (VAE)。</p>
<h2 id="变分自编码器（VAE）和扩散模型（Diffusion）"><a href="#变分自编码器（VAE）和扩散模型（Diffusion）" class="headerlink" title="变分自编码器（VAE）和扩散模型（Diffusion）"></a>变分自编码器（VAE）和扩散模型（Diffusion）</h2><h3 id="变分自编码器（VAE）"><a href="#变分自编码器（VAE）" class="headerlink" title="变分自编码器（VAE）"></a>变分自编码器（VAE）</h3><p>首先，我们来理解一下变分自编码器(VAEs)和扩散模型的基本概念。</p>
<ol>
<li>变分自编码器(VAEs): VAEs是一类生成模型，通过最大化数据的边缘对数似然性，并在潜在空间中强制执行先验分布(通常为高斯分布)，从而学习数据的隐含表示。VAEs包含编码器和解码器两部分，编码器将输入数据编码为潜在空间的均值和方差，然后从此分布中采样，生成解码器可以解码的潜在向量。</li>
<li>扩散模型: 扩散模型是一种连续生成模型，可以理解为一个逐步“去噪”的过程。在这个过程中，模型从一个简单的先验分布（如高斯噪声）开始，然后通过一系列的扩散步骤（或去噪步骤）来生成数据。扩散模型最近在图像生成任务上取得了显著的成功。</li>
</ol>
<p>结合VAEs和扩散模型来构建生成模型的一种可能的方式是，使用VAEs来学习数据的隐含表示，然后在这个隐含表示上应用扩散过程。这样可以结合VAEs的表示学习能力和扩散模型的生成能力。</p>
<p>以下是一个简单的例子，展示如何使用PyTorch构建一个基本的变分自编码器。由于扩散模型的实现较为复杂，这里我们只展示了VAEs的部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Encoder</span></span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, latent_dim*<span class="number">2</span>) <span class="comment"># 为均值和方差各自预留空间</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Decoder</span></span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(latent_dim, <span class="number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, input_dim),</span><br><span class="line">            nn.Sigmoid() <span class="comment"># 输出层使用sigmoid激活函数，使得输出在(0, 1)范围内</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span>*logvar)</span><br><span class="line">        eps = torch.randn_like(std)</span><br><span class="line">        <span class="keyword">return</span> mu + eps*std</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        h = self.encoder(x)</span><br><span class="line">        mu, logvar = torch.chunk(h, <span class="number">2</span>, dim=<span class="number">1</span>) <span class="comment"># 分割为均值和方差</span></span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(z), mu, logvar</span><br></pre></td></tr></table></figure>

<p>在上述模型中，前向传播函数返回了解码的结果，以及隐含空间的均值和方差。这些值可以用于计算重构损失和KL散度，这两者是变分自编码器的两部分损失。</p>
<p>扩散模型的部分实现涉及到更多的细节，包括扩散过程的时间步设置、噪声水平的选择等。具体的实现方式需要根据你的具体需求和任务来决定。此外，由于当前对于结合VAEs和扩散模型的研究还不多，如何结合这两种模型可能需要进行一些创新性的尝试。</p>
<h3 id="扩散模型（Diffusion）"><a href="#扩散模型（Diffusion）" class="headerlink" title="扩散模型（Diffusion）"></a>扩散模型（Diffusion）</h3><p>在实际应用中，扩散模型通常需要大量的计算资源和训练时间，而且模型的性能还取决于许多超参数的精细调整。</p>
<p>下面的代码显示了一个基本的扩散模型的框架，它使用了一个简单的多层感知器（MLP）作为反向扩散过程的神经网络。由于扩散模型的复杂性，这只是一个简化的版本，仅供参考。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DiffusionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, hidden_dim, num_steps</span>):</span><br><span class="line">        <span class="built_in">super</span>(DiffusionModel, self).__init__()</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义扩散过程的神经网络，这里使用一个简单的MLP</span></span><br><span class="line">        self.network = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, hidden_dim),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(hidden_dim, input_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 在输入上添加高斯噪声</span></span><br><span class="line">        noise = torch.randn_like(x)</span><br><span class="line">        x_noisy = x + noise</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行反向扩散过程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.num_steps):</span><br><span class="line">            x_hat = self.network(x_noisy)</span><br><span class="line">            x_noisy = x_hat + torch.randn_like(x_hat)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_noisy</span><br></pre></td></tr></table></figure>

<p>在这个模型中，前向传播函数首先在输入上添加了高斯噪声，然后进行了多步的反向扩散过程。在每一步中，它都使用神经网络来预测去噪后的数据，并在预测结果上再次添加高斯噪声。经过多步的反向扩散后，最终返回生成的数据。</p>
<p>这是一个非常基础的扩散模型，实际上，在高级应用中，这个模型可能需要使用更复杂的神经网络结构，例如卷积神经网络（CNN）或者变换器（Transformer），并可能需要在每一步中使用不同的神经网络。此外，模型的训练过程也需要进行特别的设计，例如使用噪声对比估计（Noise Contrastive Estimation）或者分解概率流（Denoising Score Matching）等方法。</p>
<p>在将VAE和扩散模型结合的时候，一个可能的方式是，首先使用VAE的编码器将输入数据编码为潜在空间的表示，然后在这个表示上进行扩散过程。在反向传播的时候，可以同时优化VAE的编码器和解码器，以及扩散模型的神经网络。</p>
<h2 id="模型构建思路-1"><a href="#模型构建思路-1" class="headerlink" title="模型构建思路"></a>模型构建思路</h2><ol>
<li>首先，我们需要收集大量的蛋白质序列数据作为训练数据。由于我们无法直接从互联网获取数据，我们假设这些数据已经被保存在一个名为<code>protein_sequences</code>的列表中。</li>
<li>接下来，我们需要将这些蛋白质序列转换为适合神经网络处理的数值数据。在这个例子中，我们将使用一个简单的方法，将每个氨基酸编码为一个唯一的整数。然后，我们可以使用one-hot编码将这些整数转换为二进制向量。</li>
<li>接下来，我们将使用VAE来学习蛋白质序列的潜在表示。我们将使用一个简单的多层感知器（MLP）作为VAE的编码器和解码器。</li>
<li>最后，我们将在VAE的潜在表示上应用扩散模型，生成新的蛋白质序列。</li>
</ol>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
      <tags>
        <tag>深度学习，机器学习，VAE，Diffusion模型</tag>
      </tags>
  </entry>
  <entry>
    <title>针对pdb文件的数据预处理</title>
    <url>/2023/07/22/%E9%92%88%E5%AF%B9pdb%E6%96%87%E4%BB%B6%E7%9A%84%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<h1 id="针对PDB文件进行数据预处理"><a href="#针对PDB文件进行数据预处理" class="headerlink" title="针对PDB文件进行数据预处理"></a>针对PDB文件进行数据预处理</h1><h2 id="PDB文件的基本构成"><a href="#PDB文件的基本构成" class="headerlink" title="PDB文件的基本构成"></a>PDB文件的基本构成</h2><p>首先，我们需要理解 PDB (Protein Data Bank) 文件的基本结构。PDB 文件通常包含以下几种记录类型：</p>
<ul>
<li>HEADER：包含有关整个结构的基本信息。</li>
<li>COMPND：描述了分子组成。</li>
<li>AUTHOR：指明了该结构的作者。</li>
<li>ATOM：描述了分子中每个原子的坐标。</li>
<li>TER：标识链的结束。</li>
<li>CONECT：提供了原子间的键连接信息。</li>
</ul>
<p>对于模型训练，我们通常关注 ATOM 记录，它包含了原子的坐标信息。在利用 PyTorch 进行模型训练时，我们可能需要将这些坐标转化为张量。</p>
<p>以下是一个基础的 PDB 文件预处理方法。这个方法将读取 PDB 文件，提取原子坐标，并将它们转化为 PyTorch 张量。</p>
<p>首先，我们需要安装并导入一些必要的库。然而，当前的环境下没有互联网访问，所以我们假设您已经安装了以下所需的库：</p>
<ul>
<li><code>numpy</code>：用于数据处理</li>
<li><code>torch</code>：PyTorch 库，用于数据预处理和模型训练</li>
<li><code>Bio.PDB</code>：一个用于处理 PDB 文件的库，是 <code>Biopython</code> 包的一部分</li>
</ul>
<p><strong>注意：在您的本地环境中运行下面的代码之前，确保已经安装了这些库。</strong></p>
<p>接下来，我们可以编写一个函数来读取 PDB 文件并将原子坐标转化为张量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pythonCopy codeimport torch</span><br><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_to_tensor</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line">    </span><br><span class="line">    coordinates = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> model <span class="keyword">in</span> structure:</span><br><span class="line">        <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">            <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">                <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                    coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">    coordinates = torch.tensor(coordinates)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> coordinates</span><br></pre></td></tr></table></figure>

<p>在上述函数中，我们首先使用 <code>PDBParser</code> 来读取 PDB 文件。然后，我们遍历该结构的所有模型、链、残基和原子，将每个原子的坐标添加到 <code>coordinates</code> 列表中。最后，我们将 <code>coordinates</code> 列表转化为 PyTorch 张量。</p>
<p>使用这个函数，可以将 PDB 文件转化为 PyTorch 可处理的张量，这样就可以用于模型训练了。</p>
<h2 id="将氨基酸纳入模型参数范围"><a href="#将氨基酸纳入模型参数范围" class="headerlink" title="将氨基酸纳入模型参数范围"></a>将氨基酸纳入模型参数范围</h2><p>如果你需要将氨基酸的化学性质和组成的结构作为参数进行训练，你需要首先编码这些属性。常见的一种方法是使用独热编码（one-hot encoding）来表示氨基酸的类型，而氨基酸的化学性质则可以通过手动定义的特征来表示。也可以使用某种嵌入（embedding）策略来表示这些特性，但这通常需要训练数据来学习。</p>
<p>以下是一个扩展了前面函数的版本，这个函数将原子坐标、氨基酸类型（通过独热编码）和二级结构（通过 DSSP 计算）一起返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser, DSSP</span><br><span class="line"><span class="keyword">from</span> Bio.SeqUtils <span class="keyword">import</span> IUPACData</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_to_tensor</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line"></span><br><span class="line">    coordinates = []</span><br><span class="line">    amino_acids = []</span><br><span class="line">    sec_structure = []</span><br><span class="line"></span><br><span class="line">    model = structure[<span class="number">0</span>] <span class="comment"># We&#x27;re only using the first model here</span></span><br><span class="line">    dssp = DSSP(model, file_path) <span class="comment"># DSSP for secondary structure</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">        <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">            <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">                <span class="comment"># One-hot encoding of amino acid type</span></span><br><span class="line">                amino_acid = [<span class="number">0</span>]*<span class="number">20</span></span><br><span class="line">                <span class="keyword">if</span> residue.get_resname() <span class="keyword">in</span> IUPACData.protein_letters_1to3:</span><br><span class="line">                    idx = IUPACData.protein_letters.index(IUPACData.protein_letters_1to3[residue.get_resname()])</span><br><span class="line">                    amino_acid[idx] = <span class="number">1</span></span><br><span class="line">                amino_acids.append(amino_acid)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Secondary structure</span></span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    sec_structure.append(dssp[(chain.get_id(), residue.get_id())][<span class="number">2</span>])</span><br><span class="line">                <span class="keyword">except</span> KeyError:</span><br><span class="line">                    sec_structure.append(<span class="number">0</span>) <span class="comment"># If residue not in DSSP output, assign a default value</span></span><br><span class="line"></span><br><span class="line">    coordinates = torch.tensor(coordinates)</span><br><span class="line">    amino_acids = torch.tensor(amino_acids)</span><br><span class="line">    sec_structure = torch.tensor(sec_structure)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> coordinates, amino_acids, sec_structure</span><br></pre></td></tr></table></figure>

<p>这个函数首先使用 <code>PDBParser</code> 来读取 PDB 文件。然后，遍历结构的所有链和残基，并将每个原子的坐标、残基的独热编码和二级结构添加到各自的列表中。最后，我们将这些列表转化为 PyTorch 张量。</p>
<p>函数使用了 DSSP 来计算二级结构。DSSP 是一个可以从原子坐标计算蛋白质二级结构的程序。<code>Bio.PDB.DSSP</code> 模块提供了一个接口来运行 DSSP 并解析其输出。然而，这需要你的计算环境中已经安装了 DSSP。</p>
<h2 id="针对多个PDB文件进行批量处理"><a href="#针对多个PDB文件进行批量处理" class="headerlink" title="针对多个PDB文件进行批量处理"></a>针对多个PDB文件进行批量处理</h2><p>为了对多个 PDB 文件进行批量处理，我们可以修改上述函数，使其可以接收一个包含多个 PDB 文件路径的列表，然后对每个文件进行处理。以下是一个修改后的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> PDBParser, DSSP</span><br><span class="line"><span class="keyword">from</span> Bio.SeqUtils <span class="keyword">import</span> IUPACData</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pdb_files_to_tensors</span>(<span class="params">file_paths</span>):</span><br><span class="line">    parser = PDBParser(QUIET=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    data = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> file_path <span class="keyword">in</span> file_paths:</span><br><span class="line">        structure = parser.get_structure(<span class="string">&quot;pdb&quot;</span>, file_path)</span><br><span class="line"></span><br><span class="line">        coordinates = []</span><br><span class="line">        amino_acids = []</span><br><span class="line">        sec_structure = []</span><br><span class="line"></span><br><span class="line">        model = structure[<span class="number">0</span>] <span class="comment"># We&#x27;re only using the first model here</span></span><br><span class="line">        dssp = DSSP(model, file_path) <span class="comment"># DSSP for secondary structure</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> chain <span class="keyword">in</span> model:</span><br><span class="line">            <span class="keyword">for</span> residue <span class="keyword">in</span> chain:</span><br><span class="line">                <span class="keyword">for</span> atom <span class="keyword">in</span> residue:</span><br><span class="line">                    coordinates.append(atom.get_coord())</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># One-hot encoding of amino acid type</span></span><br><span class="line">                    amino_acid = [<span class="number">0</span>]*<span class="number">20</span></span><br><span class="line">                    <span class="keyword">if</span> residue.get_resname() <span class="keyword">in</span> IUPACData.protein_letters_1to3:</span><br><span class="line">                        idx = IUPACData.protein_letters.index(IUPACData.protein_letters_1to3[residue.get_resname()])</span><br><span class="line">                        amino_acid[idx] = <span class="number">1</span></span><br><span class="line">                    amino_acids.append(amino_acid)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Secondary structure</span></span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        sec_structure.append(dssp[(chain.get_id(), residue.get_id())][<span class="number">2</span>])</span><br><span class="line">                    <span class="keyword">except</span> KeyError:</span><br><span class="line">                        sec_structure.append(<span class="number">0</span>) <span class="comment"># If residue not in DSSP output, assign a default value</span></span><br><span class="line"></span><br><span class="line">        coordinates = torch.tensor(coordinates)</span><br><span class="line">        amino_acids = torch.tensor(amino_acids)</span><br><span class="line">        sec_structure = torch.tensor(sec_structure)</span><br><span class="line">        </span><br><span class="line">        data.append((coordinates, amino_acids, sec_structure))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>

<p>这个函数首先创建了一个空的 <code>data</code> 列表，然后对 <code>file_paths</code> 列表中的每个文件路径，它都执行和上述函数相同的处理过程。每次处理一个文件，它都会将结果（即坐标、氨基酸类型和二级结构的张量）添加到 <code>data</code> 列表中。最后，函数返回 <code>data</code> 列表。</p>
<p>你可以使用类似以下的方式来获取目录下所有 PDB 文件的路径，然后将这些路径传递给函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pdb_dir = <span class="string">&quot;/path/to/your/pdb/files&quot;</span></span><br><span class="line">pdb_files = [os.path.join(pdb_dir, file) <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(pdb_dir) <span class="keyword">if</span> file.endswith(<span class="string">&#x27;.pdb&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">data = pdb_files_to_tensors(pdb_files)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
      <tags>
        <tag>深度学习，数据预处理</tag>
      </tags>
  </entry>
  <entry>
    <title>基于VAE和transformer模型根据抗原结构生成抗体序列</title>
    <url>/2023/07/22/%E5%9F%BA%E4%BA%8EVAE%E5%92%8Ctransformer%E6%A8%A1%E5%9E%8B%E6%A0%B9%E6%8D%AE%E6%8A%97%E5%8E%9F%E7%BB%93%E6%9E%84%E7%94%9F%E6%88%90%E6%8A%97%E4%BD%93%E5%BA%8F%E5%88%97/</url>
    <content><![CDATA[<h1 id="构建思路"><a href="#构建思路" class="headerlink" title="构建思路"></a>构建思路</h1><ol>
<li><p><strong>数据准备</strong>：首先，需要从PDB数据库获取抗体-抗原复合物的数据。需要先对数据进行预处理，以便用于训练模型。之后可以将抗体的氨基酸序列用于训练VAE，并将抗原的结构数据用于训练Transformer模型。</p>
</li>
<li><p><strong>训练VAE</strong>：VAE是一种生成模型，可以用于学习抗体氨基酸序列的潜在表示。你可以将抗体的氨基酸序列编码为一种潜在表示，然后从这种潜在表示中解码出原始序列。通过优化VAE的参数，使模型学习到如何生成新的抗体序列。</p>
</li>
<li><p><strong>训练Transformer模型</strong>：Transformer模型是一种序列到序列的模型，可以用于根据抗原的结构数据生成抗体的潜在表示。你可以将抗原的结构数据编码为一个序列，然后使用Transformer模型将这个序列转换为抗体的潜在表示。</p>
</li>
<li><p><strong>联合训练</strong>：一旦VAE和Transformer模型都被单独训练过，就可以开始联合训练这两个模型。可以将Transformer模型的输出（即抗体的潜在表示）用作VAE的输入，然后让VAE生成抗体的氨基酸序列。通过优化这两个模型的参数，使模型学习到如何根据抗原的结构数据生成抗体序列。</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p>首先，我们需要一个函数来解析PDB文件。PDB文件是一种用于存储蛋白质数据的标准格式，其中包含了蛋白质的氨基酸序列和三维结构信息。然后我们需要创建一个数据集类来加载这些数据。</p>
<p>以下是一个解析PDB文件和创建数据集的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pad_sequence</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> one_hot</span><br><span class="line"><span class="keyword">from</span> Bio.PDB <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析PDB文件并返回序列和结构信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_pdb_file</span>(<span class="params">file_path</span>):</span><br><span class="line">    parser = PDBParser()</span><br><span class="line">    structure = parser.get_structure(<span class="string">&#x27;X&#x27;</span>, file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取序列信息</span></span><br><span class="line">    ppb = PPBuilder()</span><br><span class="line">    <span class="keyword">for</span> pp <span class="keyword">in</span> ppb.build_peptides(structure):</span><br><span class="line">        sequence = pp.get_sequence()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取结构信息</span></span><br><span class="line">    atom_list = Selection.unfold_entities(structure, <span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">    <span class="comment"># 此处简化为取每个氨基酸的CA原子的坐标，实际应用中可能需要更详细的结构信息</span></span><br><span class="line">    structure_info = [atom.get_coord() <span class="keyword">for</span> atom <span class="keyword">in</span> atom_list <span class="keyword">if</span> atom.get_name() == <span class="string">&#x27;CA&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sequence, structure_info</span><br><span class="line"><span class="comment"># 创建PyTorch数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PDBDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dir_path, transform=<span class="literal">None</span></span>):</span><br><span class="line">        self.dir_path = dir_path</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.file_list = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir(dir_path) <span class="keyword">if</span> f.endswith(<span class="string">&#x27;.pdb&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.file_list)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        file_path = os.path.join(self.dir_path, self.file_list[idx])</span><br><span class="line">        sequence, structure_info = parse_pdb_file(file_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将序列和结构信息转换为Tensor</span></span><br><span class="line">        sequence_tensor = torch.tensor([amino_acid_to_index[aa] <span class="keyword">for</span> aa <span class="keyword">in</span> sequence], dtype=torch.long)</span><br><span class="line">        sequence_tensor = one_hot(sequence_tensor, num_classes=<span class="built_in">len</span>(amino_acid_to_index))  <span class="comment"># 对氨基酸进行one-hot编码</span></span><br><span class="line">        structure_tensor = torch.tensor(structure_info, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sequence_tensor, structure_tensor</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        sequences, structures = <span class="built_in">zip</span>(*batch)</span><br><span class="line">        <span class="comment"># 使用填充来处理长度不同的序列</span></span><br><span class="line">        sequences_padded = pad_sequence([torch.flatten(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences], batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">        structures_padded = pad_sequence(structures, batch_first=<span class="literal">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> sequences_padded, structures_padded</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataset = PDBDataset(<span class="string">&#x27;/path/to/your/pdb/files&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>, collate_fn=PDBDataset.collate_fn)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这段代码中，我们首先定义了一个函数<code>parse_pdb_file</code>来解析PDB文件。然后我们定义了一个PyTorch数据集类<code>PDBDataset</code>，它会遍历给定目录中的所有PDB文件，对每个文件调用<code>parse_pdb_file</code>函数，并将结果转换为Tensor。</p>
<p>由于蛋白质序列的长度可能会不同，我们需要对<code>PDBDataset</code>类进行一些修改，以处理这些长度不同的序列。</p>
<p>一种常用的方法是使用填充（padding），即在较短的序列后面添加一些特殊的元素（例如零），以使所有的序列都有相同的长度。在PyTorch中，我们可以使用<code>torch.nn.utils.rnn.pad_sequence</code>函数来实现这个功能。</p>
<p>我在<code>__getitem__</code>方法中增加了一个one-hot编码的步骤。之后在<code>collate_fn</code>方法中添加了一个填充的步骤，这个步骤会在每个批次中被调用，以处理长度不同的序列。最后，我在创建数据加载器时传入了这个新的<code>collate_fn</code>方法，以覆盖默认的数据组合方法。</p>
<h2 id="模型定义和训练"><a href="#模型定义和训练" class="headerlink" title="模型定义和训练"></a>模型定义和训练</h2><p>由于我们想要的模型是基于抗原结构生成抗体序列的生成模型，我们需要对模型做出一些修改以适应这个任务。我们需要使用抗原的结构信息作为输入，然后使用VAE的解码器部分生成抗体的氨基酸序列。此外，我们还需要在模型中添加一个Transformer层，用于处理输入的抗原结构信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义VAE</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">1024</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc21 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc22 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">20</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">400</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h1 = F.relu(self.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc21(h1), self.fc22(h1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span>*logvar)</span><br><span class="line">        eps = torch.randn_like(std)</span><br><span class="line">        <span class="keyword">return</span> mu + eps*std</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, z</span>):</span><br><span class="line">        h3 = F.relu(self.fc3(z))</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(self.fc4(h3))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mu, logvar = self.encode(x.view(-<span class="number">1</span>, <span class="number">1024</span>))</span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> self.decode(z), mu, logvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Transformer模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(<span class="string">&#x27;bert-base-uncased&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self.bert(x)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型并设置优化器</span></span><br><span class="line">vae = VAE().to(device)</span><br><span class="line">transformer = TransformerModel().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(<span class="built_in">list</span>(vae.parameters()) + <span class="built_in">list</span>(transformer.parameters()), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch, vae, transformer, optimizer, train_loader, device</span>):</span><br><span class="line">    vae.train()</span><br><span class="line">    transformer.train()</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (sequence, structure) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        sequence = sequence.to(device)</span><br><span class="line">        structure = structure.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用Transformer模型处理结构信息</span></span><br><span class="line">        structure_representation = transformer(structure)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将sequence调整为合适的形状</span></span><br><span class="line">        sequence_reshaped = sequence.view(sequence.shape[<span class="number">0</span>], -<span class="number">1</span>, <span class="built_in">len</span>(amino_acid_to_index))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用VAE生成序列</span></span><br><span class="line">        recon_batch, mu, logvar = vae(structure_representation)</span><br><span class="line">        </span><br><span class="line">        loss = loss_function(recon_batch, sequence_reshaped, mu, logvar)</span><br><span class="line">        loss.backward()</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;====&gt; Epoch: &#123;&#125; Average loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, train_loss / <span class="built_in">len</span>(train_loader.dataset)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">    train(epoch, vae, transformer, optimizer, train_loader, device)</span><br></pre></td></tr></table></figure>

<p>这个修改后的代码首先定义了一个VAE模型和一个Transformer模型。然后，在训练过程中，我们首先使用Transformer模型处理输入的结构信息，然后将这个处理后的结构信息作为输入传递给VAE，让VAE生成序列。我们使用重构损失和KL散度损失来训练这个模型。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>技术类</category>
      </categories>
      <tags>
        <tag>深度学习，transformer</tag>
      </tags>
  </entry>
</search>
